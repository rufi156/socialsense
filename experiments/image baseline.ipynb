{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee38c",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  # Enforce deterministic algorithms\n",
    "        torch.backends.cudnn.benchmark = False     # Disable benchmark for reproducibility\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)       # Seed Python hashing, which can affect ordering\n",
    "set_seed(42)\n",
    "\n",
    "# DATASET_DIR = (Path(\"..\") / \"..\" / \"datasets\").resolve()\n",
    "# DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n",
    "# LABEL_COLS = [\n",
    "#     \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n",
    "#     \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n",
    "#     \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n",
    "# ]\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from data_processing.data_processing import create_crossvalidation_loaders\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.buffers import NaiveRehearsalBuffer\n",
    "from data_processing.data_processing import get_domain_dataloaders\n",
    "from models.training_utils import heuristic_dualbranch_batch, unified_train_loop\n",
    "from models.heuristicSplitModel import DualBranchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca986",
   "metadata": {},
   "source": [
    "## CL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for potential ways to combine networks\n",
    "# residual = SpecificHead(base + invariant_feats)\n",
    "# specific_feats = invariant_feats + residual\n",
    "\n",
    "# gate = GateNet(base + invariant_feats)\n",
    "# residual = SpecificHead(base)\n",
    "# specific_feats = invariant_feats + gate * residual\n",
    "\n",
    "# residual = Attention(invariant_feats, base)\n",
    "# specific_feats = invariant_feats + residual\n",
    "\n",
    "# gamma, beta = SpecificHead(base)\n",
    "# specific_feats = gamma * invariant_feats + beta\n",
    "\n",
    "# invariant_feats = InvariantHead(base)\n",
    "# specific_feats = SpecificHead(base)\n",
    "# cos_sim(invariant_feats, specific_feats) ~ 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849e928",
   "metadata": {},
   "source": [
    "### K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_fold_histories(fold_history):\n",
    "#     \"\"\"\n",
    "#     Combine multiple training histories by averaging them element-wise.\n",
    "#     Handles both simple lists and lists of dictionaries.\n",
    "#     \"\"\"\n",
    "#     combined_history = {}\n",
    "#     metric_keys = list(fold_history[0].keys())\n",
    "    \n",
    "#     for key in metric_keys:\n",
    "#         # Check if this metric contains dictionaries\n",
    "#         first_element = fold_history[0][key][0] if fold_history[0][key] else None\n",
    "        \n",
    "#         if isinstance(first_element, dict):\n",
    "#             # Handle lists of dictionaries (train_epoch_metrics, grad_norms)\n",
    "#             combined_history[key] = average_list_of_dicts(fold_history, key)\n",
    "#         else:\n",
    "#             # Handle simple lists (train_epoch_loss, val_epoch_loss, cross_domain_val)\n",
    "#             stacked_metrics = np.stack([fold_history[fold][key] for fold in fold_history])\n",
    "#             combined_history[key] = np.mean(stacked_metrics, axis=0).tolist()\n",
    "    \n",
    "#     return combined_history\n",
    "\n",
    "# def average_list_of_dicts(fold_history, metric_key):\n",
    "#     \"\"\"\n",
    "#     Average a list of dictionaries across folds.\n",
    "#     \"\"\"\n",
    "#     # Get the dictionary keys from the first fold's first epoch\n",
    "#     dict_keys = list(fold_history[0][metric_key][0].keys())\n",
    "    \n",
    "#     # Convert each fold's list of dicts to a 2D numpy array\n",
    "#     fold_arrays = []\n",
    "#     for fold in fold_history:\n",
    "#         # Convert list of dicts to 2D array: [epochs, sub_metrics]\n",
    "#         fold_array = np.array([[epoch_dict[k] for k in dict_keys] \n",
    "#                               for epoch_dict in fold_history[fold][metric_key]])\n",
    "#         fold_arrays.append(fold_array)\n",
    "    \n",
    "#     # Stack all folds and average: [folds, epochs, sub_metrics] -> [epochs, sub_metrics]\n",
    "#     stacked = np.stack(fold_arrays)\n",
    "#     averaged = np.mean(stacked, axis=0)\n",
    "    \n",
    "#     # Convert back to list of dictionaries\n",
    "#     result = []\n",
    "#     for epoch_values in averaged:\n",
    "#         epoch_dict = dict(zip(dict_keys, epoch_values))\n",
    "#         result.append(epoch_dict)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Add K fold labels to datapoints in dataframe, later used for creating K Fold Dataloaders\n",
    "\n",
    "# df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "# df['image_path'] = '../' + df['image_path']\n",
    "\n",
    "# # Initialize fold column\n",
    "# df['fold'] = -1\n",
    "\n",
    "# # Get unique image paths for splitting\n",
    "# unique_images_df = df[['image_path', 'domain']].reset_index(drop=True)\n",
    "# #exclude test subset test idx get from get_dataloader()\n",
    "# unique_images_df = unique_images_df[~unique_images_df['image_path'].isin(test_split_idx)]\n",
    "\n",
    "# # Create stratified 5-fold splits based on domain\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# # Assign fold numbers based on domain stratification\n",
    "# for fold, (_, val_idx) in enumerate(skf.split(unique_images_df['image_path'], unique_images_df['domain'])):\n",
    "#     val_image_paths = unique_images_df.iloc[val_idx]['image_path'].tolist()\n",
    "#     df.loc[df['image_path'].isin(val_image_paths), 'fold'] = fold\n",
    "\n",
    "# # Verify domain distribution across folds\n",
    "# print(\"Domain distribution across folds:\")\n",
    "# print(df.groupby(['fold', 'domain']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_loaders = create_crossvalidation_loaders(df, 5, batch_sizes=(32, 64, 64), resize_img_to=(512, 288))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ec863",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2a457",
   "metadata": {},
   "source": [
    "### current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tests: retrain the model with\n",
    "# 3 trains averaged\n",
    "# both branches + random masks\n",
    "\n",
    "# but first: fix underfitting: validation set\n",
    "# lower droput\n",
    "# diff branch:\n",
    "#     resnet18\n",
    "#     mobilenetv2\n",
    "#     efficientnetb0\n",
    "#     clip\n",
    "\n",
    "#     + ablations - only_soc, only_env, no_mask\n",
    "# learning rate scheduler with warmup and decay\n",
    "\n",
    "# then\n",
    "# best model, x3 random initialisaiton, on same test set, no validation, report final metrics on test set\n",
    "#same for ablations\n",
    "#same for random mask\n",
    "\n",
    "\"\"\"\n",
    "mobilenetv2 is fine\n",
    "\n",
    "test mobilenetv2 vs simple CNN\n",
    "vs normalised features\n",
    "vs 3 epochs\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502be9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((144,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "domains = ['Home', 'BigOffice-2', 'BigOffice-3', 'Hallway', 'MeetingRoom', 'SmallOffice']\n",
    "\n",
    "testing_scenarios = {\n",
    "    'simplebranch':      (False, 30, {'branch':'simple'}),\n",
    "    'simplebranch_branchnorm':      (True, 30, {'branch':'simple'}),\n",
    "    'mobilenetv2_branchnorm':      (True, 30, {'branch':'mobilenetv2'}),\n",
    "    'mobilenetv2':       (False, 3, {'branch':'mobilenetv2'}),\n",
    "}\n",
    "    \n",
    "for name, (branch_norm, epochs, setup) in testing_scenarios.items():\n",
    "\n",
    "        # Training Data\n",
    "        df = pd.read_pickle(\"../data/pepper_data_train.pkl\")\n",
    "\n",
    "        domain_dataloaders = get_domain_dataloaders(df, batch_sizes=(32, 64, 64), double_img=True, transforms=[transform]*2, num_workers=0, include_test=None)\n",
    "\n",
    "        print(f\"\\nTesting: {name}\")\n",
    "        model = DualBranchModel(dropout_rate=0.1, setup=setup, branch_norm=branch_norm)\n",
    "        dual_model = model.to(device)\n",
    "        trainable_params = [p for p in dual_model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=1e-3)\n",
    "        buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "\n",
    "        exp_name = f\"{name}_epochs{epochs}_dropout{0.1}__{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        dualbranch_kwargs = {\n",
    "                'mse_criterion': nn.MSELoss(),\n",
    "                'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            }\n",
    "        \n",
    "        \n",
    "        unified_train_loop(\n",
    "            model=dual_model,\n",
    "            domains=domains,\n",
    "            domain_dataloaders=domain_dataloaders,\n",
    "            buffer=buffer,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            batch_fn=heuristic_dualbranch_batch,\n",
    "            batch_kwargs=dualbranch_kwargs,\n",
    "            num_epochs=epochs,\n",
    "            exp_name=exp_name,\n",
    "            gradient_clipping=True,\n",
    "            collect_tsne_data=False,\n",
    "            checkpoint_dir=\"../checkpoints\",\n",
    "            validation_set='val',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# model_names = [\n",
    "#     'heuristic_small_env',\n",
    "#     'heuristic_square_img',\n",
    "#     'heuristic_eval_buffer'\n",
    "# ]\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     print(f\"Running {model_name}\")\n",
    "#     process = subprocess.Popen(\n",
    "#         [\"python\", \"train_models.py\", \"--model_name\", model_name, \"--num_workers\", \"0\"],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.STDOUT,\n",
    "#         text=True\n",
    "#     )\n",
    "\n",
    "#     for line in process.stdout:\n",
    "#         print(line, end='')\n",
    "\n",
    "#     process.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
