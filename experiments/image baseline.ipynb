{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee38c",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "DATASET_DIR = (Path(\"..\") / \"..\" / \"datasets\").resolve()\n",
    "DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n",
    "LABEL_COLS = [\n",
    "    \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n",
    "    \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n",
    "    \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9da04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_path, dataset):\n",
    "    \"\"\"Process individual CSV files\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop(columns=df.columns[-1])\n",
    "    \n",
    "    # Extract metadata from first column\n",
    "    first_col = df.columns[0]\n",
    "    split_data = df[first_col].str.split('_', n=2, expand=True)\n",
    "    \n",
    "    df[\"robot\"] = split_data[0]\n",
    "    df[\"domain\"] = split_data[1]\n",
    "    df[\"image_ref\"] = split_data[2].astype(int)\n",
    "    df[\"dataset\"] = dataset\n",
    "\n",
    "    df = df.drop(columns=[first_col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def consolidate_data(datasets):\n",
    "    \"\"\"Aggregate all CSVs\"\"\"\n",
    "    all_dfs = []\n",
    "    for dataset in datasets:\n",
    "        source_path = DATASET_DIR / dataset\n",
    "        \n",
    "        for robot in [\"NAO\", \"Pepper\", \"PR2\"]:\n",
    "            ann_dir = source_path / robot / \"Annotations\"\n",
    "            if not ann_dir.exists():\n",
    "                raise ValueError(f\"Labels csv file path ({ann_dir}) doesn't exist\")\n",
    "                \n",
    "            \n",
    "            for csv_file in ann_dir.glob(\"*.csv\"):\n",
    "                try:\n",
    "                    df = process_csv(csv_file, dataset)\n",
    "                    all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53657ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_raw_data(df):\n",
    "    \"\"\"Comprehensive data quality checks for raw annotation data\"\"\"\n",
    "    required_columns = {'robot', 'domain', 'image_ref', 'dataset'}\n",
    "\n",
    "    # Check for any missing columns\n",
    "    missing_cols = required_columns - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Label value validation (should be between 1 and 5)\n",
    "    for col in LABEL_COLS:\n",
    "        if df[col].min() < 1 or df[col].max() > 5:\n",
    "            raise ValueError(f\"Label {col} has invalid range [{df[col].min()}, {df[col].max()}]\")\n",
    "\n",
    "    # Null values check\n",
    "    null_cols = df.columns[df.isnull().any()].tolist()\n",
    "    if null_cols:\n",
    "        raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
    "\n",
    "    # Data type and value validation for image_ref\n",
    "    if not pd.api.types.is_integer_dtype(df['image_ref']):\n",
    "        raise TypeError(\"image_ref must be integer type\")\n",
    "    if (df['image_ref'] < 0).any():\n",
    "        raise ValueError(\"image_ref contains negative values, which is invalid\")\n",
    "\n",
    "    # Categorical value validation\n",
    "    valid_robots = {'NAO', 'Pepper', 'PR2'}\n",
    "    invalid_robots = set(df['robot']) - valid_robots\n",
    "    if invalid_robots:\n",
    "        raise ValueError(f\"Invalid robot values: {invalid_robots}\")\n",
    "\n",
    "    valid_sources = {'OFFICE-MANNERSDB', 'MANNERSDBPlus'}\n",
    "    invalid_sources = set(df['dataset']) - valid_sources\n",
    "    if invalid_sources:\n",
    "        raise ValueError(f\"Invalid source directories: {invalid_sources}\")\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb45d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_labels(df):\n",
    "    \"\"\"Aggregate multiple annotations per image by image path\"\"\"    \n",
    "    agg_dict = {\n",
    "        **{col: 'mean' for col in LABEL_COLS},\n",
    "        **{col: 'first' for col in df.columns.difference(LABEL_COLS).tolist()},\n",
    "    }\n",
    "    \n",
    "    return df.groupby('image_path', as_index=False).agg(agg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_image_path(row):\n",
    "    \"\"\"Robust path resolution with validation\"\"\"\n",
    "    base_dir = DATASET_DIR / row['dataset'] / row['robot'] / \"Images\"\n",
    "    \n",
    "    if row['dataset'] == \"OFFICE-MANNERSDB\":\n",
    "        target = base_dir / f\"{row['domain']}_{row['image_ref']}.png\"\n",
    "    else:\n",
    "        target = next(base_dir.glob(f\"{row['image_ref']}_*.png\"), None)\n",
    "    \n",
    "    if target and target.exists():\n",
    "        return str(target.resolve())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "            \n",
    "        img_path = str(self.df.at[idx, \"image_path\"])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {img_path}: {str(e)}\")\n",
    "        \n",
    "        raw_labels = self.df.iloc[idx][LABEL_COLS].values.astype(np.float32)\n",
    "        scaled_labels = (raw_labels - 1) / 4  # Convert 1-5 â†’ 0-1\n",
    "        domain_labels = self.df.at[idx, 'domain']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.from_numpy(scaled_labels), domain_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128)):\n",
    "    \"\"\"Create train/val/test dataloaders using image_path as unique key\"\"\"\n",
    "    \n",
    "    # Get image paths as indexing for split\n",
    "    unique_images = df[['image_path']].reset_index(drop=True)\n",
    "    \n",
    "    # Split using image_path as key #TODO is is spliting based on index or path strings? index faster\n",
    "    train_paths, temp_paths = train_test_split(\n",
    "        unique_images['image_path'], \n",
    "        test_size=0.3, \n",
    "        random_state=42\n",
    "    )\n",
    "    val_paths, test_paths = train_test_split(\n",
    "        temp_paths,\n",
    "        test_size=0.5, \n",
    "        random_state=42\n",
    "    )\n",
    "   \n",
    "    # Create subsets\n",
    "    train_df = df[df['image_path'].isin(train_paths)].reset_index(drop=True)\n",
    "    val_df = df[df['image_path'].isin(val_paths)].reset_index(drop=True)\n",
    "    test_df = df[df['image_path'].isin(test_paths)].reset_index(drop=True)\n",
    "    \n",
    "    #TODO add coordinate values for spacialy aware CNN Uber's CoordNav\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_img_to),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ImageLabelDataset(train_df, transform)\n",
    "    val_dataset = ImageLabelDataset(val_df, transform)\n",
    "    test_dataset = ImageLabelDataset(test_df, transform)\n",
    "    \n",
    "    # Create loaders\n",
    "    num_workers = 0\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_sizes[0], shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n",
    "        'val': DataLoader(val_dataset, batch_size=batch_sizes[1], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_sizes[2], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    }\n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_final_data(df):\n",
    "    \"\"\"Final validation after aggregation\"\"\"\n",
    "    # Missing image paths\n",
    "    missing = df[df['image_path'].isnull()]\n",
    "    if not missing.empty:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{len(missing)} images missing after aggregation. Examples:\\n\"\n",
    "            f\"{missing[['robot', 'domain', 'image_ref']].head()}\"\n",
    "        )\n",
    "    \n",
    "    # Null values check\n",
    "    null_cols = df.columns[df.isnull().any()].tolist()\n",
    "    if null_cols:\n",
    "        raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
    "\n",
    "    # Duplicate image paths\n",
    "    duplicates = df[df.duplicated('image_path', keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"Duplicate image paths after aggregation:\\n\"\n",
    "            f\"{duplicates['image_path'].unique()}\"\n",
    "        )\n",
    "\n",
    "    # Label validity (1-5)\n",
    "    for col in LABEL_COLS:\n",
    "        if df[col].min() < 1 or df[col].max() > 5:\n",
    "            raise ValueError(\n",
    "                f\"Aggregated label {col} out of range: \"\n",
    "                f\"[{df[col].min()}, {df[col].max()}]\"\n",
    "            )\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd34a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2a2a2b",
   "metadata": {},
   "source": [
    "## --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66130",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_df = consolidate_data(DATASETS)\n",
    "    validate_raw_data(raw_df)\n",
    "    raw_df['image_path'] = raw_df.apply(resolve_image_path, axis=1)   \n",
    "    aggregated_df = aggregate_labels(raw_df)\n",
    "    validate_final_data(aggregated_df) \n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "aggregated_df.to_pickle(\"../data/processed_all_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca986",
   "metadata": {},
   "source": [
    "## CL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialContinualModel(nn.Module):\n",
    "    def __init__(self, num_tasks=9):\n",
    "        super().__init__()\n",
    "        # Initialize the ResNet50 architecture with Places365 configuration\n",
    "        self.backbone = models.resnet50(num_classes=365)\n",
    "        \n",
    "        # get Places365 weights and fix their naming leftover from troch saving convention\n",
    "        places365_weights = torch.load('resnet50_places365.pth.tar', weights_only=True)\n",
    "        state_dict = places365_weights['state_dict']\n",
    "        state_dict = {k.replace('module.', ''): v \n",
    "                     for k, v in state_dict.items()}\n",
    "        \n",
    "        # Load weights\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "        # Remove classification head\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        #Freeze all params except last layer\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'layer4' not in name:\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        #TODO human mask average, std, quadrants, human in realtion to robot std\n",
    "        \n",
    "\n",
    "\n",
    "        # Shared layers #TODO deeper shared space?\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads with more expensive but finegrained GELU\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(512, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        shared = self.shared_fc(features)\n",
    "        outputs = [head(shared) for head in self.heads]\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class LGRBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    @misc{churamani_feature_2024,\n",
    "\t\ttitle = {Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours},\n",
    "\t\turl = {http://arxiv.org/abs/2405.15773},\n",
    "\t\tdoi = {10.48550/arXiv.2405.15773},\n",
    "\t\tnumber = {{arXiv}:2405.15773},\n",
    "\t\tpublisher = {{arXiv}},\n",
    "\t\tauthor = {Churamani, Nikhil and Checker, Saksham and Chiang, Hao-Tien Lewis and Gunes, Hatice},\n",
    "\t\turldate = {2025-01-30},\n",
    "\t\tdate = {2024-03-16},\n",
    "\t}\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(LGRBaseline, self).__init__()\n",
    "        \n",
    "        # MobileNetV2 backbone\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        # Backbone feature processing\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Regression head\n",
    "        self.fc1_bn = nn.BatchNorm1d(1280)\n",
    "        self.fc2 = nn.Linear(1280, 32)\n",
    "        # self.fc2_bn = nn.BatchNorm1d(128)\n",
    "\t\t# self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Spatial reduction\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Regression\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.fc2_bn(x)\n",
    "\t\t# x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return {'output': x}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class ReservoirBuffer:\n",
    "    def __init__(self, capacity=1000, replay_ratio=0.2, input_shape=(3, 384, 216), label_shape=(9,), device=torch.device('cpu')):\n",
    "        self.capacity = capacity\n",
    "        self.inputs = torch.empty((capacity, *input_shape), dtype=torch.float32, device=device)\n",
    "        self.labels = torch.empty((capacity, *label_shape), dtype=torch.float32, device=device)\n",
    "        self.domains = [None] * capacity\n",
    "        self.size = 0          # Number of samples currently in buffer\n",
    "        self.num_seen = 0      # Total samples seen\n",
    "        self.replay_ratio = replay_ratio\n",
    "\n",
    "    def add(self, new_samples):\n",
    "        for sample in new_samples:\n",
    "            self.num_seen += 1\n",
    "            if self.size < self.capacity:\n",
    "                idx = self.size\n",
    "                self.size += 1\n",
    "            else:\n",
    "                idx = random.randint(0, self.num_seen - 1)\n",
    "                if idx >= self.capacity:\n",
    "                    continue\n",
    "            self.inputs[idx].copy_(sample[0])\n",
    "            self.labels[idx].copy_(sample[1])\n",
    "            self.domains[idx] = sample[2]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.size == 0:\n",
    "            return []\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return [(self.inputs[i], self.labels[i], self.domains[i]) for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_domain_distribution(self):\n",
    "        return pd.Series(self.domains[:self.size]).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRehearsalBuffer:\n",
    "    \"\"\"\n",
    "    @inproceedings{Hsu18_EvalCL,\n",
    "        title={Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},\n",
    "        author={Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},\n",
    "        booktitle={NeurIPS Continual learning Workshop },\n",
    "        year={2018},\n",
    "        url={https://arxiv.org/abs/1810.12488}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.domain_buffer = {}\n",
    "\n",
    "    def update_buffer(self, domain, dataset):\n",
    "        # Add/overwrite current domain\n",
    "        self.domain_buffer[domain] = Subset(dataset, torch.arange(len(dataset)))\n",
    "        \n",
    "        # Recalculate quota\n",
    "        num_domains = len(self.domain_buffer)\n",
    "        buffer_quota_per_domain = self.buffer_size // num_domains\n",
    "        \n",
    "        # Reduce all domains (including current)\n",
    "        for domain in self.domain_buffer:\n",
    "            subset = self.domain_buffer[domain]\n",
    "            max_safe_samples_to_overwrite = min(buffer_quota_per_domain, len(subset.dataset))\n",
    "            rand_indices = torch.randperm(len(dataset))[:max_safe_samples_to_overwrite].numpy()\n",
    "            self.domain_buffer[domain] = Subset(dataset, rand_indices)\n",
    "\n",
    "    def get_loader_with_replay(self, current_domain, current_loader):\n",
    "        current_dataset = current_loader.dataset\n",
    "        replay_datasets = [dataset for domain, dataset in self.domain_buffer.items() if domain != current_domain]\n",
    "\n",
    "        #Enforces 1:1 ratio when current â‰¥ buffer\n",
    "        total_replay = sum(len(dataset) for dataset in replay_datasets)\n",
    "        if total_replay > 0:\n",
    "            K = max(len(current_dataset) // total_replay, 1)\n",
    "            replay_datasets = replay_datasets * K\n",
    "\n",
    "        combined_dataset =  ConcatDataset(replay_datasets + [current_dataset])\n",
    "        combined_dataset = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=current_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=current_loader.num_workers,\n",
    "            pin_memory=current_loader.pin_memory,\n",
    "            drop_last=current_loader.drop_last\n",
    "        )\n",
    "        return combined_dataset\n",
    "    \n",
    "    def get_domain_distribution(self):\n",
    "        \"\"\"Returns {domain: num_samples} without needing Storage\"\"\"\n",
    "        return {domain: len(subset) for domain, subset in self.domain_buffer.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return x\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -grad_output\n",
    "\n",
    "class GradientReversal(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x)\n",
    "\n",
    "\n",
    "class DualBranchNet(nn.Module):\n",
    "    def __init__(self, num_outputs=9, num_domains=6, weights_init=False, weights_norm=False):\n",
    "        super().__init__()\n",
    "\n",
    "        def linear(in_f, out_f):\n",
    "            layer = nn.Linear(in_f, out_f)\n",
    "            return torch.nn.utils.parametrizations.weight_norm(layer, dim=0) if weights_norm else layer \n",
    "        \n",
    "        def layer_norm(dim):\n",
    "            return nn.Identity() if weights_norm else nn.LayerNorm(dim)\n",
    "\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feature_dim = 1280\n",
    "\n",
    "        self.invariant = nn.Sequential(\n",
    "            linear(self.feature_dim, 256),\n",
    "            GradientReversal(),\n",
    "            layer_norm(256),\n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "\n",
    "        self.invariant_domain_classifier = nn.Sequential(\n",
    "            layer_norm(256),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.specific = nn.Sequential(\n",
    "            linear(self.feature_dim, 256),\n",
    "            layer_norm(256), \n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.specific_domain_classifier = nn.Sequential(\n",
    "            layer_norm(256),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "        if weights_init:\n",
    "            self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        custom_modules = [\n",
    "            'invariant',\n",
    "            'invariant_domain_classifier',\n",
    "            'specific',\n",
    "            'specific_domain_classifier',\n",
    "            'head'\n",
    "        ]\n",
    "        for name in custom_modules:\n",
    "            module = getattr(self, name, None)\n",
    "            for m in module.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.backbone(x)\n",
    "        base = self.pool(base).view(x.size(0), -1)\n",
    "        \n",
    "        invariant_feats = self.invariant(base)\n",
    "        invariant_domain_pred = self.invariant_domain_classifier(invariant_feats)\n",
    "\n",
    "        specific_feats = self.specific(base)\n",
    "        specific_domain_pred = self.specific_domain_classifier(specific_feats)\n",
    "\n",
    "        combined = torch.cat([invariant_feats, specific_feats], dim=1)\n",
    "        \n",
    "        scores = self.head(combined)      \n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': invariant_domain_pred,\n",
    "            'specific_domain': specific_domain_pred,\n",
    "            'invariant_feats': invariant_feats,\n",
    "            'specific_feats': specific_feats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet_deep(DualBranchNet):\n",
    "    \"\"\"thicker invariant network to learn features instead of realying on the backbone\"\"\"\n",
    "    def __init__(self, num_outputs=9, num_domains=6, weights_init=True, weights_norm=False):\n",
    "        super().__init__(num_outputs, num_domains, weights_init, weights_norm)\n",
    "\n",
    "        def linear(in_f, out_f):\n",
    "            layer = nn.Linear(in_f, out_f)\n",
    "            return torch.nn.utils.parametrizations.weight_norm(layer, dim=0) if weights_norm else layer \n",
    "\n",
    "        self.invariant = nn.Sequential(\n",
    "            linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "\n",
    "        self.invariant_domain_classifier = nn.Sequential(\n",
    "            GradientReversal(),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849e928",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79536968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def extract_data_subset(df, robot_name='Pepper'):\n",
    "    # Modify the saving section at the end\n",
    "    # Create the required directory structure\n",
    "    project_root = Path.cwd().parent  # Goes up from experiments/ to project/\n",
    "    data_dir = project_root / \"data\"\n",
    "    images_dir = data_dir / \"images\"\n",
    "\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Filter for Pepper first\n",
    "    pepper_df = df[df['robot'] == robot_name].copy()\n",
    "    pepper_df = pepper_df.reset_index(drop=True)\n",
    "\n",
    "    # Copy images and update paths\n",
    "    def copy_and_update_path(row):\n",
    "        src_path = Path(row['image_path'])\n",
    "        dst_path = images_dir / src_path.name\n",
    "        \n",
    "        if not dst_path.exists():\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        \n",
    "        return str(dst_path.relative_to(project_root))\n",
    "\n",
    "    pepper_df['image_path'] = pepper_df.apply(copy_and_update_path, axis=1)\n",
    "\n",
    "    # Save the filtered dataframe\n",
    "    pepper_df.to_pickle(data_dir / \"pepper_data.pkl\")\n",
    "    return pepper_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "df['image_path'] = '../' + df['image_path']\n",
    "\n",
    "# Create domain-specific dataloaders\n",
    "domains = df['domain'].unique()\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    #domain_df = domain_df.sample(frac=0.5, random_state=42)\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128))  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f961279",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b327ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LGRBaseline\n",
    "def baseline_batch(model, batch, device, **kwargs):\n",
    "    inputs, labels, _ = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)['output']\n",
    "    loss = kwargs['mse_criterion'](outputs, labels)\n",
    "    metrics = {}\n",
    "    return loss, metrics\n",
    "\n",
    "# For DualBranchNet\n",
    "def dualbranch_batch(model, batch, device, **kwargs):\n",
    "    inputs, labels, domain_labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    domain_to_idx = kwargs['domain_to_idx']\n",
    "    domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    ce_criterion = kwargs['ce_criterion']\n",
    "    cos_criterion = kwargs['cos_criterion']\n",
    "\n",
    "    # Split batch\n",
    "    current_domain = kwargs['current_domain']\n",
    "    current_mask = (domain_labels == domain_to_idx[current_domain])\n",
    "    replay_mask = ~current_mask\n",
    "\n",
    "    # 1. Current samples: update all parameters\n",
    "    if current_mask.any():\n",
    "        inputs_current = inputs[current_mask]\n",
    "        labels_current = labels[current_mask]\n",
    "        domain_labels_current = domain_labels[current_mask]\n",
    "\n",
    "        outputs_current = model(inputs_current)\n",
    "        inv_feats = outputs_current['invariant_feats']\n",
    "        spec_feats = outputs_current['specific_feats']\n",
    "\n",
    "        task_loss = mse_criterion(outputs_current['output'], labels_current)\n",
    "        inv_domain_loss = ce_criterion(outputs_current['invariant_domain'], domain_labels_current)\n",
    "        spec_domain_loss = ce_criterion(outputs_current['specific_domain'], domain_labels_current)\n",
    "        similarity_loss = cos_criterion(inv_feats, spec_feats)\n",
    "        \n",
    "        total_loss = (task_loss +\n",
    "                      0.5 * inv_domain_loss +\n",
    "                      0.2 * spec_domain_loss +\n",
    "                      0.1 * similarity_loss)\n",
    "        \n",
    "        total_loss.backward(retain_graph=True)\n",
    "        \n",
    "        inv_acc = (outputs_current['invariant_domain'].argmax(1) == domain_labels_current).float().mean().item()\n",
    "        spec_acc = (outputs_current['specific_domain'].argmax(1) == domain_labels_current).float().mean().item()\n",
    "    else:\n",
    "        total_loss = torch.tensor(0.0, device=device)\n",
    "        inv_acc = 0.0\n",
    "        spec_acc = 0.0\n",
    "        task_loss = torch.tensor(0.0, device=device)\n",
    "        inv_domain_loss = torch.tensor(0.0, device=device)\n",
    "        spec_domain_loss = torch.tensor(0.0, device=device)\n",
    "        similarity_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # 2. Replay samples: update only specific branch + head\n",
    "    if replay_mask.any():\n",
    "        inputs_replay = inputs[replay_mask]\n",
    "        labels_replay = labels[replay_mask]\n",
    "        domain_labels_replay = domain_labels[replay_mask]\n",
    "\n",
    "        #no_grad, unlike requires_grad=False, detaches all elements from the gradient computation graph\n",
    "        with torch.no_grad():\n",
    "            base_replay = model.backbone(inputs_replay)\n",
    "            base_replay = model.pool(base_replay).flatten(1)\n",
    "            inv_feats_replay = model.invariant(base_replay)\n",
    "\n",
    "        specific_feats = model.specific(base_replay)\n",
    "        spec_domain_pred = model.specific_domain_classifier(specific_feats)\n",
    "\n",
    "        combined = torch.cat([inv_feats_replay, specific_feats], dim=1)  \n",
    "        scores = model.head(combined)\n",
    "        \n",
    "        task_loss_replay = mse_criterion(scores, labels_replay)\n",
    "        spec_domain_loss_replay = ce_criterion(spec_domain_pred, domain_labels_replay)\n",
    "        total_loss_replay = task_loss_replay + 0.2 * spec_domain_loss_replay\n",
    "        \n",
    "        total_loss_replay.backward()\n",
    "    \n",
    "    metrics = {\n",
    "        'task_loss': task_loss.item(),\n",
    "        'inv_domain': inv_domain_loss.item(),\n",
    "        'spec_domain': spec_domain_loss.item(),\n",
    "        'similarity': similarity_loss.item(),\n",
    "        'inv_acc': inv_acc,\n",
    "        'spec_acc': spec_acc,\n",
    "        'replay_count': replay_mask.sum().item(),\n",
    "        'current_count': current_mask.sum().item()\n",
    "    }\n",
    "    return total_loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6df3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in dataloader:\n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs)['output']\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def cross_domain_validation(model, domain_dataloaders, criterion, device):\n",
    "    results = {}\n",
    "    for domain, loaders in domain_dataloaders.items():\n",
    "        val_loader = loaders['val']\n",
    "        val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        results[domain] = val_loss\n",
    "    return results\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    # metrics_list: list of dicts, each dict contains metrics for a batch\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    avg_metrics = {}\n",
    "    for k in keys:\n",
    "        avg_metrics[k] = float(np.mean([m[k] for m in metrics_list if k in m]))\n",
    "    return avg_metrics\n",
    "\n",
    "def collect_tsne_features(model, loader, device):\n",
    "    model.eval()\n",
    "    all_inv, all_spec, all_domains = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for domain, loaders in domain_dataloaders.items():\n",
    "            loader = loaders['val']\n",
    "            for x, _, d in loader:\n",
    "                x = x.to(device)\n",
    "                out = model(x)\n",
    "                all_inv.append(out['invariant_feats'].cpu())\n",
    "                all_spec.append(out['specific_feats'].cpu())\n",
    "                all_domains += list(d)\n",
    "    inv_feats = torch.cat(all_inv, dim=0).numpy()\n",
    "    spec_feats = torch.cat(all_spec, dim=0).numpy()\n",
    "    return inv_feats, spec_feats, all_domains\n",
    "\n",
    "def collect_gradients(model):\n",
    "    grad_norms = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and not name.startswith(\"backbone\"):\n",
    "            module = name.split('.')[0]\n",
    "            norm = param.grad.norm(2).item()\n",
    "            if module not in grad_norms:\n",
    "                grad_norms[module] = []\n",
    "            grad_norms[module].append(norm)\n",
    "    # Take mean per module\n",
    "    grad_norms = {k: float(np.mean(v)) for k, v in grad_norms.items()}\n",
    "    return grad_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unified_train_loop(\n",
    "    model, domains, domain_dataloaders, buffer, optimizer, writer, device,\n",
    "    batch_fn, batch_kwargs, num_epochs=5, exp_name=\"exp\", gradient_clipping=False\n",
    "):\n",
    "    global_step = 0\n",
    "    history = {\n",
    "        'train_epoch_loss': [],\n",
    "        'val_epoch_loss': [],\n",
    "        'train_epoch_metrics': [],\n",
    "        'cross_domain_val': [],\n",
    "        'grad_norms': [],\n",
    "    }\n",
    "    for domain_idx, current_domain in enumerate(domains):\n",
    "        print(buffer.get_domain_distribution())\n",
    "        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            samples = 0\n",
    "            batch_metrics_list = []\n",
    "            \n",
    "            # for batch_idx, batch in enumerate(train_loader):\n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Domain {current_domain} Epoch {epoch+1}/{num_epochs}\")):\n",
    "                optimizer.zero_grad()\n",
    "                loss, metrics = batch_fn(model, batch, device, **{**batch_kwargs, 'current_domain': current_domain})\n",
    "                if gradient_clipping:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                batch_size = batch[0].size(0)\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                samples += batch_size\n",
    "                global_step += 1\n",
    "                batch_metrics_list.append(metrics)\n",
    "                # TensorBoard logging (every 10 batches)\n",
    "                if writer and batch_idx % 10 == 0:\n",
    "                    writer.add_scalar(f'{exp_name}/train_loss', loss.item(), global_step)\n",
    "                    for k, v in metrics.items():\n",
    "                        writer.add_scalar(f'{exp_name}/train_{k}', v, global_step)\n",
    "            avg_epoch_loss = epoch_loss / samples\n",
    "            writer.add_scalar(f'{exp_name}/train_epoch_loss', avg_epoch_loss, global_step)\n",
    "            history['train_epoch_loss'].append(avg_epoch_loss)\n",
    "            # Average batch metrics for this epoch\n",
    "            avg_metrics = average_metrics(batch_metrics_list)\n",
    "            history['train_epoch_metrics'].append(avg_metrics)\n",
    "\n",
    "            # Collect gradients\n",
    "            grad_norms = collect_gradients(model)\n",
    "            history['grad_norms'].append(grad_norms)\n",
    "\n",
    "            # Validation on current domain\n",
    "            val_loss = evaluate_model(model, domain_dataloaders[current_domain]['val'], batch_kwargs['mse_criterion'], device)\n",
    "            writer.add_scalar(f'{exp_name}/val_epoch_loss', val_loss, global_step)\n",
    "            history['val_epoch_loss'].append(val_loss)\n",
    "\n",
    "            # Collect data for t-SNE domain separation graphs\n",
    "            inv_feats, spec_feats, domain_labels = collect_tsne_features(model, domain_dataloaders, device)\n",
    "            tsne_data = {\n",
    "                'inv_feats': inv_feats,\n",
    "                'spec_feats': spec_feats,\n",
    "                'domain_labels': domain_labels\n",
    "            }\n",
    "\n",
    "            # Cross-domain validation (after each domain)\n",
    "            if epoch == num_epochs-1:\n",
    "                cross_val = cross_domain_validation(model, domain_dataloaders, batch_kwargs['mse_criterion'], device)\n",
    "                history['cross_domain_val'].append(cross_val)\n",
    "\n",
    "            # Save model and metrics\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'history': history,\n",
    "                'tsne' : tsne_data,\n",
    "            }, f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pt\")\n",
    "            with open(f\"../checkpoints/{exp_name}_history.pkl\", \"wb\") as f:\n",
    "                pickle.dump(history, f)\n",
    "            \n",
    "        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "# # For baseline model\n",
    "# baseline_model = LGRBaseline().to(device)\n",
    "# optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "# buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "# exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "# baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "# unified_train_loop(\n",
    "#     model=baseline_model,\n",
    "#     domains=domains,\n",
    "#     domain_dataloaders=domain_dataloaders,\n",
    "#     buffer=buffer,\n",
    "#     optimizer=optimizer,\n",
    "#     writer=writer,\n",
    "#     device=device,\n",
    "#     batch_fn=baseline_batch,\n",
    "#     batch_kwargs=baseline_kwargs,\n",
    "#     num_epochs=10,\n",
    "#     exp_name=exp_name\n",
    "# )\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9dfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_norm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36adb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for potential ways to combine networks\n",
    "residual = SpecificHead(base + invariant_feats)\n",
    "specific_feats = invariant_feats + residual\n",
    "\n",
    "gate = GateNet(base + invariant_feats)\n",
    "residual = SpecificHead(base)\n",
    "specific_feats = invariant_feats + gate * residual\n",
    "\n",
    "residual = Attention(invariant_feats, base)\n",
    "specific_feats = invariant_feats + residual\n",
    "\n",
    "gamma, beta = SpecificHead(base)\n",
    "specific_feats = gamma * invariant_feats + beta\n",
    "\n",
    "invariant_feats = InvariantHead(base)\n",
    "specific_feats = SpecificHead(base)\n",
    "cos_sim(invariant_feats, specific_feats) ~ 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e848bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different cos 1-abs fixed residual - fixedred_fixedcos_dualbranchmodel_20250604_014811_history\n",
    "#fixed res but old cosine ^2 - fixedred_dualbranchmodel_20250604_002951_history\n",
    "#broken baseline - baselinemodel_20250603_200948_history\n",
    "# recent dualbranch - dualbranchmodel_20250603_135728_history\n",
    "#normalised and standarised residual - dualbranchmodel_20250609_013632_history\n",
    "#standarised residual - nonorm_dualbranchmodel_20250609_131820_history\n",
    "#no residual connection - nores_dualbranchmodel_20250609_232842_history\n",
    "#same plus gradient clipping - nores_gradclip_dualbranchmodel_20250610_010647_history\n",
    "#same, no graidient clipping, weights initialisation and normalisation - nores_winit_wnorm_dualbranchmodel_20250610_024401_history\n",
    "#same, gradient clipping and weights optimisation - nores_gradclip_winit_wnorm_dualbranchmodel_20250610_042118_history\n",
    "\n",
    "with open('../checkpoints/nonorm_dualbranchmodel_20250609_131820_history.pkl', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_dualbranchmodel_20250609_232842_history.pkl', 'rb') as f:\n",
    "    history_nores = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_gradclip_dualbranchmodel_20250610_010647_history.pkl', 'rb') as f:\n",
    "    history_nores_gradclip = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/dualbranchmodel_20250609_013632_history.pkl', 'rb') as f:\n",
    "    history_lnorm = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_winit_wnorm_dualbranchmodel_20250610_024401_history.pkl', 'rb') as f:\n",
    "    history_nores_wnorm = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_gradclip_winit_wnorm_dualbranchmodel_20250610_042118_history.pkl', 'rb') as f:\n",
    "    history_nores_gradclip_wnorm = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c9190dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deeper invariant network - deep_norm_dualbranchmodel_20250610_223350_history\n",
    "#same but with weiths normalisation - deep_dualbranchmodel_20250610_205411_history\n",
    "\n",
    "with open('../checkpoints/deep_norm_dualbranchmodel_20250610_223350_history.pkl', 'rb') as f:\n",
    "    deep_norm = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/deep_dualbranchmodel_20250610_205411_history.pkl', 'rb') as f:\n",
    "    deep = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_file = torch.load('../checkpoints/dualbranchmodel_20250609_013632_domainSmallOffice_epoch9_step1300.pt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "model.load_state_dict(pt_file['model_state_dict'])\n",
    "history = pt_file['history']\n",
    "tsne_data = pt_file['tsne']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bf879",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf83e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e13900",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "'deep_norm': deep_norm,\n",
    "'deep': deep\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'history': history,\n",
    "'history_nores': history_nores,\n",
    "'history_nores_gradclip': history_nores_gradclip,\n",
    "'history_lnorm': history_lnorm,\n",
    "'history_nores_wnorm': history_nores_wnorm,\n",
    "'history_nores_gradclip_wnorm': history_nores_gradclip_wnorm\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f792f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,7))\n",
    "for name, model in models.items():\n",
    "    plt.plot(model['train_epoch_loss'], label=f'{name} Train Loss')\n",
    "    plt.plot(model['val_epoch_loss'], label=f'{name} Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(-0.01, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2374ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in models.items():\n",
    "    filtered = [x for i, x in enumerate(m['cross_domain_val']) if i in [9, 19, 29, 39, 49, 58]]\n",
    "    m['cross_domain_val'] = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513f9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in models.items():\n",
    "    print(len(m['cross_domain_val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in models.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c42bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for name, model in models.items():\n",
    "    for i in range(6):\n",
    "        final += list(model['cross_domain_val'][i].values())\n",
    "max_loss = max(final)\n",
    "min_loss = min(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = models['deep']['cross_domain_val']\n",
    "\n",
    "# Extract domain names\n",
    "domains = list(history[0].keys())\n",
    "\n",
    "# Prepare accuracy per domain over time\n",
    "domain_scores = {domain: [] for domain in domains}\n",
    "for snapshot in history:\n",
    "    for domain in domains:\n",
    "        domain_scores[domain].append(snapshot[domain])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for domain, scores in domain_scores.items():\n",
    "    plt.plot(domains[:len(scores)], scores, label=domain, marker='o')\n",
    "\n",
    "plt.xlabel(\"After training on domain X\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Domain-wise Accuracy Over Time\")\n",
    "# plt.ylim(min(-0.1, min_loss), max_loss)\n",
    "# plt.ylim(-0.05, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92afd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = models['deep_norm']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot([m['similarity'] for m in h['train_epoch_metrics']])\n",
    "plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.title('cosine similarity of two branches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = models['deep_norm']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['inv_acc', 'spec_acc']:\n",
    "    plt.plot([m[metric] for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.title('Branch accuracies and their similarity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321bd830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [m['spec_domain'] for m in history['train_epoch_metrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = models['deep']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['inv_domain', 'spec_domain', 'task_loss']:\n",
    "    plt.plot([m[metric] for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.ylim(-0.01, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5599bcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = models['deep']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['replay_count', 'current_count']:\n",
    "    plt.plot([m[metric]/32*100 for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.title('Type of samples in batch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('% of batch')\n",
    "# plt.axhline(32, color='r')\n",
    "# plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = models['deep_norm']\n",
    "\n",
    "print(h['grad_norms'][0].keys())\n",
    "for grad_dict in h['grad_norms']:\n",
    "    for key in h['grad_norms'][0].keys():\n",
    "        grad_dict.setdefault(key, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "# for module in ['invariant', 'specific_residual', 'domain_classifier']:\n",
    "for module in h['grad_norms'][0].keys():\n",
    "    plt.plot([m[f'{module}'] for m in h['grad_norms']], label=module)\n",
    "plt.title('Gradient Norms by Module')\n",
    "# plt.ylim(-0.001, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c851e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97530f3f",
   "metadata": {},
   "source": [
    "## --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "# 1. Gather all checkpoint files\n",
    "checkpoint_files = glob.glob(\"../checkpoints/dualbranchmodel_20250609_013632_*.pt\")\n",
    "\n",
    "# 2. Parse out the step value and sort\n",
    "pattern = re.compile(r\"_step(\\d+)\\.pt\")\n",
    "files_with_steps = []\n",
    "for f in checkpoint_files:\n",
    "    match = pattern.search(f)\n",
    "    if match:\n",
    "        step = int(match.group(1))\n",
    "        files_with_steps.append((step, f))\n",
    "files_with_steps.sort()  # Sort by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb858a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Precompute t-SNE for all checkpoints (0-59)\n",
    "tsne_projections = []\n",
    "for idx, (step, ckpt_file) in enumerate(tqdm(files_with_steps, desc=\"Processing checkpoints\")):\n",
    "    ckpt = torch.load(ckpt_file, map_location='cpu')\n",
    "    data = ckpt['tsne']\n",
    "    inv_feats = np.array(data['inv_feats'])\n",
    "    spec_feats = np.array(data['spec_feats'])\n",
    "    domain_labels = np.array(data['domain_labels'])\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    inv_2d = tsne.fit_transform(inv_feats)\n",
    "    spec_2d = tsne.fit_transform(spec_feats)\n",
    "\n",
    "    tsne_projections.append({\n",
    "        'timeline_idx': idx,  # 0 to 59\n",
    "        'inv_2d': inv_2d,\n",
    "        'spec_2d': spec_2d,\n",
    "        'domains': domain_labels,\n",
    "        'filename': ckpt_file\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# State variable for current index\n",
    "current_idx = 0\n",
    "\n",
    "# Output widget for the plot\n",
    "out = widgets.Output()\n",
    "\n",
    "# Buttons\n",
    "button_prev = widgets.Button(description=\"Previous\")\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "\n",
    "# Precompute limits\n",
    "all_x = np.concatenate([d['inv_2d'][:,0] for d in tsne_projections] + [d['spec_2d'][:,0] for d in tsne_projections])\n",
    "all_y = np.concatenate([d['inv_2d'][:,1] for d in tsne_projections] + [d['spec_2d'][:,1] for d in tsne_projections])\n",
    "x_min, x_max = all_x.min(), all_x.max()\n",
    "y_min, y_max = all_y.min(), all_y.max()\n",
    "\n",
    "def plot_epoch(timeline_idx):\n",
    "    data = tsne_projections[timeline_idx]\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=True)\n",
    "    domain_to_int = {name: i for i, name in enumerate(domains)}\n",
    "    domain_ints = np.array([domain_to_int[name] for name in data['domains']])\n",
    "    scatter1 = ax1.scatter(data['inv_2d'][:,0], data['inv_2d'][:,1], \n",
    "                          c=domain_ints, cmap='tab10', alpha=0.7, vmin=0, vmax=len(domains)-1)\n",
    "    ax1.set_title(f\"Invariant Features - Timeline {timeline_idx}\")\n",
    "    ax1.set_xlim(x_min, x_max)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    scatter2 = ax2.scatter(data['spec_2d'][:,0], data['spec_2d'][:,1],\n",
    "                          c=domain_ints, cmap='tab10', alpha=0.7, vmin=0, vmax=len(domains)-1)\n",
    "    ax2.set_title(f\"Specific Features - Timeline {timeline_idx}\")\n",
    "    ax2.set_xlim(x_min, x_max)\n",
    "    ax2.set_ylim(y_min, y_max)\n",
    "    cbar = fig.colorbar(scatter1, ax=[ax1, ax2], label='Domain', \n",
    "                        ticks=np.arange(len(domains)), boundaries=np.arange(len(domains)+1)-0.5)\n",
    "    cbar.set_ticks(np.arange(len(domains)))\n",
    "    cbar.set_ticklabels(domains)\n",
    "    plt.show()\n",
    "\n",
    "def on_prev_clicked(b):\n",
    "    global current_idx\n",
    "    if current_idx > 0:\n",
    "        current_idx -= 1\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plot_epoch(current_idx)\n",
    "\n",
    "def on_next_clicked(b):\n",
    "    global current_idx\n",
    "    if current_idx < len(tsne_projections) - 1:\n",
    "        current_idx += 1\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plot_epoch(current_idx)\n",
    "\n",
    "button_prev.on_click(on_prev_clicked)\n",
    "button_next.on_click(on_next_clicked)\n",
    "\n",
    "# Display everything\n",
    "display(widgets.HBox([button_prev, button_next]))\n",
    "display(out)\n",
    "\n",
    "# Initial plot\n",
    "with out:\n",
    "    plot_epoch(current_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870cd48",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895de6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model output\n",
    "outputs = model(inputs)  # Should be in [1,5]\n",
    "print(f\"Output range: {outputs.min().item()}â€“{outputs.max().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LGRBaseline().to(device)\n",
    "test_optimizer = optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels, _ = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)\n",
    "\n",
    "# %%\n",
    "num_test_epochs = 400\n",
    "for epoch in range(num_test_epochs):\n",
    "    test_optimizer.zero_grad()\n",
    "\n",
    "    outputs = test_model(inputs)\n",
    "    loss = criterion(outputs['output'], labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    test_optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Overfit Epoch {epoch+1}/{num_test_epochs} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# device = torch.device('cpu')torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "\n",
    "writer = SummaryWriter(\"visualisation/\")\n",
    "model = DualBranchNet().to(device)\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels, _ = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)\n",
    "# writer.add_graph(model, inputs)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = [\"image\"]\n",
    "output_names = [\"appropriateness scores\"]\n",
    "\n",
    "torch.onnx.export(model, inputs, \"model.onnx\", input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "y = model(inputs)\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cadf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Get a single batch from any domain's train loader\n",
    "domain = domains[0]\n",
    "single_batch = next(iter(domain_dataloaders[domain]['train']))\n",
    "\n",
    "\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "\n",
    "# 4. Overfit loop for both models\n",
    "def overfit_model(\n",
    "    model, optimizer, batch_fn, batch_kwargs, device, num_epochs=100, exp_name=\"overfit\"\n",
    "):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss, metrics = batch_fn(model, single_batch, device, **batch_kwargs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Baseline model overfit\n",
    "baseline_model = LGRBaseline().to(device)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "baseline_losses = overfit_model(\n",
    "    baseline_model, optimizer, baseline_batch, {'mse_criterion': torch.nn.MSELoss()}, device\n",
    ")\n",
    "\n",
    "# 6. DualBranch model overfit\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': torch.nn.MSELoss(),\n",
    "    'ce_criterion': torch.nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': lambda a, b: (torch.nn.CosineSimilarity()(a, b) ** 2).mean(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'current_domain': domain\n",
    "}\n",
    "dualbranch_losses = overfit_model(\n",
    "    dual_model, optimizer, dualbranch_batch, dualbranch_kwargs, device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot the loss curves (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(baseline_losses, label='Baseline')\n",
    "plt.plot(dualbranch_losses, label='DualBranch')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Overfitting to a Single Batch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa9229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
