{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee38c",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import datetime\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# DATASET_DIR = (Path(\"..\") / \"..\" / \"datasets\").resolve()\n",
    "# DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n",
    "# LABEL_COLS = [\n",
    "#     \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n",
    "#     \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n",
    "#     \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n",
    "# ]\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "# from data_processing.data_processing import create_crossvalidation_loaders\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.buffers import NaiveRehearsalBuffer\n",
    "from data_processing.data_processing import get_domain_dataloaders\n",
    "from models.training_utils import heuristic_dualbranch_batch, unified_train_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c5d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca986",
   "metadata": {},
   "source": [
    "## CL Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for potential ways to combine networks\n",
    "# residual = SpecificHead(base + invariant_feats)\n",
    "# specific_feats = invariant_feats + residual\n",
    "\n",
    "# gate = GateNet(base + invariant_feats)\n",
    "# residual = SpecificHead(base)\n",
    "# specific_feats = invariant_feats + gate * residual\n",
    "\n",
    "# residual = Attention(invariant_feats, base)\n",
    "# specific_feats = invariant_feats + residual\n",
    "\n",
    "# gamma, beta = SpecificHead(base)\n",
    "# specific_feats = gamma * invariant_feats + beta\n",
    "\n",
    "# invariant_feats = InvariantHead(base)\n",
    "# specific_feats = SpecificHead(base)\n",
    "# cos_sim(invariant_feats, specific_feats) ~ 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184030",
   "metadata": {},
   "source": [
    "### dualbranch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_layer_size(input, output, n_layers):\n",
    "    start_exp = (output + 1).bit_length()\n",
    "    end_exp = (input - 1).bit_length()\n",
    "    \n",
    "    total_powers = end_exp - start_exp\n",
    "    if total_powers < n_layers:\n",
    "        return None\n",
    "\n",
    "    result = []\n",
    "    denominator = n_layers + 1\n",
    "    half_denominator = denominator // 2\n",
    "\n",
    "    for i in range(1, n_layers + 1):\n",
    "        numerator = i * total_powers + half_denominator #works same as rounding\n",
    "        idx = numerator // denominator\n",
    "        power = 1 << (start_exp + idx)\n",
    "        result.append(power)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "intermediate_layer_size(1280+64, 9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d27c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class CLIPEncoderWrapper(nn.Module):\n",
    "    \"\"\"Wrapper for the forward .encode_image() function of the CLIP encoder\"\"\"\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.clip_model = clip_model\n",
    "        for p in self.clip_model.parameters():\n",
    "            p.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.clip_model.encode_image(x)\n",
    "\n",
    "class DualBranchModel(nn.Module):\n",
    "    def __init__(self, num_outputs=9, dropout_rate=0.3, setup={'branch':'mobilenetv2'}, clip_model=None, freeze_branches=False):\n",
    "        assert not (setup['branch'] == 'clip' and clip_model is None), \"clip_model must be provided for CLIP branch\"\n",
    "        assert not (setup['branch'] == 'clip' and freeze_branches is False), \"CLIP branch will be frozen regardless of freeze_branches\"\n",
    "        super(DualBranchModel, self).__init__()\n",
    "        self.setup = setup\n",
    "        self.freeze_branches = freeze_branches\n",
    "\n",
    "        if self.setup['branch'] == 'resnet18':\n",
    "            model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            branch = nn.Sequential(\n",
    "                *list(model.children())[:-2],\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            branch_feature_dim = 512\n",
    "        elif self.setup['branch'] == 'mobilenetv2':\n",
    "            model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "            branch = nn.Sequential(\n",
    "                model,\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            branch_feature_dim = 1280\n",
    "        elif self.setup['branch'] == 'efficientnetb0':\n",
    "            model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1).features\n",
    "            branch = nn.Sequential(\n",
    "                model,\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            branch_feature_dim = 1280\n",
    "        elif self.setup['branch'] == 'clip':\n",
    "            branch = CLIPEncoderWrapper(clip_model)\n",
    "            branch_feature_dim = 512\n",
    "\n",
    "        # Optional freeze params\n",
    "        if self.freeze_branches:\n",
    "            for p in branch.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        # If using frozen CLIP reusing the model is more efficient\n",
    "        if self.setup['branch'] == 'clip':\n",
    "            self.social_branch = branch\n",
    "            self.env_branch = branch\n",
    "        else:\n",
    "            self.social_branch = branch\n",
    "            self.env_branch = copy.deepcopy(branch)\n",
    "\n",
    "        soc_feature_dim = branch_feature_dim\n",
    "        env_feature_dim = branch_feature_dim\n",
    "\n",
    "        # Override one of the branches to run ablations\n",
    "        if self.setup.get('env') == 'ablated':\n",
    "            env_feature_dim = 0\n",
    "\n",
    "\n",
    "        self.fusion_dim = soc_feature_dim + env_feature_dim\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.fusion_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(128, num_outputs)\n",
    "        )\n",
    "        \n",
    "    def forward(self, social_imgs, env_imgs=None):\n",
    "        if env_imgs is None:\n",
    "            env_features = torch.zeros(social_imgs.size(0), 0, device=social_imgs.device, dtype=social_imgs.dtype)\n",
    "        else:\n",
    "            env_features = self.env_branch(env_imgs)\n",
    "        \n",
    "        social_features = self.social_branch(social_imgs)\n",
    "\n",
    "        fused_features = torch.cat([social_features, env_features], dim=1)\n",
    "        scores = self.head(fused_features)\n",
    "\n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_feats': social_features,\n",
    "            'specific_feats': env_features\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849e928",
   "metadata": {},
   "source": [
    "### K Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_fold_histories(fold_history):\n",
    "#     \"\"\"\n",
    "#     Combine multiple training histories by averaging them element-wise.\n",
    "#     Handles both simple lists and lists of dictionaries.\n",
    "#     \"\"\"\n",
    "#     combined_history = {}\n",
    "#     metric_keys = list(fold_history[0].keys())\n",
    "    \n",
    "#     for key in metric_keys:\n",
    "#         # Check if this metric contains dictionaries\n",
    "#         first_element = fold_history[0][key][0] if fold_history[0][key] else None\n",
    "        \n",
    "#         if isinstance(first_element, dict):\n",
    "#             # Handle lists of dictionaries (train_epoch_metrics, grad_norms)\n",
    "#             combined_history[key] = average_list_of_dicts(fold_history, key)\n",
    "#         else:\n",
    "#             # Handle simple lists (train_epoch_loss, val_epoch_loss, cross_domain_val)\n",
    "#             stacked_metrics = np.stack([fold_history[fold][key] for fold in fold_history])\n",
    "#             combined_history[key] = np.mean(stacked_metrics, axis=0).tolist()\n",
    "    \n",
    "#     return combined_history\n",
    "\n",
    "# def average_list_of_dicts(fold_history, metric_key):\n",
    "#     \"\"\"\n",
    "#     Average a list of dictionaries across folds.\n",
    "#     \"\"\"\n",
    "#     # Get the dictionary keys from the first fold's first epoch\n",
    "#     dict_keys = list(fold_history[0][metric_key][0].keys())\n",
    "    \n",
    "#     # Convert each fold's list of dicts to a 2D numpy array\n",
    "#     fold_arrays = []\n",
    "#     for fold in fold_history:\n",
    "#         # Convert list of dicts to 2D array: [epochs, sub_metrics]\n",
    "#         fold_array = np.array([[epoch_dict[k] for k in dict_keys] \n",
    "#                               for epoch_dict in fold_history[fold][metric_key]])\n",
    "#         fold_arrays.append(fold_array)\n",
    "    \n",
    "#     # Stack all folds and average: [folds, epochs, sub_metrics] -> [epochs, sub_metrics]\n",
    "#     stacked = np.stack(fold_arrays)\n",
    "#     averaged = np.mean(stacked, axis=0)\n",
    "    \n",
    "#     # Convert back to list of dictionaries\n",
    "#     result = []\n",
    "#     for epoch_values in averaged:\n",
    "#         epoch_dict = dict(zip(dict_keys, epoch_values))\n",
    "#         result.append(epoch_dict)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Add K fold labels to datapoints in dataframe, later used for creating K Fold Dataloaders\n",
    "\n",
    "# df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "# df['image_path'] = '../' + df['image_path']\n",
    "\n",
    "# # Initialize fold column\n",
    "# df['fold'] = -1\n",
    "\n",
    "# # Get unique image paths for splitting\n",
    "# unique_images_df = df[['image_path', 'domain']].reset_index(drop=True)\n",
    "# #exclude test subset test idx get from get_dataloader()\n",
    "# unique_images_df = unique_images_df[~unique_images_df['image_path'].isin(test_split_idx)]\n",
    "\n",
    "# # Create stratified 5-fold splits based on domain\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# # Assign fold numbers based on domain stratification\n",
    "# for fold, (_, val_idx) in enumerate(skf.split(unique_images_df['image_path'], unique_images_df['domain'])):\n",
    "#     val_image_paths = unique_images_df.iloc[val_idx]['image_path'].tolist()\n",
    "#     df.loc[df['image_path'].isin(val_image_paths), 'fold'] = fold\n",
    "\n",
    "# # Verify domain distribution across folds\n",
    "# print(\"Domain distribution across folds:\")\n",
    "# print(df.groupby(['fold', 'domain']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_loaders = create_crossvalidation_loaders(df, 5, batch_sizes=(32, 64, 64), resize_img_to=(512, 288))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478ec863",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2a457",
   "metadata": {},
   "source": [
    "### current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tests: retrain the model with\n",
    "# 3 trains averaged\n",
    "# both branches + random masks\n",
    "\n",
    "# but first: fix underfitting: validation set\n",
    "# lower droput\n",
    "# diff branch:\n",
    "#     resnet18\n",
    "#     mobilenetv2\n",
    "#     efficientnetb0\n",
    "#     clip\n",
    "\n",
    "#     + ablations - only_soc, only_env, no_mask\n",
    "# learning rate scheduler with warmup and decay\n",
    "\n",
    "# then\n",
    "# best model, x3 random initialisaiton, on same test set, no validation, report final metrics on test set\n",
    "#same for ablations\n",
    "#same for random mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502be9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import clip\n",
    "clip_model, clip_transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "default_transform = transforms.Compose([\n",
    "        transforms.Resize((144,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "domains = ['Home', 'BigOffice-2', 'BigOffice-3', 'Hallway', 'MeetingRoom', 'SmallOffice']\n",
    "\n",
    "testing_scenarios = {\n",
    "    'mobilenetv2':      (False,),\n",
    "    'resnet18':         (False,),\n",
    "    'efficientnetb0':   (False,),\n",
    "    'clip':             (True,),\n",
    "}\n",
    "    \n",
    "for name, (freeze_branches) in testing_scenarios.items():\n",
    "    for ablation in ['base', 'no_mask', 'only_soc', 'only_env']:\n",
    "        \n",
    "        # Training Data\n",
    "        df = pd.read_pickle(\"../data/pepper_data_train.pkl\")\n",
    "        \n",
    "        transform = clip_transform if name == 'clip' else default_transform\n",
    "\n",
    "        if ablation == 'base':\n",
    "            domain_dataloaders = get_domain_dataloaders(df, batch_sizes=(32, 64, 64), double_img=True, transforms=[transform]*2, num_workers=0, include_test=None)\n",
    "        elif ablation == 'no_mask':\n",
    "            domain_dataloaders = get_domain_dataloaders(df, batch_sizes=(32, 64, 64), double_img=False, transforms=transform, num_workers=0, include_test=None)\n",
    "        elif ablation == 'only_soc':\n",
    "            df['image_path'] = df['image_path_social']\n",
    "            domain_dataloaders = get_domain_dataloaders(df, batch_sizes=(32, 64, 64), double_img=False, transforms=transform, num_workers=0, include_test=None)\n",
    "        elif ablation == 'only_env':\n",
    "            df['image_path'] = df['image_path_env']\n",
    "            domain_dataloaders = get_domain_dataloaders(df, batch_sizes=(32, 64, 64), double_img=False, transforms=transform, num_workers=0, include_test=None)\n",
    "\n",
    "        print(f\"\\nTesting: {name} Ablation: {ablation}\")\n",
    "        setup = {'branch':name} if ablation == 'base' else {'branch': name, 'env': 'ablated'}\n",
    "        auxilary_model = clip_model if name == 'clip' else None\n",
    "        model = DualBranchModel(dropout_rate=0.1, setup=setup, freeze_branches=freeze_branches, clip_model=auxilary_model)\n",
    "        dual_model = model.to(device)\n",
    "        trainable_params = [p for p in dual_model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.Adam(trainable_params, lr=1e-3)\n",
    "        buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "\n",
    "        epochs = 20\n",
    "\n",
    "        exp_name = f\"{name}_dropout{0.1}_epochs{epochs}_ablation-{ablation}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        dualbranch_kwargs = {\n",
    "                'mse_criterion': nn.MSELoss(),\n",
    "                'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            }\n",
    "        \n",
    "        \n",
    "        unified_train_loop(\n",
    "            model=dual_model,\n",
    "            domains=domains,\n",
    "            domain_dataloaders=domain_dataloaders,\n",
    "            buffer=buffer,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            batch_fn=heuristic_dualbranch_batch,\n",
    "            batch_kwargs=dualbranch_kwargs,\n",
    "            num_epochs=epochs,\n",
    "            exp_name=exp_name,\n",
    "            gradient_clipping=True,\n",
    "            collect_tsne_data=False,\n",
    "            checkpoint_dir=\"../checkpoints\",\n",
    "            validation_set='val',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# model_names = [\n",
    "#     'heuristic_small_env',\n",
    "#     'heuristic_square_img',\n",
    "#     'heuristic_eval_buffer'\n",
    "# ]\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     print(f\"Running {model_name}\")\n",
    "#     process = subprocess.Popen(\n",
    "#         [\"python\", \"train_models.py\", \"--model_name\", model_name, \"--num_workers\", \"0\"],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         stderr=subprocess.STDOUT,\n",
    "#         text=True\n",
    "#     )\n",
    "\n",
    "#     for line in process.stdout:\n",
    "#         print(line, end='')\n",
    "\n",
    "#     process.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
