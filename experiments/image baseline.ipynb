{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee38c",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# DATASET_DIR = (Path(\"..\") / \"..\" / \"datasets\").resolve()\n",
    "# DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n",
    "# LABEL_COLS = [\n",
    "#     \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n",
    "#     \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n",
    "#     \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n",
    "# ]\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from data_processing.data_processing import ImageLabelDataset,DualImageDataset,create_dataloaders,create_crossvalidation_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7254948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "def get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(224,224), return_splits=False, double_img=False, transforms=None, num_workers=0):\n",
    "    \"\"\"\n",
    "    for yolo_depthanything model 224,224\n",
    "    otherwise 512, 288\n",
    "    LGR had 128,128 \n",
    "    MobileNetv2 had 224, 224\n",
    "    \"\"\"\n",
    "\n",
    "    # Create domain-specific dataloaders\n",
    "    domains = df['domain'].unique()\n",
    "    domain_dataloaders = {}\n",
    "    test_split_idx = set()\n",
    "    for domain in domains:\n",
    "        domain_df = df[df['domain'] == domain]\n",
    "        #domain_df = domain_df.sample(frac=0.5, random_state=42)\n",
    "        loaders, split_idx = create_dataloaders(domain_df, batch_sizes=batch_sizes, resize_img_to=resize_img_to, seed=SEED, return_splits=True, double_img=double_img, transforms=transforms, num_workers=num_workers)\n",
    "        domain_dataloaders[domain] = loaders\n",
    "        test_split_idx.update(set(split_idx['test']))\n",
    "\n",
    "    return domain_dataloaders if not return_splits else (domain_dataloaders, test_split_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ca986",
   "metadata": {},
   "source": [
    "## CL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563a996",
   "metadata": {},
   "source": [
    "### base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c51f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b035498",
   "metadata": {},
   "source": [
    "#### baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c036329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialContinualModel(nn.Module):\n",
    "    def __init__(self, num_tasks=9):\n",
    "        super().__init__()\n",
    "        # Initialize the ResNet50 architecture with Places365 configuration\n",
    "        self.backbone = models.resnet50(num_classes=365)\n",
    "        \n",
    "        # get Places365 weights and fix their naming leftover from troch saving convention\n",
    "        places365_weights = torch.load('resnet50_places365.pth.tar', weights_only=True)\n",
    "        state_dict = places365_weights['state_dict']\n",
    "        state_dict = {k.replace('module.', ''): v \n",
    "                     for k, v in state_dict.items()}\n",
    "        \n",
    "        # Load weights\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "        # Remove classification head\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        #Freeze all params except last layer\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'layer4' not in name:\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        #TODO human mask average, std, quadrants, human in realtion to robot std\n",
    "        \n",
    "\n",
    "\n",
    "        # Shared layers #TODO deeper shared space?\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads with more expensive but finegrained GELU\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(512, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        shared = self.shared_fc(features)\n",
    "        outputs = [head(shared) for head in self.heads]\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ff4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class LGRBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    @misc{churamani_feature_2024,\n",
    "\t\ttitle = {Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours},\n",
    "\t\turl = {http://arxiv.org/abs/2405.15773},\n",
    "\t\tdoi = {10.48550/arXiv.2405.15773},\n",
    "\t\tnumber = {{arXiv}:2405.15773},\n",
    "\t\tpublisher = {{arXiv}},\n",
    "\t\tauthor = {Churamani, Nikhil and Checker, Saksham and Chiang, Hao-Tien Lewis and Gunes, Hatice},\n",
    "\t\turldate = {2025-01-30},\n",
    "\t\tdate = {2024-03-16},\n",
    "\t}\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(LGRBaseline, self).__init__()\n",
    "        \n",
    "        # MobileNetV2 backbone\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        # Backbone feature processing\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Regression head\n",
    "        self.fc1_bn = nn.BatchNorm1d(1280)\n",
    "        self.fc2 = nn.Linear(1280, 32)\n",
    "        # self.fc2_bn = nn.BatchNorm1d(128)\n",
    "\t\t# self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Spatial reduction\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Regression\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.fc2_bn(x)\n",
    "\t\t# x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return {'output': x}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f94a9",
   "metadata": {},
   "source": [
    "#### buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class ReservoirBuffer:\n",
    "    def __init__(self, capacity=1000, replay_ratio=0.2, input_shape=(3, 384, 216), label_shape=(9,), device=torch.device('cpu')):\n",
    "        self.capacity = capacity\n",
    "        self.inputs = torch.empty((capacity, *input_shape), dtype=torch.float32, device=device)\n",
    "        self.labels = torch.empty((capacity, *label_shape), dtype=torch.float32, device=device)\n",
    "        self.domains = [None] * capacity\n",
    "        self.size = 0          # Number of samples currently in buffer\n",
    "        self.num_seen = 0      # Total samples seen\n",
    "        self.replay_ratio = replay_ratio\n",
    "\n",
    "    def add(self, new_samples):\n",
    "        for sample in new_samples:\n",
    "            self.num_seen += 1\n",
    "            if self.size < self.capacity:\n",
    "                idx = self.size\n",
    "                self.size += 1\n",
    "            else:\n",
    "                idx = random.randint(0, self.num_seen - 1)\n",
    "                if idx >= self.capacity:\n",
    "                    continue\n",
    "            self.inputs[idx].copy_(sample[0])\n",
    "            self.labels[idx].copy_(sample[1])\n",
    "            self.domains[idx] = sample[2]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.size == 0:\n",
    "            return []\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return [(self.inputs[i], self.labels[i], self.domains[i]) for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_domain_distribution(self):\n",
    "        return pd.Series(self.domains[:self.size]).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c7ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# from itertools import product\n",
    "\n",
    "# df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "# domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(1,1), return_splits=False, double_img=False)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# domains = df['domain'].unique()\n",
    "\n",
    "# versions = list(product([1000, 500, 200, 100], ['', 'downsample_buffer', 'upsample_current']))\n",
    "\n",
    "# for buffer_size, balanced in tqdm(versions):\n",
    "#     print(f\"{buffer_size=} {balanced}\\n\")\n",
    "\n",
    "#     buffer = NaiveRehearsalBuffer(buffer_size, balanced)\n",
    "#     for domain_idx, current_domain in enumerate(tqdm(domains)):\n",
    "#         train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        \n",
    "#         counter = Counter()\n",
    "#         for _,_,labels_batch in tqdm(train_loader, leave=False):  # batch of (images, labels, domain_labels)\n",
    "#             counter.update(labels_batch)\n",
    "        \n",
    "#         ratio = {key: counter.get(key, 0) for key in domains}\n",
    "#         print(f\"{current_domain}: {ratio}\")\n",
    "\n",
    "#         buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRehearsalBuffer:\n",
    "    \"\"\"\n",
    "    @inproceedings{Hsu18_EvalCL,\n",
    "        title={Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},\n",
    "        author={Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},\n",
    "        booktitle={NeurIPS Continual learning Workshop },\n",
    "        year={2018},\n",
    "        url={https://arxiv.org/abs/1810.12488}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size=1000, balancing=True):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = {}\n",
    "        self.balancing=balancing\n",
    "\n",
    "    def update_buffer(self, current_domain, current_dataset):\n",
    "        # Add/overwrite current domain\n",
    "        self.buffer[current_domain] = Subset(current_dataset, torch.arange(len(current_dataset)))\n",
    "        \n",
    "        # Recalculate quota - even for each domain\n",
    "        num_domains = len(self.buffer)\n",
    "        buffer_quota_per_domain = self.buffer_size // num_domains\n",
    "        \n",
    "        # Reduce all domains (including current)\n",
    "        for domain in self.buffer:\n",
    "            domain_buffer = self.buffer[domain]\n",
    "            max_safe_samples_to_overwrite = min(buffer_quota_per_domain, len(domain_buffer.dataset))\n",
    "            rand_indices = torch.randperm(len(domain_buffer.dataset))[:max_safe_samples_to_overwrite].numpy()\n",
    "            self.buffer[domain] = Subset(domain_buffer.dataset, rand_indices)\n",
    "    \n",
    "    def get_loader_with_replay(self, current_domain, current_loader):\n",
    "        if self.balancing:\n",
    "            return self.balanced_combine_training_and_replay_loader(current_domain, current_loader)\n",
    "        else:\n",
    "            return self.combine_training_and_replay_loader(current_domain, current_loader)\n",
    "    \n",
    "    def balanced_combine_training_and_replay_loader(self, current_domain, current_loader):\n",
    "        current_dataset = current_loader.dataset\n",
    "        replay_datasets = [dataset for domain, dataset in self.buffer.items() if domain != current_domain]\n",
    "\n",
    "        if not replay_datasets:\n",
    "            return current_loader\n",
    "        \n",
    "        current_size = len(current_dataset)\n",
    "        total_replay_size = sum(len(d) for d in replay_datasets)\n",
    "        samples_per_domain = current_size // len(replay_datasets)\n",
    "        replay_subsets = []\n",
    "        \n",
    "        # Case 1: Small buffer - upsample the buffer - domain stratified upsampling with replacement\n",
    "        if total_replay_size < current_size:\n",
    "            for domain_data in replay_datasets:\n",
    "                indices = torch.randint(0, len(domain_data), (samples_per_domain,))\n",
    "                replay_subsets.append(Subset(domain_data, indices))\n",
    "            replay_dataset = ConcatDataset(replay_subsets)\n",
    "        \n",
    "        # Case 2: Large buffer - upsample the training data - upsampling with replacement\n",
    "        elif (total_replay_size > current_size):\n",
    "            indices = torch.randint(0, current_size, (total_replay_size,))\n",
    "            current_dataset = Subset(current_dataset, indices)\n",
    "            replay_dataset = ConcatDataset(replay_datasets)\n",
    "        \n",
    "        # Case 3: Large buffer - downsample the buffer - domain stratified downsampling\n",
    "        # Obsolete - just set the buffer size smaller\n",
    "        # elif (total_replay_size > current_size) and (self.balancing == 'downsample_buffer'):\n",
    "        #     for domain_data in replay_datasets:\n",
    "        #         indices = torch.randperm(len(domain_data))[:samples_per_domain]\n",
    "        #         replay_subsets.append(Subset(domain_data, indices))\n",
    "        #     replay_dataset = ConcatDataset(replay_subsets)\n",
    "        \n",
    "        # Case 3: Replay buffer = training dataset\n",
    "        else: \n",
    "            replay_dataset = ConcatDataset(replay_datasets)\n",
    "\n",
    "        combined_dataset = ConcatDataset([replay_dataset, current_dataset])\n",
    "        combined_loader = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=current_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=current_loader.num_workers,\n",
    "            pin_memory=current_loader.pin_memory,\n",
    "            drop_last=current_loader.drop_last\n",
    "        )\n",
    "        return combined_loader\n",
    "\n",
    "    def combine_training_and_replay_loader(self, current_domain, current_loader):\n",
    "        current_dataset = current_loader.dataset\n",
    "        replay_datasets = [dataset for domain, dataset in self.buffer.items() if domain != current_domain]\n",
    "\n",
    "        #Enforces 1:1 ratio when current ≥ buffer\n",
    "        total_replay = sum(len(dataset) for dataset in replay_datasets)\n",
    "        if total_replay > 0:\n",
    "            K = max(len(current_dataset) // total_replay, 1)\n",
    "            replay_datasets = replay_datasets * K\n",
    "\n",
    "        combined_dataset =  ConcatDataset(replay_datasets + [current_dataset])\n",
    "        combined_dataset = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=current_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=current_loader.num_workers,\n",
    "            pin_memory=current_loader.pin_memory,\n",
    "            drop_last=current_loader.drop_last\n",
    "        )\n",
    "        return combined_dataset\n",
    "    \n",
    "    def get_domain_distribution(self):\n",
    "        \"\"\"Returns {domain: num_samples} without needing Storage\"\"\"\n",
    "        return {domain: len(subset) for domain, subset in self.buffer.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonstratifiedNaiveRehearsalBuffer:\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []  # List of (dataset_object, idx) tuples\n",
    "\n",
    "    def update_buffer(self, current_dataset):\n",
    "        # Add all new samples as (dataset_object, idx) pairs\n",
    "        new_samples = [(current_dataset, idx) for idx in range(len(current_dataset))]\n",
    "        self.buffer += new_samples\n",
    "\n",
    "        # If buffer is too large, randomly keep only buffer_size samples\n",
    "        if len(self.buffer) > self.buffer_size:\n",
    "            perm = torch.randperm(len(self.buffer))[:self.buffer_size]\n",
    "            self.buffer = [self.buffer[i] for i in perm]\n",
    "\n",
    "    def get_loader_with_replay(self, current_loader):\n",
    "        current_dataset = current_loader.dataset\n",
    "\n",
    "        # Group buffer samples by dataset for efficient Subset creation       \n",
    "        dataset_to_indices = defaultdict(list)\n",
    "        for dataset, idx in self.samples:\n",
    "            dataset_to_indices[dataset].append(idx)\n",
    "        buffer_subsets = [Subset(ds, idxs) for ds, idxs in dataset_to_indices.items()]\n",
    "        \n",
    "        if not buffer_subsets:\n",
    "            return current_loader\n",
    "        \n",
    "        replay_dataset = ConcatDataset(buffer_subsets)\n",
    "\n",
    "        current_size = len(current_dataset)\n",
    "        total_replay_size = len(replay_dataset)\n",
    "\n",
    "        N_max = max(total_replay_size, current_size)\n",
    "\n",
    "        # Upsample with replacement if needed\n",
    "        if total_replay_size < current_size:\n",
    "            idxs = torch.randint(0, total_replay_size, (current_size,))\n",
    "            replay_dataset = Subset(replay_dataset, idxs)\n",
    "        if total_replay_size > current_size:\n",
    "            idxs = torch.randint(0, current_size, (total_replay_size,))\n",
    "            current_dataset = Subset(current_dataset, idxs)\n",
    "\n",
    "        combined_dataset = ConcatDataset([replay_dataset, current_dataset])\n",
    "        combined_loader =  DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=current_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=current_loader.num_workers,\n",
    "            pin_memory=current_loader.pin_memory,\n",
    "            drop_last=current_loader.drop_last\n",
    "        )\n",
    "        return combined_loader\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def get_domain_distribution(self):\n",
    "        return {\"buffer\": len(self.buffer)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801b37e",
   "metadata": {},
   "source": [
    "#### dualbranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "class GradientReversal(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, 1.0)\n",
    "\n",
    "\n",
    "class DualBranchNet(nn.Module):\n",
    "    def __init__(self, num_outputs=9, num_domains=6, weights_init=False, weights_norm=False, layer_norm=False, detach_base=True, freeze_base=''):\n",
    "        super().__init__()\n",
    "\n",
    "        self.detach_base = detach_base\n",
    "\n",
    "        def linear(in_f, out_f):\n",
    "            layer = nn.Linear(in_f, out_f)\n",
    "            return torch.nn.utils.parametrizations.weight_norm(layer, dim=0) if weights_norm else layer \n",
    "        \n",
    "        def layer_norm(dim):\n",
    "            return nn.LayerNorm(dim) if layer_norm else nn.Identity()\n",
    "\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.feature_dim = 1280\n",
    "\n",
    "        if freeze_base in ('full', 'partial'):\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        if freeze_base == 'partial':\n",
    "            last_layer = list(self.backbone.children())[-1]\n",
    "            for param in last_layer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        self.invariant = nn.Sequential(\n",
    "            linear(self.feature_dim, 256),\n",
    "            GradientReversal(),\n",
    "            layer_norm(256),\n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "\n",
    "        self.invariant_domain_classifier = nn.Sequential(\n",
    "            layer_norm(256),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "        \n",
    "        self.specific = nn.Sequential(\n",
    "            linear(self.feature_dim, 256),\n",
    "            layer_norm(256), \n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.specific_domain_classifier = nn.Sequential(\n",
    "            layer_norm(256),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )\n",
    "\n",
    "        head_in = 512 + self.feature_dim if self.detach_base else 512\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_in, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "        if weights_init:\n",
    "            self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        custom_modules = [\n",
    "            'invariant',\n",
    "            'invariant_domain_classifier',\n",
    "            'specific',\n",
    "            'specific_domain_classifier',\n",
    "            'head'\n",
    "        ]\n",
    "        for name in custom_modules:\n",
    "            module = getattr(self, name, None)\n",
    "            for m in module.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.orthogonal_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.backbone(x)\n",
    "        base = self.pool(base).view(x.size(0), -1)\n",
    "\n",
    "        if self.detach_base:\n",
    "            invariant_feats = self.invariant(base.detach())\n",
    "            specific_feats = self.specific(base.detach())\n",
    "            combined = torch.cat([invariant_feats, specific_feats, base], dim=1)\n",
    "        else:\n",
    "            invariant_feats = self.invariant(base)\n",
    "            specific_feats = self.specific(base)\n",
    "            combined = torch.cat([invariant_feats, specific_feats], dim=1)\n",
    "        \n",
    "        invariant_domain_pred = self.invariant_domain_classifier(invariant_feats)\n",
    "        specific_domain_pred = self.specific_domain_classifier(specific_feats)\n",
    "        \n",
    "        scores = self.head(combined)      \n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': invariant_domain_pred,\n",
    "            'specific_domain': specific_domain_pred,\n",
    "            'invariant_feats': invariant_feats,\n",
    "            'specific_feats': specific_feats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet_deep(DualBranchNet):\n",
    "    \"\"\"thicker invariant network to learn features instead of realying on the backbone\"\"\"\n",
    "    def __init__(self, num_outputs=9, num_domains=6, weights_init=True, weights_norm=False, layer_norm=False, detach_base=True, freeze_base=''):\n",
    "        super().__init__(num_outputs, num_domains, weights_init, weights_norm, layer_norm, detach_base, freeze_base)\n",
    "\n",
    "        def linear(in_f, out_f):\n",
    "            layer = nn.Linear(in_f, out_f)\n",
    "            return torch.nn.utils.parametrizations.weight_norm(layer, dim=0) if weights_norm else layer \n",
    "\n",
    "        self.invariant = nn.Sequential(\n",
    "            linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "\n",
    "        self.specific = nn.Sequential(\n",
    "            linear(self.feature_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            linear(256, 256)\n",
    "        )\n",
    "\n",
    "        self.invariant_domain_classifier = nn.Sequential(\n",
    "            GradientReversal(),\n",
    "            nn.Linear(256, num_domains)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457558f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet_binary(DualBranchNet_deep):\n",
    "    \"\"\"binary classifier for the invariant branch\"\"\"\n",
    "    def __init__(self, num_outputs=9, num_domains=6, weights_init=True, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='', explicit_grl=False):\n",
    "        super().__init__(num_outputs, num_domains, weights_init, weights_norm, layer_norm, detach_base, freeze_base)\n",
    "\n",
    "        self.explicit_grl = explicit_grl\n",
    "        \n",
    "        def gradient_layer():\n",
    "            return nn.Identity() if explicit_grl else GradientReversal()\n",
    "\n",
    "        self.invariant_domain_classifier = nn.Sequential(\n",
    "            gradient_layer(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        base = self.backbone(x)\n",
    "        base = self.pool(base).view(x.size(0), -1)\n",
    "\n",
    "        if self.detach_base:\n",
    "            invariant_feats = self.invariant(base.detach())\n",
    "            specific_feats = self.specific(base.detach())\n",
    "            combined = torch.cat([invariant_feats, specific_feats, base], dim=1)\n",
    "        else:\n",
    "            invariant_feats = self.invariant(base)\n",
    "            specific_feats = self.specific(base)\n",
    "            combined = torch.cat([invariant_feats, specific_feats], dim=1)\n",
    "        \n",
    "        if self.explicit_grl:\n",
    "            invariant_domain_pred = self.invariant_domain_classifier(GradientReversalFunction.apply(invariant_feats, 1.0))\n",
    "        else:\n",
    "            invariant_domain_pred = self.invariant_domain_classifier(invariant_feats)\n",
    "            \n",
    "        specific_domain_pred = self.specific_domain_classifier(specific_feats)\n",
    "        \n",
    "        scores = self.head(combined)      \n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': invariant_domain_pred,\n",
    "            'specific_domain': specific_domain_pred,\n",
    "            'invariant_feats': invariant_feats,\n",
    "            'specific_feats': specific_feats\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1450b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchCNNNet(nn.Module):\n",
    "    def __init__(self, num_outputs=9, num_domains=6, backbone_type='3conv', branch_type='special', end_type='simple', batch_norm=False, detach_base=False):\n",
    "        super().__init__()\n",
    "        self.backbone_type = backbone_type\n",
    "        self.branch_type = branch_type\n",
    "        self.end_type = end_type\n",
    "        self.detach_base = detach_base\n",
    "        \n",
    "        def BatchNorm(in_channels):\n",
    "            return nn.BatchNorm2d(in_channels) if batch_norm else nn.Identity()\n",
    "\n",
    "        #Input resized to 512,288\n",
    "\n",
    "        if self.backbone_type == 'none':\n",
    "            self.backbone = nn.Identity()\n",
    "            self.backbone_channels = 3\n",
    "\n",
    "        elif self.backbone_type == '2conv':\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                BatchNorm(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                BatchNorm(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "            self.backbone_channels = 64\n",
    "\n",
    "        elif self.backbone_type == '3conv':\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                BatchNorm(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                BatchNorm(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "            self.backbone_channels = 128\n",
    "\n",
    "        elif self.backbone_type == 'pretrained':\n",
    "            self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "            if not self.detach_base:\n",
    "                for param in self.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "                \n",
    "            self.backbone_channels = 1280\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"backbone must be 'none' | '2conv' | '3conv' | 'pretrained'\")\n",
    "\n",
    "        if self.branch_type == 'linear':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.backbone_channels, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256)\n",
    "            )\n",
    "            self.room_branch = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(self.backbone_channels, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256)\n",
    "            )\n",
    "            self.branch_channels = 256\n",
    "\n",
    "        elif self.branch_type == 'simple':\n",
    "            self.room_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, dilation=2, padding=2),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.branch_channels = 128*4*4\n",
    "\n",
    "        elif self.branch_type == 'special':\n",
    "            self.room_branch = nn.Sequential(\n",
    "                CoordConv(self.backbone_channels, 128, 3),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, dilation=2, padding=2),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                SpatialMultiheadAttention(128, 4),\n",
    "                nn.Conv2d(128, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.branch_channels = 128*4*4\n",
    "\n",
    "        elif self.branch_type == 'adapted_adversarial':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 256, 5, padding=2),\n",
    "                BatchNorm(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 512, 5, padding=2),\n",
    "                BatchNorm(512),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.room_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 256, 5, padding=2),\n",
    "                BatchNorm(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(256, 512, 5, padding=2),\n",
    "                BatchNorm(512),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            # Calculate output dimensions based on backbone output            \n",
    "            adversarial_channels = {\n",
    "                'none': 512 * 128 * 72,       # 4,718,592\n",
    "                '2conv': 512 * 32 * 18,       # 294,912\n",
    "                '3conv': 512 * 16 * 9,        # 73,728\n",
    "                'pretrained': 512 * 4 * 2     # 4,096\n",
    "            }\n",
    "            self.branch_channels = adversarial_channels[self.backbone_type]\n",
    "\n",
    "        elif self.branch_type == 'adversarial':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 96, 5, padding=2),\n",
    "                BatchNorm(96),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(96, 144, 3, padding=1),\n",
    "                BatchNorm(144),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(144, 256, 5, padding=2),\n",
    "                BatchNorm(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.room_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 96, 5, padding=2),\n",
    "                BatchNorm(96),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(96, 144, 3, padding=1),\n",
    "                BatchNorm(144),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(144, 256, 5, padding=2),\n",
    "                BatchNorm(256),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            # Calculate output dimensions based on backbone output\n",
    "            adversarial_channels = {\n",
    "                'none': 256 * 64 * 36,        # 589,824\n",
    "                '2conv': 256 * 16 * 9,        # 36,864 \n",
    "                '3conv': 256 * 8 * 4,         # 8,192\n",
    "                'pretrained': 256 * 2 * 1     # 512\n",
    "            }\n",
    "            self.branch_channels = adversarial_channels[self.backbone_type]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"branch must be 'linear' | 'simple' | 'special' | 'adversarial' | 'adapted_adversarial'\")\n",
    "\n",
    "        if self.end_type == 'simple':\n",
    "            self.room_domain_cls = nn.Linear(self.branch_channels, num_domains)\n",
    "            self.social_domain_cls = nn.Linear(self.branch_channels, 1)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels*2, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_outputs)\n",
    "            )\n",
    "        elif self.end_type == 'adversarial':\n",
    "            self.room_domain_cls = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, num_domains)\n",
    "            )\n",
    "            self.social_domain_cls = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024, 1)\n",
    "            )\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels*2, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, num_outputs)\n",
    "            )\n",
    "        \n",
    "        elif self.end_type == '3linear':\n",
    "            self.room_domain_cls = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_domains)\n",
    "            )\n",
    "            self.social_domain_cls = nn.Sequential(\n",
    "                nn.Linear(self.branch_channels, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "            if self.detach_base:\n",
    "                self.backbone_proj = nn.Sequential(\n",
    "                    nn.AdaptiveAvgPool2d((4,4)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(self.backbone_channels*4*4, self.branch_channels),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                proj_backbone_channels = self.branch_channels\n",
    "            total_channels = self.branch_channels*2\n",
    "            total_channels += proj_backbone_channels if self.detach_base else 0\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(total_channels, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_outputs)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"end must be 'simple' | '3linear' | 'adversarial'\")\n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        base = self.backbone(x)\n",
    "        \n",
    "        if self.detach_base:\n",
    "            room_feat = self.room_branch(base.detach())\n",
    "            social_feat = self.social_branch(base.detach())\n",
    "            proj_base = self.backbone_proj(base)\n",
    "            scores = self.head(torch.cat([room_feat, social_feat, proj_base], 1))\n",
    "        else:\n",
    "            room_feat = self.room_branch(base)\n",
    "            social_feat = self.social_branch(base)\n",
    "            scores = self.head(torch.cat([room_feat, social_feat], 1))\n",
    "\n",
    "        room_domain_cls = self.room_domain_cls(room_feat)\n",
    "        social_domain_cls = self.social_domain_cls(GradientReversalFunction.apply(social_feat, alpha))\n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': social_domain_cls,\n",
    "            'specific_domain': room_domain_cls,\n",
    "            'invariant_feats': social_feat,\n",
    "            'specific_feats': room_feat\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet_DANN(nn.Module):\n",
    "    def __init__(self, num_outputs=9, backbone_type='3conv'):\n",
    "        super().__init__()\n",
    "        self.backbone_type = backbone_type\n",
    "\n",
    "        #Input RGB of size 512,288\n",
    "\n",
    "        if self.backbone_type == 'mobilenet':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features,\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.backbone_channels = 1280\n",
    "\n",
    "        elif self.backbone_type == '3conv':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, 5, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(96, 144, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(144, 256, 5, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.AdaptiveAvgPool2d(output_size=(4, 4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            self.backbone_channels = 256*4*4\n",
    "        else:\n",
    "            raise ValueError(\"backbone_type must be '3conv' | 'mobilenet'\")\n",
    "\n",
    "        self.social_domain_cls = nn.Sequential(\n",
    "            nn.Linear(self.backbone_channels, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone_channels, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, alpha=1.0, is_first_domain=False):\n",
    "        social_feat = self.social_branch(x)\n",
    "        scores = self.head(social_feat)\n",
    "        \n",
    "        social_domain_cls = None if is_first_domain else self.social_domain_cls(GradientReversalFunction.apply(social_feat, alpha))\n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': social_domain_cls,\n",
    "            'invariant_feats': social_feat\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519205fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels + 2, \n",
    "                             out_channels, \n",
    "                             kernel_size, \n",
    "                             padding=kernel_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, _, h, w = x.shape\n",
    "        \n",
    "        # Create coordinate grids (range [-1, 1])\n",
    "        x_coord = torch.linspace(-1, 1, w).repeat(h, 1)\n",
    "        y_coord = torch.linspace(-1, 1, h).repeat(w, 1).t()\n",
    "        coords = torch.stack([x_coord, y_coord], dim=0)\n",
    "        coords = coords.unsqueeze(0).repeat(batch, 1, 1, 1).to(x.device)\n",
    "        x = torch.cat([x, coords], dim=1)\n",
    "        \n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419135e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialMultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x_small = self.pool(x)\n",
    "        # Reshape to sequence format\n",
    "        x_flat = x_small.flatten(2).permute(0, 2, 1)\n",
    "        attn_output, _ = self.mha(x_flat, x_flat, x_flat)\n",
    "        # Reshape back to spatial format\n",
    "        attn_reshaped = attn_output.permute(0, 2, 1).view(B, C, 8, 8)\n",
    "        return F.interpolate(attn_reshaped, size=(H, W), mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualBranchNet_minimal(nn.Module):\n",
    "    def __init__(self, num_outputs=9, backbone_type='3conv', branch_type='simple', head_type='3layer', batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.backbone_type = backbone_type\n",
    "        self.branch_type = branch_type\n",
    "        self.head_type = head_type\n",
    "        \n",
    "        def BatchNorm(in_channels):\n",
    "            return nn.BatchNorm2d(in_channels) if batch_norm else nn.Identity()\n",
    "\n",
    "        #Input resized to 512,288\n",
    "        if self.backbone_type == 'absurd':\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2)\n",
    "            ) \n",
    "\n",
    "        elif self.backbone_type == '3conv':\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, 3, padding=1),\n",
    "                BatchNorm(32),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1),\n",
    "                BatchNorm(64),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "            self.backbone_channels = 128\n",
    "\n",
    "        elif self.backbone_type == 'pretrained':\n",
    "            self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.backbone_channels = 1280\n",
    "\n",
    "        if self.branch_type == 'simple':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, dilation=2, padding=2),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(128, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "\n",
    "        elif self.branch_type == 'special':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, dilation=2, padding=2),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                SpatialMultiheadAttention(128, 4),\n",
    "                nn.Conv2d(128, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "        elif self.branch_type == '1convo':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(self.backbone_channels, 128, 3, padding=1),\n",
    "                BatchNorm(128),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveAvgPool2d((4,4)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "        elif self.branch_type == 'absurd':\n",
    "            self.social_branch = nn.Sequential(\n",
    "                nn.Conv2d(16, 32, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "\n",
    "        if self.head_type == '3layer':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(128*4*4, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_outputs)\n",
    "            )\n",
    "        elif self.head_type == '2layer256':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(128*4*4, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_outputs)\n",
    "            )\n",
    "        elif self.head_type == 'absurd':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(32, num_outputs)\n",
    "            )\n",
    "    \n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        base = self.backbone(x)\n",
    "    \n",
    "        social_feat = self.social_branch(base)\n",
    "        scores = self.head(social_feat)\n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_feats': social_feat,\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fd8e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models.segmentation as segmentation\n",
    "\n",
    "class DANN_classifier_poc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pretrained_model = segmentation.deeplabv3_mobilenet_v3_large(\n",
    "            pretrained=True,\n",
    "            weights=segmentation.DeepLabV3_MobileNet_V3_Large_Weights.DEFAULT\n",
    "        )\n",
    "        self.backbone = pretrained_model.backbone\n",
    "        self.classifier = pretrained_model.classifier\n",
    "        self.bn = nn.BatchNorm1d(4116)\n",
    "\n",
    "        self.social_domain_cls = nn.Sequential(\n",
    "            nn.Linear(4116, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(4116, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 9)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, alpha=1.0, is_first_domain=False):\n",
    "        features = self.backbone(x)['out']\n",
    "        features = self.classifier(features)\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = torch.flatten(features, 1)\n",
    "        features = self.bn(features)\n",
    "\n",
    "        domain = None if is_first_domain else self.social_domain_cls(GradientReversalFunction.apply(features, alpha))\n",
    "        output = self.head(features)\n",
    "        return {\n",
    "            'output': output,\n",
    "            'invariant_domain': domain,\n",
    "            'invariant_feats': features,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e9597",
   "metadata": {},
   "source": [
    "### ForgettingGatedSplitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Tuple, Optional\n",
    "from collections import deque\n",
    "\n",
    "sys.path.append('D:/projects/Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14a20a",
   "metadata": {},
   "source": [
    "#### backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51fcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO11FeatureExtractor(nn.Module):\n",
    "    \"\"\"Extract features from YOLO11-seg backbone (layers 0-8)\"\"\"\n",
    "    def __init__(self, model_path='../models/yolo11n-seg.pt'):\n",
    "        super().__init__()\n",
    "        yolo = YOLO(model_path)\n",
    "\n",
    "        # Extract layers 0-8 as discussed (before SPPF to preserve spatial granularity)\n",
    "        self.backbone = nn.Sequential(*list(yolo.model.model[:9]))\n",
    "        del yolo\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # Output: [B, 256, H/32, W/32]\n",
    "\n",
    "class DepthAnythingFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_path='../models/depth_anything_v2_vits.pth'):\n",
    "        super().__init__()\n",
    "        model_configs = {\n",
    "            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        }\n",
    "        model = DepthAnythingV2(**model_configs['vits'])\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        \n",
    "        self.backbone = model.pretrained\n",
    "        del model\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Patch embedding\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            \n",
    "            # Pass through transformer blocks\n",
    "            for block in self.backbone.blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            # Apply final norm\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            # Remove CLS token and reshape to spatial format\n",
    "            if x.shape[1] > (H//14) * (W//14):\n",
    "                patch_tokens = x[:, 1:, :]  # Remove CLS token\n",
    "            else:\n",
    "                patch_tokens = x\n",
    "            \n",
    "            # Reshape to spatial format\n",
    "            patch_h, patch_w = H // 14, W // 14\n",
    "            spatial_features = patch_tokens.transpose(1, 2).reshape(B, 384, patch_h, patch_w)\n",
    "            \n",
    "            return spatial_features\n",
    "\n",
    "\n",
    "class DualBackboneFeatureExtractor(nn.Module):\n",
    "    \"\"\"Combines YOLO11 and DepthAnything feature extractors\"\"\"\n",
    "    def __init__(self, model_paths):\n",
    "        super().__init__()\n",
    "        # Individual feature extractors\n",
    "        self.yolo_extractor = YOLO11FeatureExtractor(model_paths['YOLO11n_seg'])\n",
    "        self.depth_extractor = DepthAnythingFeatureExtractor(model_paths['DepthAnythingV2_small'])\n",
    "        \n",
    "        # Feature projection layers (optional - can keep features as-is)\n",
    "        self.yolo_proj = nn.Conv2d(256, 256, 1)\n",
    "        self.depth_proj = nn.Conv2d(384, 384, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from both backbones\n",
    "        yolo_features = self.yolo_extractor(x[0])  # [B, 256, H/32, W/32]\n",
    "        depth_features = self.depth_extractor(x[1])  # [B, 384, H/14, W/14]\n",
    "        \n",
    "        # Apply projections\n",
    "        yolo_features = self.yolo_proj(yolo_features)\n",
    "        depth_features = self.depth_proj(depth_features)\n",
    "        \n",
    "        # Downsample the depthanything features to match yolo\n",
    "        depth_features = F.adaptive_avg_pool2d(depth_features, (7, 7)) #image size /32 224,224 > 7,7\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([yolo_features, depth_features], dim=1)  # [B, 640, H/14, W/14] \n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397757eb",
   "metadata": {},
   "source": [
    "#### model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatastrophicForgettingAdapter(nn.Module):\n",
    "    \"\"\"Standalone adapter that measures forgetting and gates spatial features\"\"\"\n",
    "    def __init__(self, num_channels=640, num_outputs=9):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # Forgetting measurement storage (per channel)\n",
    "        self.previous_adapter_state = None\n",
    "        self.current_forgetting_scores = None\n",
    "        \n",
    "        # Adapter that predicts main task (for forgetting measurement)\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_outputs)  # Predicts main task\n",
    "        )\n",
    "        \n",
    "        # Learnable gating network (operates on channel-wise forgetting scores)\n",
    "        self.gating = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // 4, num_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, backbone_features, use_gating=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_features: [B, 640, 36, 36] spatial features\n",
    "        Returns:\n",
    "            forgetting_features: [B, 640, 36, 36] features routed to forgetting branch\n",
    "            not_forgetting_features: [B, 640, 36, 36] features routed to stable branch\n",
    "            adapter_predictions: [B, 9] task predictions for forgetting measurement\n",
    "        \"\"\"\n",
    "        B, C, H, W = backbone_features.shape\n",
    "        \n",
    "        # Adapter makes predictions (for forgetting measurement)\n",
    "        adapter_predictions = self.adapter(backbone_features)\n",
    "        \n",
    "        if use_gating and self.current_forgetting_scores is not None:\n",
    "            # Apply channel-wise gating based on forgetting scores\n",
    "            forgetting_scores_batch = self.current_forgetting_scores.unsqueeze(0).expand(B, -1)  # [B, 640]\n",
    "            gating_mask = self.gating(forgetting_scores_batch)  # [B, 640]\n",
    "            \n",
    "            # Reshape for spatial broadcasting\n",
    "            gating_mask = gating_mask.unsqueeze(2).unsqueeze(3)  # [B, 640, 1, 1]\n",
    "            \n",
    "            # Route features channel-wise\n",
    "            forgetting_features = backbone_features * gating_mask\n",
    "            not_forgetting_features = backbone_features * (1 - gating_mask)\n",
    "        else:\n",
    "            # No gating - both branches get all features\n",
    "            forgetting_features = backbone_features\n",
    "            not_forgetting_features = backbone_features\n",
    "        \n",
    "        return forgetting_features, not_forgetting_features, adapter_predictions\n",
    "    \n",
    "    def store_adapter_state(self):\n",
    "        \"\"\"Store current adapter state for forgetting measurement\"\"\"\n",
    "        self.previous_adapter_state = {\n",
    "            name: param.clone().detach() \n",
    "            for name, param in self.adapter.named_parameters()\n",
    "        }\n",
    "    \n",
    "    def compute_forgetting_scores(self):\n",
    "        \"\"\"Compute channel-wise forgetting scores by comparing adapter states\"\"\"\n",
    "        # Get the device from model parameters\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        if self.previous_adapter_state is None:\n",
    "            # First domain - no forgetting to measure\n",
    "            self.current_forgetting_scores = torch.zeros(self.num_channels, device=device)\n",
    "            return\n",
    "        \n",
    "        # Compare adapter weights to measure which channels are most affected\n",
    "        forgetting_scores = []\n",
    "        \n",
    "        for name, current_param in self.adapter.named_parameters():\n",
    "            if name in self.previous_adapter_state:\n",
    "                previous_param = self.previous_adapter_state[name]\n",
    "                \n",
    "                if 'weight' in name and len(current_param.shape) == 2:\n",
    "                    # For first linear layer after pooling: [256, 640]\n",
    "                    if current_param.shape[1] == self.num_channels:\n",
    "                        # Compute change per input channel\n",
    "                        weight_change = torch.norm(current_param - previous_param, dim=0)\n",
    "                        forgetting_scores.append(weight_change)\n",
    "                        break  # Use only first layer that maps from channels\n",
    "        \n",
    "        # Use channel-wise forgetting scores\n",
    "        if forgetting_scores:\n",
    "            self.current_forgetting_scores = forgetting_scores[0].to(device)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            if self.current_forgetting_scores.max() > 0:\n",
    "                self.current_forgetting_scores = (\n",
    "                    self.current_forgetting_scores / self.current_forgetting_scores.max()\n",
    "                )\n",
    "        else:\n",
    "            # Fallback if no forgetting scores computed\n",
    "            self.current_forgetting_scores = torch.zeros(self.num_channels, device=device)\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    \"\"\"Convolutional processing branch for spatial features\"\"\"\n",
    "    def __init__(self, input_channels=640, hidden_channels=256, output_channels=128):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Output conv block\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Global pooling to get fixed-size output\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()  # [B, output_channels]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "    \n",
    "class FeatureFusion(nn.Module):\n",
    "    \"\"\"Attention-based fusion of branch features\"\"\"\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, high_features, low_features):\n",
    "        # Concatenate features\n",
    "        combined = torch.cat([high_features, low_features], dim=1)  # [B, 512]\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention(combined)  # [B, 2]\n",
    "        \n",
    "        # Apply attention\n",
    "        weighted_high = high_features * attention_weights[:, 0:1]\n",
    "        weighted_low = low_features * attention_weights[:, 1:2]\n",
    "        \n",
    "        # Combine with residual connection\n",
    "        fused = weighted_high + weighted_low\n",
    "        \n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35dfb2",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatastrophicForgettingDisentanglementModel(nn.Module):\n",
    "    \"\"\"Updated model with spatial features and convolutional branches\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 backbone_output_channels: int = 640,\n",
    "                 branch_hidden_channels: int = 256,\n",
    "                 branch_output_channels: int = 128,\n",
    "                 num_outputs: int = 9):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"🚀 Initializing Spatial Catastrophic Forgetting Model...\")\n",
    "        \n",
    "        # Dual backbone (frozen) - now outputs spatial features\n",
    "        model_paths={\n",
    "            'YOLO11n_seg': '../models/yolo11n-seg.pt', \n",
    "            'DepthAnythingV2_small': '../models/depth_anything_v2_vits.pth'\n",
    "            }\n",
    "        missing = [name for name, path in model_paths.items() if not os.path.exists(path)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing model files for: {', '.join(missing)}\")\n",
    "\n",
    "        self.backbone = DualBackboneFeatureExtractor(model_paths=model_paths)  # [B, 640, 36, 36]\n",
    "        \n",
    "        # Standalone forgetting adapter\n",
    "        self.forgetting_adapter = CatastrophicForgettingAdapter(\n",
    "            num_channels=backbone_output_channels,\n",
    "            num_outputs=num_outputs\n",
    "        )\n",
    "        \n",
    "        # Convolutional branches for spatial processing\n",
    "        self.branch_forgetting = ConvBranch(\n",
    "            backbone_output_channels, \n",
    "            branch_hidden_channels, \n",
    "            branch_output_channels\n",
    "        )\n",
    "        self.branch_not_forgetting = ConvBranch(\n",
    "            backbone_output_channels, \n",
    "            branch_hidden_channels, \n",
    "            branch_output_channels\n",
    "        )\n",
    "        \n",
    "        self.fusion = FeatureFusion(branch_output_channels)\n",
    "\n",
    "        # Final head combines branch outputs\n",
    "        self.head =  nn.Sequential(\n",
    "            nn.Linear(branch_output_channels, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "        \n",
    "        print(\"✅ Spatial model initialized successfully!\")\n",
    "        \n",
    "    def forward(self, x, use_gating: bool = True):\n",
    "        # Extract spatial features from frozen backbone\n",
    "        backbone_features = self.backbone(x)  # [B, 640, 36, 36]\n",
    "        \n",
    "        # Process through forgetting adapter (gating + task prediction)\n",
    "        forgetting_features, not_forgetting_features, adapter_predictions = \\\n",
    "            self.forgetting_adapter(backbone_features, use_gating)\n",
    "        \n",
    "        # Process through convolutional branches\n",
    "        forgetting_output = self.branch_forgetting(forgetting_features)      # [B, 128]\n",
    "        not_forgetting_output = self.branch_not_forgetting(not_forgetting_features)  # [B, 128]\n",
    "        \n",
    "        # Combine branch outputs for final prediction\n",
    "        fused_features = self.fusion(forgetting_output, not_forgetting_output)\n",
    "        final_output = self.head(fused_features)\n",
    "        \n",
    "        return {\n",
    "            'output': final_output,\n",
    "            'adapter_output': adapter_predictions,\n",
    "            'backbone_features': backbone_features,\n",
    "            'forgetting_features': forgetting_features,\n",
    "            'not_forgetting_features': not_forgetting_features\n",
    "        }\n",
    "    \n",
    "    def store_adapter_state(self):\n",
    "        \"\"\"Delegate to forgetting adapter\"\"\"\n",
    "        self.forgetting_adapter.store_adapter_state()\n",
    "    \n",
    "    def compute_forgetting_scores(self):\n",
    "        \"\"\"Delegate to forgetting adapter\"\"\"\n",
    "        self.forgetting_adapter.compute_forgetting_scores()\n",
    "\n",
    "    @property\n",
    "    def current_forgetting_scores(self):\n",
    "        \"\"\"Access forgetting scores from adapter\"\"\"\n",
    "        return self.forgetting_adapter.current_forgetting_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b41e1",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize your model\n",
    "# cf_model = CatastrophicForgettingDisentanglementModel(\n",
    "#     backbone_output_channels=640,\n",
    "#     branch_hidden_channels=256,\n",
    "#     branch_output_channels=128,\n",
    "#     num_outputs=9\n",
    "# ).to(device)\n",
    "\n",
    "# # Set model to evaluation mode (important for testing)\n",
    "# cf_model.eval()\n",
    "\n",
    "# # Create dummy input tensor matching your expected input size\n",
    "# # Your model expects: [batch_size, 3, height, width]\n",
    "# # Using 224x224 as we discussed for memory efficiency\n",
    "# batch_size = 2\n",
    "# dummy1 = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "# dummy2 = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "# dummy_input = (dummy1, dummy2)\n",
    "\n",
    "# print(f\"Input shape: {dummy_input[0].shape}\")\n",
    "\n",
    "# # Test forward pass without gating (first domain)\n",
    "# with torch.no_grad():\n",
    "#     outputs_no_gating = cf_model(dummy_input, use_gating=False)\n",
    "\n",
    "# print(\"✅ Forward pass without gating successful!\")\n",
    "# print(f\"Output shape: {outputs_no_gating['output'].shape}\")\n",
    "# print(f\"Adapter output shape: {outputs_no_gating['adapter_output'].shape}\")\n",
    "# print(f\"Backbone features shape: {outputs_no_gating['backbone_features'].shape}\")\n",
    "\n",
    "# # Test forward pass with gating (after first domain)\n",
    "# # First, simulate having forgetting scores\n",
    "# cf_model.forgetting_adapter.current_forgetting_scores = torch.randn(640).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs_with_gating = cf_model(dummy_input, use_gating=True)\n",
    "\n",
    "# print(\"✅ Forward pass with gating successful!\")\n",
    "# print(f\"Forgetting features shape: {outputs_with_gating['forgetting_features'].shape}\")\n",
    "# print(f\"Not forgetting features shape: {outputs_with_gating['not_forgetting_features'].shape}\")\n",
    "\n",
    "# # Test forgetting measurement functions\n",
    "# cf_model.store_adapter_state()\n",
    "# print(\"✅ Adapter state stored successfully!\")\n",
    "\n",
    "# cf_model.compute_forgetting_scores()\n",
    "# print(\"✅ Forgetting scores computed successfully!\")\n",
    "# print(f\"Forgetting scores shape: {cf_model.current_forgetting_scores.shape}\")\n",
    "\n",
    "# outputs = cf_model(dummy_input, use_gating=True)\n",
    "# print(outputs['output'].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2d6c1",
   "metadata": {},
   "source": [
    "#### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cf(model, dataloader, criterion, device):\n",
    "    \"\"\"Modified evaluation function for catastrophic forgetting model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for yolo_images, depth_images, labels, _ in dataloader:\n",
    "            yolo_images = yolo_images.to(device, dtype=torch.float32)\n",
    "            depth_images = depth_images.to(device, dtype=torch.float32)\n",
    "            inputs = (yolo_images, depth_images)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs, use_gating=True)  # Use gating during evaluation\n",
    "            loss = criterion(outputs['output'], labels)\n",
    "            total_loss += loss.item() * inputs[0].size(0)\n",
    "            total_samples += inputs[0].size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def cross_domain_validation_cf(model, domain_dataloaders, criterion, device):\n",
    "    \"\"\"Modified cross-domain validation for catastrophic forgetting model\"\"\"\n",
    "    results = {}\n",
    "    for domain, loaders in domain_dataloaders.items():\n",
    "        val_loader = loaders['val']\n",
    "        val_loss = evaluate_model_cf(model, val_loader, criterion, device)\n",
    "        results[domain] = val_loss\n",
    "    return results\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    avg_metrics = {}\n",
    "    for k in keys:\n",
    "        avg_metrics[k] = float(np.mean([m[k] for m in metrics_list if k in m]))\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12b00",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd794edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gating_loss(forgetting_scores: torch.Tensor, \n",
    "                       gating_mask: torch.Tensor,\n",
    "                       lambda_balance: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"Compute loss for gating network\"\"\"\n",
    "    high_forgetting_features = forgetting_scores * gating_mask\n",
    "    low_forgetting_features = forgetting_scores * (1 - gating_mask)\n",
    "    \n",
    "    high_mean = high_forgetting_features.sum() / (gating_mask.sum() + 1e-8)\n",
    "    low_mean = low_forgetting_features.sum() / ((1 - gating_mask).sum() + 1e-8)\n",
    "    \n",
    "    split_loss = -(high_mean - low_mean)\n",
    "    balance_loss = torch.abs(gating_mask.mean() - 0.5)\n",
    "    \n",
    "    total_loss = split_loss + lambda_balance * balance_loss\n",
    "    return total_loss\n",
    "\n",
    "def catastrophic_forgetting_batch(model, batch, device, loss_params={'main': 1.0, 'adapter': 0.5, 'gating': 0.1}, **kwargs):\n",
    "    \"\"\"CORRECTED: Batch function with proper adapter access\"\"\"\n",
    "    yolo_images, depth_images, labels, domain_labels = batch\n",
    "    yolo_images = yolo_images.to(device)\n",
    "    depth_images = depth_images.to(device)\n",
    "    inputs = (yolo_images, depth_images)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    domain_idx = kwargs.get('domain_idx', 0)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs, use_gating=(domain_idx > 0))\n",
    "    \n",
    "    # Main task loss (from final head)\n",
    "    main_loss = mse_criterion(outputs['output'], labels)\n",
    "    \n",
    "    # Adapter loss (for monitoring forgetting)\n",
    "    adapter_loss = mse_criterion(outputs['adapter_output'], labels)\n",
    "    \n",
    "    # Main loss backpropagation\n",
    "    main_loss.backward(retain_graph=True) \n",
    "    \n",
    "    # CORRECTED: Gating loss with proper path access\n",
    "    gating_loss = torch.tensor(0.0, device=device)\n",
    "    if domain_idx > 0 and model.current_forgetting_scores is not None:\n",
    "        backbone_features = outputs['backbone_features']\n",
    "        batch_size = backbone_features.size(0)\n",
    "        forgetting_scores_batch = model.current_forgetting_scores.unsqueeze(0).expand(batch_size, -1).to(device)\n",
    "        \n",
    "        # FIXED: Use correct path to gating network\n",
    "        gating_mask = model.forgetting_adapter.gating(forgetting_scores_batch)\n",
    "        \n",
    "        gating_loss = compute_gating_loss(\n",
    "            model.current_forgetting_scores.to(device), \n",
    "            gating_mask.mean(dim=0)\n",
    "        )\n",
    "    \n",
    "    metrics = {\n",
    "        'main_loss': main_loss.item(),\n",
    "        'adapter_loss': adapter_loss.item(),\n",
    "        'gating_loss': gating_loss.item()\n",
    "    }\n",
    "    \n",
    "    return main_loss, gating_loss, metrics\n",
    "\n",
    "def catastrophic_forgetting_train_loop(\n",
    "    model, domains, domain_dataloaders, buffer, optimizer, gating_optimizer, device,\n",
    "    batch_fn, batch_kwargs, loss_params, num_epochs=5, exp_name=\"cf_exp\", \n",
    "    gradient_clipping=False, restart={}\n",
    "):\n",
    "    \"\"\"CORRECTED: Training loop with proper forgetting measurement\"\"\"\n",
    "    start_domain_idx = 0\n",
    "    global_step = 0\n",
    "    history = {\n",
    "        'train_epoch_loss': [],\n",
    "        'val_epoch_loss': [],\n",
    "        'train_epoch_metrics': [],\n",
    "        'cross_domain_val': [],\n",
    "        'grad_norms': [],\n",
    "        'forgetting_scores_history': [],\n",
    "        'gating_losses': []\n",
    "    }\n",
    "    \n",
    "    if restart:\n",
    "        global_step = restart['global_step']\n",
    "        history = restart['history']\n",
    "        start_domain_idx = np.where(domains == restart['domain'])[0][0]\n",
    "        for domain_idx, current_domain in enumerate(domains[:start_domain_idx]):\n",
    "            buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset) \n",
    "        print(f\"Restarting from domain {restart['domain']} index {start_domain_idx}\")\n",
    "        print(f\"Buffer: {buffer.get_domain_distribution()}\")         \n",
    "\n",
    "    for domain_idx, current_domain in enumerate(tqdm(domains[start_domain_idx:], desc=f\"Total training\"), start=start_domain_idx):\n",
    "        print(f\"\\n=== Training on Domain {domain_idx}: {current_domain} ===\")\n",
    "        \n",
    "        # Store adapter state before training (for forgetting measurement)\n",
    "        if domain_idx > 0:\n",
    "            model.store_adapter_state()\n",
    "        \n",
    "        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        \n",
    "        for epoch in trange(num_epochs, desc=f\"Current domain {current_domain}\"):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            samples = 0\n",
    "            batch_metrics_list = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Current epoch {epoch}\", leave=False)):\n",
    "                # Main model training\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_kwargs_with_domain = {**batch_kwargs, 'current_domain': current_domain, 'domain_idx': domain_idx}\n",
    "                \n",
    "                main_loss, gating_loss, metrics = batch_fn(model, batch, device, loss_params, **batch_kwargs_with_domain)\n",
    "                \n",
    "                if gradient_clipping:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Gating training (in same batch, separate optimizer)\n",
    "                if domain_idx > 0 and gating_loss.item() > 0:\n",
    "                    gating_optimizer.zero_grad()\n",
    "                    gating_loss.backward()\n",
    "                    gating_optimizer.step()\n",
    "                \n",
    "                batch_size = batch[0].size(0)\n",
    "                epoch_loss += main_loss.item() * batch_size\n",
    "                samples += batch_size\n",
    "                global_step += 1\n",
    "                batch_metrics_list.append(metrics)\n",
    "                \n",
    "            avg_epoch_loss = epoch_loss / samples\n",
    "            history['train_epoch_loss'].append(avg_epoch_loss)\n",
    "            \n",
    "            avg_metrics = average_metrics(batch_metrics_list)\n",
    "            history['train_epoch_metrics'].append(avg_metrics)\n",
    "            \n",
    "            grad_norms = collect_gradients(model)\n",
    "            history['grad_norms'].append(grad_norms)\n",
    "            \n",
    "            # Validation on current domain\n",
    "            val_loss = evaluate_model_cf(model, domain_dataloaders[current_domain]['val'], batch_kwargs['mse_criterion'], device)\n",
    "            history['val_epoch_loss'].append(val_loss)\n",
    "            \n",
    "            # Cross-domain validation (after each domain)\n",
    "            if epoch == num_epochs-1:\n",
    "                cross_val = cross_domain_validation_cf(model, domain_dataloaders, batch_kwargs['mse_criterion'], device)\n",
    "                history['cross_domain_val'].append(cross_val)\n",
    "\n",
    "                # Compute forgetting scores after training on domain (for next domain)\n",
    "                model.compute_forgetting_scores()\n",
    "                history['forgetting_scores_history'].append(\n",
    "                    model.current_forgetting_scores.clone() if model.current_forgetting_scores is not None else None\n",
    "                )\n",
    "                if model.current_forgetting_scores is not None:\n",
    "                    print(f\"Forgetting scores computed. Mean: {model.current_forgetting_scores.mean():.4f}, \"\n",
    "                        f\"Std: {model.current_forgetting_scores.std():.4f}\")\n",
    "                \n",
    "\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'gating_optimizer_state_dict': gating_optimizer.state_dict(),\n",
    "                    'history': history,\n",
    "                    'forgetting_scores': model.current_forgetting_scores,\n",
    "                }, f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pt\")\n",
    "            \n",
    "            with open(f\"../checkpoints/{exp_name}_history.pkl\", \"wb\") as f:\n",
    "                pickle.dump(history, f)\n",
    "        \n",
    "        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n",
    "        print(f\"Domain {domain_idx} completed. Buffer: {buffer.get_domain_distribution()}\")\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82339e1",
   "metadata": {},
   "source": [
    "### Explicit Heuristic Split Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb05cd5",
   "metadata": {},
   "source": [
    "#### ZoeDepth - HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a4bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "# from torchvision import transforms\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# output_dir = Path('../data/depth')\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", use_fast=True)\n",
    "# model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(device).eval()\n",
    "\n",
    "# # Prepare image paths list\n",
    "# image_paths = df['image_path'].tolist()\n",
    "\n",
    "# batch_size = 10  # or whatever batch size you want\n",
    "# for batch_idx in tqdm(range(0, len(image_paths), batch_size)):\n",
    "#     batch_paths = image_paths[batch_idx:batch_idx + batch_size]\n",
    "    \n",
    "#     # Load images as PIL Images (no manual transform)\n",
    "#     batch_images = [Image.open(img_path).convert(\"RGB\") for img_path in batch_paths]\n",
    "    \n",
    "#     # Preprocess with ZoeDepth image processor\n",
    "#     inputs = image_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # Post-process depth maps to original sizes\n",
    "#     source_sizes = [(img.height, img.width) for img in batch_images]\n",
    "#     post_processed = image_processor.post_process_depth_estimation(\n",
    "#         outputs,\n",
    "#         source_sizes=source_sizes\n",
    "#     )\n",
    "    \n",
    "#     for i, depth_dict in enumerate(post_processed):\n",
    "#         # Get raw depth map\n",
    "#         depth_array = depth_dict[\"predicted_depth\"].cpu().numpy()\n",
    "#         img_stem = Path(batch_paths[i]).stem\n",
    "#         np.save(output_dir / f\"{img_stem}.npy\", depth_array)\n",
    "#         # Save visualization PNG\n",
    "#         depth_norm = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min())\n",
    "#         depth_img = Image.fromarray((depth_norm * 255).astype(np.uint8))\n",
    "#         depth_img.save(output_dir / f\"{img_stem}_depth.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6ee64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "498f923f",
   "metadata": {},
   "source": [
    "#### segmentation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill.utils import plot\n",
    "import cv2\n",
    "import pickle\n",
    "import bz2\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858438c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an ontology to map class names to our GroundedSAM prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = GroundedSAM(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"human . child . person\": \"human\",\n",
    "            \"robot\": \"robot\",\n",
    "            \"dog\": \"dog\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# human : human, anima: animal, robot:robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run inference on a single image\n",
    "# results = base_model.predict(df['image_path'].iloc[0])\n",
    "\n",
    "# plot(\n",
    "#     image=cv2.imread(df['image_path'].iloc[0]),\n",
    "#     classes=base_model.ontology.classes(),\n",
    "#     detections=results\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf4520",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "    dataset_home = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a5b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bz2.BZ2File('autodistill_dataset.pbz2', 'wb') as f:\n",
    "#     pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3684de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_home = base_model.label(\"../../socialsense/data/images/home\", extension=\".png\")\n",
    "# with bz2.BZ2File('autodistill_dataset_home.pbz2', 'wb') as f:\n",
    "#     pickle.dump(dataset_home, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd0427",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"autodistill_temp_dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ee43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import os\n",
    "import supervision as sv\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Define a color palette for your classes\n",
    "# Use RGBA for masks (with transparency), RGB for boxes (solid)\n",
    "class_colors = {\n",
    "    0: (255, 0, 0, 100),    # Red for class 0 (human)\n",
    "    1: (0, 255, 0, 100),    # Green for class 1 (robot)\n",
    "    2: (0, 0, 255, 100),    # Blue for class 2 (animal)\n",
    "    3: (122, 122, 0, 100),\n",
    "    # Add more if you have more classes\n",
    "}\n",
    "\n",
    "def visualize_and_save_pil_colored(dataset, confidence_threshold=0.3, output_dir=\"../data/temp_masks_coloured\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, (path, image, detections) in enumerate(tqdm(dataset)):\n",
    "        # Convert BGR to RGB for PIL (if your image is BGR)\n",
    "        rgb_image = image[:, :, ::-1]\n",
    "        pil_img = Image.fromarray(rgb_image)\n",
    "        draw = ImageDraw.Draw(pil_img, \"RGBA\")\n",
    "        \n",
    "        # Filter detections by confidence\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        if not keep_indices:\n",
    "            print(f\"No detections above threshold for image {path}\")\n",
    "            continue\n",
    "\n",
    "        filtered_boxes = detections.xyxy[keep_indices]\n",
    "        filtered_masks = detections.mask[keep_indices]\n",
    "        filtered_confidences = detections.confidence[keep_indices]\n",
    "        filtered_class_ids = detections.class_id[keep_indices]\n",
    "        \n",
    "        # Overlay masks with transparency and class colors\n",
    "        for i, (mask, conf) in enumerate(zip(filtered_masks, filtered_confidences)):\n",
    "            class_id = filtered_class_ids[i] if filtered_class_ids is not None else 0\n",
    "            color = class_colors.get(class_id, (255, 255, 255, 100))  # Default white if class unknown\n",
    "            # Create a colored mask with alpha\n",
    "            mask_img = Image.fromarray((mask * 255).astype(np.uint8), mode=\"L\")\n",
    "            colored_mask = Image.new(\"RGBA\", pil_img.size, color)\n",
    "            # Composite the colored mask onto the image with transparency\n",
    "            pil_img = Image.alpha_composite(pil_img.convert(\"RGBA\"), Image.composite(colored_mask, Image.new(\"RGBA\", pil_img.size), mask_img))\n",
    "        \n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        \n",
    "        # Draw bounding boxes and confidence with class colors\n",
    "        for i, (box, conf) in enumerate(zip(filtered_boxes, filtered_confidences)):\n",
    "            class_id = filtered_class_ids[i] if filtered_class_ids is not None else 0\n",
    "            color = class_colors[class_id][:3]  # Use RGB for boxes (no alpha)\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
    "            draw.text((x1, y1 - 10*class_id), f\"{class_id}: {conf:.2f}\", fill=color)\n",
    "        \n",
    "        # Save the image as PNG\n",
    "        filename = os.path.basename(path)\n",
    "        save_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \".png\")\n",
    "        pil_img.convert(\"RGB\").save(save_path)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a90879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from supervision import Detections\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union for two boxes [x1,y1,x2,y2]\"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    return inter_area / (box1_area + box2_area - inter_area + 1e-6)\n",
    "\n",
    "def remove_duplicates_any_class(detections: Detections, iou_threshold=0.9) -> Detections:\n",
    "    \"\"\"\n",
    "    Remove duplicate detections using IoU and confidence, regardless of class.\n",
    "    \n",
    "    Args:\n",
    "        detections: supervision.Detections object\n",
    "        iou_threshold: IoU threshold for considering duplicates\n",
    "        \n",
    "    Returns:\n",
    "        Filtered Detections object\n",
    "    \"\"\"\n",
    "    # Extract components from Detections\n",
    "    xyxy = detections.xyxy\n",
    "    confidence = detections.confidence\n",
    "    class_id = detections.class_id\n",
    "    mask = detections.mask\n",
    "\n",
    "    # Convert to list of dicts for processing\n",
    "    detections_list = [\n",
    "        {\n",
    "            'box': xyxy[i],\n",
    "            'mask': mask[i] if mask is not None else None,\n",
    "            'confidence': confidence[i],\n",
    "            'class_id': class_id[i]\n",
    "        }\n",
    "        for i in range(len(xyxy))\n",
    "    ]\n",
    "\n",
    "    # Sort by confidence (highest first)\n",
    "    detections_list.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Filter duplicates (do NOT check class_id)\n",
    "    keep = []\n",
    "    while detections_list:\n",
    "        current = detections_list.pop(0)\n",
    "        keep.append(current)\n",
    "        detections_list = [\n",
    "            d for d in detections_list\n",
    "            if calculate_iou(current['box'], d['box']) <= iou_threshold\n",
    "        ]\n",
    "\n",
    "    # Reconstruct Detections object\n",
    "    return Detections(\n",
    "        xyxy=np.array([d['box'] for d in keep]),\n",
    "        confidence=np.array([d['confidence'] for d in keep]),\n",
    "        class_id=np.array([d['class_id'] for d in keep]),\n",
    "        mask=np.array([d['mask'] for d in keep]) if mask is not None else None\n",
    "    )\n",
    "\n",
    "# Usage example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = []\n",
    "for path, image, detections in dataset_746:\n",
    "    filtered_detections = remove_duplicates_any_class(detections, iou_threshold=0.95)\n",
    "    filtered_dataset.append((path, image, filtered_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5746dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_and_save_pil_colored(dataset, confidence_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abbcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2ab6bf",
   "metadata": {},
   "source": [
    "#### depth mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b30629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_mean_std_for_npy(folder_path):\n",
    "    total_sum = 0\n",
    "    total_sum_sq = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # List all .npy files\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "    \n",
    "    # Wrap the loop with tqdm for progress bar\n",
    "    for filename in tqdm(files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        img = np.load(file_path).astype(np.float64)\n",
    "        total_sum += img.sum()\n",
    "        total_sum_sq += (img ** 2).sum()\n",
    "        total_count += img.size\n",
    "\n",
    "    mean = total_sum / total_count if total_count > 0 else None\n",
    "    variance = (total_sum_sq / total_count) - (mean ** 2) if total_count > 0 else None\n",
    "    std = np.sqrt(variance) if variance is not None else None\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d709f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_mean_std_for_npy('../data/depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64981e76",
   "metadata": {},
   "source": [
    "#### mask fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_segmentation_data(detections_dataset, output_dir, imagenet_mean=[0.485, 0.456, 0.406], confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process supervision Detections dataset to create social and environment images\n",
    "    \n",
    "    Args:\n",
    "        detections_dataset: List of tuples (image_path, Detection_object, ...)\n",
    "        output_dir: Base directory to save processed images\n",
    "        imagenet_mean: RGB mean values for filling masked areas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directories\n",
    "    social_dir = Path(output_dir) / \"social\"\n",
    "    env_dir = Path(output_dir) / \"environment\"\n",
    "    social_dir.mkdir(parents=True, exist_ok=True)\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = item[0]\n",
    "        detections = item[2]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = original_image.shape[:2]\n",
    "        \n",
    "        # Get filename for saving\n",
    "        filename = Path(image_path).name\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        filtered_masks = detections.mask[keep_indices]\n",
    "        \n",
    "        # Combine all masks into one\n",
    "        combined_mask = combine_masks(filtered_masks, height, width)\n",
    "        \n",
    "        # Create social image (only people and robot visible)\n",
    "        social_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=True\n",
    "        )\n",
    "        \n",
    "        # Create environment image (room only, people and robot masked out)\n",
    "        env_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=False\n",
    "        )\n",
    "        \n",
    "        # Save images\n",
    "        social_path = social_dir / filename\n",
    "        env_path = env_dir / filename\n",
    "        \n",
    "        save_image(social_image, social_path)\n",
    "        save_image(env_image, env_path)\n",
    "\n",
    "\n",
    "def combine_masks(masks, height, width, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Combine multiple masks into a single binary mask using union operation\n",
    "    \n",
    "    Args:\n",
    "        masks: Array of individual masks from Detection object\n",
    "        height, width: Dimensions of the original image\n",
    "        \n",
    "    Returns:\n",
    "        combined_mask: Single binary mask (1 = object, 0 = background)\n",
    "    \"\"\"\n",
    "    if masks is None or len(masks) == 0:\n",
    "        return np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Initialize combined mask\n",
    "    combined_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Union all individual masks using maximum function (as shown in search results)\n",
    "    for mask in masks:\n",
    "        # Ensure mask is the right size\n",
    "        if mask.shape != (height, width):\n",
    "            raise ValueError(f\"Mask shape incorrect: {mask.shape}, should be {(height, width)}\")\n",
    "        \n",
    "        # Union operation: take maximum of current combined mask and new mask\n",
    "        combined_mask = np.maximum(combined_mask, mask.astype(np.uint8))\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "def apply_mask_with_mean(image, mask, imagenet_mean, keep_masked=True):\n",
    "    \"\"\"\n",
    "    Apply mask to image and fill empty areas with ImageNet mean values\n",
    "    \n",
    "    Args:\n",
    "        image: Original RGB image (H, W, 3)\n",
    "        mask: Binary mask (H, W) where 1 = object, 0 = background\n",
    "        imagenet_mean: RGB mean values [R, G, B] in range [0, 1]\n",
    "        keep_masked: If True, keep masked areas (social). If False, remove masked areas (environment)\n",
    "        \n",
    "    Returns:\n",
    "        processed_image: Image with mask applied and filled with mean values\n",
    "    \"\"\"\n",
    "    processed_image = image.copy().astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert imagenet_mean to same range as image\n",
    "    mean_values = np.array(imagenet_mean).reshape(1, 1, 3)\n",
    "    \n",
    "    if keep_masked:\n",
    "        # Social image: keep people/robot, fill background with mean\n",
    "        fill_mask = (mask == 0)  # Areas to fill (background)\n",
    "    else:\n",
    "        # Environment image: keep background, fill people/robot with mean  \n",
    "        fill_mask = (mask == 1)  # Areas to fill (people/robot)\n",
    "    \n",
    "    # Fill specified areas with ImageNet mean values\n",
    "    for c in range(3):  # RGB channels\n",
    "        processed_image[:, :, c][fill_mask] = imagenet_mean[c]\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    processed_image = (processed_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return processed_image\n",
    "\n",
    "def save_image(image, save_path):\n",
    "    \"\"\"\n",
    "    Save image to specified path\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        save_path: Path to save the image\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image and save\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d59426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bz2.BZ2File('autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "#     dataset_home = pickle.load(f)\n",
    "\n",
    "# process_segmentation_data(\n",
    "#     detections_dataset=dataset_home,\n",
    "#     output_dir='../data/masked',\n",
    "#     imagenet_mean=[0.485, 0.456, 0.406]  # ImageNet RGB means\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59184030",
   "metadata": {},
   "source": [
    "#### dualbranch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_layer_size(input, output, n_layers):\n",
    "    start_exp = (output + 1).bit_length()\n",
    "    end_exp = (input - 1).bit_length()\n",
    "    \n",
    "    total_powers = end_exp - start_exp\n",
    "    if total_powers < n_layers:\n",
    "        return None\n",
    "\n",
    "    result = []\n",
    "    denominator = n_layers + 1\n",
    "    half_denominator = denominator // 2\n",
    "\n",
    "    for i in range(1, n_layers + 1):\n",
    "        numerator = i * total_powers + half_denominator #works same as rounding\n",
    "        idx = numerator // denominator\n",
    "        power = 1 << (start_exp + idx)\n",
    "        result.append(power)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "intermediate_layer_size(1280+64, 9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d27c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class DualBranchModel(nn.Module):\n",
    "    def __init__(self, num_outputs=9, dropout_rate=0.3, architecture={'env':'lightweight', 'head':'deep'}):\n",
    "        super(DualBranchModel, self).__init__()\n",
    "        self.setup = architecture\n",
    "        \n",
    "        self.social_branch = nn.Sequential(\n",
    "            models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features,\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        soc_feature_dim = 1280\n",
    "        \n",
    "\n",
    "        if self.setup['env'] == 'lightweight':\n",
    "            self.env_branch = nn.Sequential(\n",
    "                models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features,\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            env_feature_dim = 1280\n",
    "\n",
    "        elif self.setup['env'] == 'label':\n",
    "            max_rooms = 30\n",
    "            self.env_branch = nn.Embedding(max_rooms, 64)\n",
    "            env_feature_dim = 64\n",
    "\n",
    "\n",
    "        self.fusion_dim = soc_feature_dim + env_feature_dim\n",
    "\n",
    "        layers = intermediate_layer_size(soc_feature_dim, num_outputs, 1)\n",
    "        self.social_classifier = nn.Sequential(\n",
    "            nn.Linear(soc_feature_dim, layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[0], num_outputs)\n",
    "        )\n",
    "        layers = intermediate_layer_size(env_feature_dim, num_outputs, 1)\n",
    "        self.env_classifier = nn.Sequential(\n",
    "            nn.Linear(env_feature_dim, layers[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(layers[0], num_outputs)\n",
    "        )\n",
    "        \n",
    "        if self.setup['head'] == 'deep':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(self.fusion_dim, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                \n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                \n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                \n",
    "                nn.Linear(128, num_outputs)\n",
    "            )\n",
    "        elif self.setup['head'] == 'shallow':\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(self.fusion_dim, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, num_outputs)\n",
    "            )\n",
    "        \n",
    "    def forward(self, social_img, env_img):\n",
    "        social_features = self.social_branch(social_img)\n",
    "        env_features = self.env_branch(env_img)\n",
    "        \n",
    "        fused_features = torch.cat([social_features, env_features], dim=1)\n",
    "        scores = self.head(fused_features)\n",
    "\n",
    "        social_class = self.social_classifier(social_features.detach())\n",
    "        env_class = self.env_classifier(env_features.detach())\n",
    "        \n",
    "        return {\n",
    "            'output': scores,\n",
    "            'invariant_domain': social_class,\n",
    "            'specific_domain': env_class,\n",
    "            'invariant_feats': social_features,\n",
    "            'specific_feats': env_features\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849e928",
   "metadata": {},
   "source": [
    "## ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_fold_histories(fold_history):\n",
    "    \"\"\"\n",
    "    Combine multiple training histories by averaging them element-wise.\n",
    "    Handles both simple lists and lists of dictionaries.\n",
    "    \"\"\"\n",
    "    combined_history = {}\n",
    "    metric_keys = list(fold_history[0].keys())\n",
    "    \n",
    "    for key in metric_keys:\n",
    "        # Check if this metric contains dictionaries\n",
    "        first_element = fold_history[0][key][0] if fold_history[0][key] else None\n",
    "        \n",
    "        if isinstance(first_element, dict):\n",
    "            # Handle lists of dictionaries (train_epoch_metrics, grad_norms)\n",
    "            combined_history[key] = average_list_of_dicts(fold_history, key)\n",
    "        else:\n",
    "            # Handle simple lists (train_epoch_loss, val_epoch_loss, cross_domain_val)\n",
    "            stacked_metrics = np.stack([fold_history[fold][key] for fold in fold_history])\n",
    "            combined_history[key] = np.mean(stacked_metrics, axis=0).tolist()\n",
    "    \n",
    "    return combined_history\n",
    "\n",
    "def average_list_of_dicts(fold_history, metric_key):\n",
    "    \"\"\"\n",
    "    Average a list of dictionaries across folds.\n",
    "    \"\"\"\n",
    "    # Get the dictionary keys from the first fold's first epoch\n",
    "    dict_keys = list(fold_history[0][metric_key][0].keys())\n",
    "    \n",
    "    # Convert each fold's list of dicts to a 2D numpy array\n",
    "    fold_arrays = []\n",
    "    for fold in fold_history:\n",
    "        # Convert list of dicts to 2D array: [epochs, sub_metrics]\n",
    "        fold_array = np.array([[epoch_dict[k] for k in dict_keys] \n",
    "                              for epoch_dict in fold_history[fold][metric_key]])\n",
    "        fold_arrays.append(fold_array)\n",
    "    \n",
    "    # Stack all folds and average: [folds, epochs, sub_metrics] -> [epochs, sub_metrics]\n",
    "    stacked = np.stack(fold_arrays)\n",
    "    averaged = np.mean(stacked, axis=0)\n",
    "    \n",
    "    # Convert back to list of dictionaries\n",
    "    result = []\n",
    "    for epoch_values in averaged:\n",
    "        epoch_dict = dict(zip(dict_keys, epoch_values))\n",
    "        result.append(epoch_dict)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return torch.sqrt(self.mse(x, y) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "df['image_path'] = '../' + df['image_path']\n",
    "\n",
    "# Initialize fold column\n",
    "df['fold'] = -1\n",
    "\n",
    "# Get unique image paths for splitting\n",
    "unique_images_df = df[['image_path', 'domain']].reset_index(drop=True)\n",
    "unique_images_df = unique_images_df[~unique_images_df['image_path'].isin(test_split_idx)]\n",
    "\n",
    "# Create stratified 5-fold splits based on domain\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Assign fold numbers based on domain stratification\n",
    "for fold, (_, val_idx) in enumerate(skf.split(unique_images_df['image_path'], unique_images_df['domain'])):\n",
    "    val_image_paths = unique_images_df.iloc[val_idx]['image_path'].tolist()\n",
    "    df.loc[df['image_path'].isin(val_image_paths), 'fold'] = fold\n",
    "\n",
    "# Verify domain distribution across folds\n",
    "print(\"Domain distribution across folds:\")\n",
    "print(df.groupby(['fold', 'domain']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_loaders = create_crossvalidation_loaders(df, 5, batch_sizes=(32, 64, 64), resize_img_to=(512, 288))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f961279",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0cba9f",
   "metadata": {},
   "source": [
    "### setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b327ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LGRBaseline\n",
    "def baseline_batch(model, batch, device, detach_base, binary, full_replay, loss_params, **kwargs):\n",
    "    inputs, labels, _ = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model(inputs)['output']\n",
    "    loss = kwargs['mse_criterion'](outputs, labels)\n",
    "    loss.backward()\n",
    "    metrics = {}\n",
    "    return loss, metrics\n",
    "\n",
    "#For DANN\n",
    "def dann_batch(model, batch, device, detach_base, binary, full_replay, loss_params={'head':0.5, 'social':0.5}, **kwargs):\n",
    "    inputs, labels, domain_labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    domain_to_idx = kwargs['domain_to_idx']\n",
    "    domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    bce_criterion = kwargs['bce_criterion']\n",
    "    alpha = kwargs['alpha']\n",
    "\n",
    "    current_domain = kwargs['current_domain']\n",
    "    current_binary_labels = (domain_labels == domain_to_idx[current_domain]).float()\n",
    "    is_first_domain = bool(domain_to_idx[current_domain] == 0)\n",
    "\n",
    "    outputs = model(inputs, alpha=alpha, is_first_domain=is_first_domain)\n",
    "\n",
    "    task_loss = mse_criterion(outputs['output'], labels)\n",
    "\n",
    "    if is_first_domain:\n",
    "        inv_domain_loss = 0\n",
    "        inv_acc = 0\n",
    "    else:\n",
    "        inv_domain_loss = bce_criterion(outputs['invariant_domain'].squeeze(), current_binary_labels)\n",
    "        preds = (outputs['invariant_domain'].squeeze() > 0).float()\n",
    "        inv_acc = (preds == current_binary_labels).float().mean().item()\n",
    "    \n",
    "    total_loss = (loss_params['head'] * task_loss +\n",
    "                    loss_params['social'] * inv_domain_loss)\n",
    "\n",
    "    total_loss.backward()\n",
    "    \n",
    "    metrics = {\n",
    "        'task_loss': task_loss.item(),\n",
    "        'inv_domain': 0 if is_first_domain else inv_domain_loss.item(),\n",
    "        'inv_acc': inv_acc\n",
    "    }\n",
    "    return total_loss, metrics\n",
    "\n",
    "def heuristic_dualbranch_batch(model, batch, device, detach_base, binary, full_replay, loss_params={}, **kwargs):\n",
    "    inputs1, inputs2, labels, domain_labels = batch\n",
    "    inputs1, inputs2, labels, domain_labels = inputs1.to(device), inputs2.to(device), labels.to(device), domain_labels.to(device)\n",
    "    domain_to_idx = kwargs['domain_to_idx']\n",
    "    # domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    ce_criterion = kwargs['ce_criterion']\n",
    "\n",
    "    outputs = model(inputs1, inputs2)\n",
    "\n",
    "    loss = mse_criterion(outputs['output'], labels)\n",
    "    loss.backward()\n",
    "\n",
    "    class_optimiser = kwargs['class_optimizer']\n",
    "    class_optimiser.zero_grad()\n",
    "    inv_domain_loss = ce_criterion(outputs['invariant_domain'], domain_labels)\n",
    "    spec_domain_loss = ce_criterion(outputs['specific_domain'], domain_labels)\n",
    "    (inv_domain_loss + spec_domain_loss).backward()\n",
    "    class_optimiser.step()\n",
    "    inv_acc = (outputs['invariant_domain'].argmax(1) == domain_labels).float().mean().item()\n",
    "    spec_acc = (outputs['specific_domain'].argmax(1) == domain_labels).float().mean().item()\n",
    "    \n",
    "    metrics = {\n",
    "        'inv_domain': inv_domain_loss.item(),\n",
    "        'spec_domain': spec_domain_loss.item(),\n",
    "        'inv_acc': inv_acc,\n",
    "        'spec_acc': spec_acc\n",
    "    }\n",
    "    return loss, metrics\n",
    "\n",
    "\n",
    "# For DualBranchNet\n",
    "def dualbranch_batch(model, batch, device, detach_base, binary, full_replay, loss_params={'head': 1, 'social': 0.5, 'room': 0.2}, **kwargs):\n",
    "    inputs, labels, domain_labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    domain_to_idx = kwargs['domain_to_idx']\n",
    "    domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    ce_criterion = kwargs['ce_criterion']\n",
    "    cos_criterion = kwargs['cos_criterion']\n",
    "    alpha = kwargs['alpha']\n",
    "    if binary:\n",
    "        bce_criterion = kwargs['bce_criterion']\n",
    "\n",
    "    # Split batch\n",
    "    current_domain = kwargs['current_domain']\n",
    "    current_binary_labels = (domain_labels == domain_to_idx[current_domain]).float()\n",
    "    current_mask = (domain_labels == domain_to_idx[current_domain])\n",
    "\n",
    "    if full_replay:\n",
    "        current_mask = torch.ones_like(current_mask, dtype=torch.bool)\n",
    "\n",
    "    replay_mask = ~current_mask\n",
    "\n",
    "    # 1. Current samples: update all parameters\n",
    "    if current_mask.any():\n",
    "        inputs_current = inputs[current_mask]\n",
    "        labels_current = labels[current_mask]\n",
    "        domain_labels_current = domain_labels[current_mask]\n",
    "\n",
    "        outputs_current = model(inputs_current, alpha=alpha)\n",
    "        inv_feats = outputs_current['invariant_feats']\n",
    "        spec_feats = outputs_current['specific_feats']\n",
    "\n",
    "        task_loss = mse_criterion(outputs_current['output'], labels_current)\n",
    "        if binary:\n",
    "            inv_domain_loss = bce_criterion(outputs_current['invariant_domain'].squeeze(), current_binary_labels[current_mask])\n",
    "        else:\n",
    "            inv_domain_loss = ce_criterion(outputs_current['invariant_domain'], domain_labels_current)\n",
    "        spec_domain_loss = ce_criterion(outputs_current['specific_domain'], domain_labels_current)\n",
    "        similarity_loss = cos_criterion(inv_feats, spec_feats)\n",
    "        \n",
    "        total_loss = (loss_params['head'] * task_loss +\n",
    "                      loss_params['social'] * inv_domain_loss +\n",
    "                      loss_params['room'] * spec_domain_loss)\n",
    "\n",
    "        total_loss.backward(retain_graph= not full_replay)\n",
    "        \n",
    "        if binary:\n",
    "            # Threshold at 0 (sigmoid(0) = 0.5)\n",
    "            preds = (outputs_current['invariant_domain'].squeeze() > 0).float()\n",
    "            inv_acc = (preds == current_binary_labels[current_mask]).float().mean().item()\n",
    "        else:\n",
    "            inv_acc = (outputs_current['invariant_domain'].argmax(1) == domain_labels_current).float().mean().item()\n",
    "        spec_acc = (outputs_current['specific_domain'].argmax(1) == domain_labels_current).float().mean().item()\n",
    "    else:\n",
    "        total_loss = torch.tensor(0.0, device=device)\n",
    "        inv_acc = 0.0\n",
    "        spec_acc = 0.0\n",
    "        task_loss = torch.tensor(0.0, device=device)\n",
    "        inv_domain_loss = torch.tensor(0.0, device=device)\n",
    "        spec_domain_loss = torch.tensor(0.0, device=device)\n",
    "        similarity_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "    # 2. Replay samples: update only specific branch + head\n",
    "    if replay_mask.any():\n",
    "        inputs_replay = inputs[replay_mask]\n",
    "        labels_replay = labels[replay_mask]\n",
    "        domain_labels_replay = domain_labels[replay_mask]\n",
    "\n",
    "        #no_grad, unlike requires_grad=False, detaches all elements from the gradient computation graph\n",
    "        with torch.no_grad():\n",
    "            base_replay = model.backbone(inputs_replay)\n",
    "            base_replay = model.pool(base_replay).flatten(1)\n",
    "            inv_feats_replay = model.invariant(base_replay)\n",
    "\n",
    "        specific_feats = model.specific(base_replay)\n",
    "        spec_domain_pred = model.specific_domain_classifier(specific_feats)\n",
    "        \n",
    "        if detach_base:\n",
    "            combined = torch.cat([inv_feats_replay, specific_feats, base_replay], dim=1)\n",
    "        else:\n",
    "            combined = torch.cat([inv_feats_replay, specific_feats], dim=1)  \n",
    "        \n",
    "        scores = model.head(combined)\n",
    "        \n",
    "        task_loss_replay = mse_criterion(scores, labels_replay)\n",
    "        spec_domain_loss_replay = ce_criterion(spec_domain_pred, domain_labels_replay)\n",
    "        total_loss_replay = task_loss_replay + 0.2 * spec_domain_loss_replay\n",
    "        \n",
    "        total_loss_replay.backward()\n",
    "    \n",
    "    metrics = {\n",
    "        'task_loss': task_loss.item(),\n",
    "        'inv_domain': inv_domain_loss.item(),\n",
    "        'spec_domain': spec_domain_loss.item(),\n",
    "        'similarity': similarity_loss.item(),\n",
    "        'inv_acc': inv_acc,\n",
    "        'spec_acc': spec_acc,\n",
    "        'replay_count': replay_mask.sum().item(),\n",
    "        'current_count': current_mask.sum().item()\n",
    "    }\n",
    "    return total_loss, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6df3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change dataset to return torch domain labels indexes not strings\n",
    "def evaluate_model(model, dataloader, criterion, device , tsne=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 4:\n",
    "                inputs1, inputs2, labels, domain_labels = batch\n",
    "                inputs1 = inputs1.to(device, dtype=torch.float32)\n",
    "                inputs2 = inputs2.to(device, dtype=torch.float32)\n",
    "                labels = labels.to(device, dtype=torch.float32)\n",
    "                inputs = (inputs1, inputs2)\n",
    "            elif len(batch) == 3:\n",
    "                inputs, labels, _ = batch\n",
    "                inputs = inputs.to(device, dtype=torch.float32)\n",
    "                labels = labels.to(device, dtype=torch.float32)\n",
    "                inputs = (inputs,)\n",
    "            else:\n",
    "                raise ValueError(f\"Batch contains {len(batch)} objects. Should contain 3 or 4 - image/two, labels, domain_labels\")\n",
    "\n",
    "            outputs = model(*inputs)['output']\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs[0].size(0)\n",
    "            total_samples += inputs[0].size(0)\n",
    "            \n",
    "            if tsne:\n",
    "                tsne['social'].append(outputs['invariant_feats'].cpu())\n",
    "                tsne['environmental'].append(outputs['specific_feats'].cpu())\n",
    "                tsne['domains'].append(domain_labels.cpu())\n",
    "\n",
    "            val_loss = total_loss / total_samples\n",
    "\n",
    "    return (val_loss, tsne) if tsne else val_loss\n",
    "\n",
    "def cross_domain_validation(model, domain_dataloaders, criterion, device, tsne=None):\n",
    "    results = {}\n",
    "    for domain, loaders in domain_dataloaders.items():\n",
    "        val_loader = loaders['val']\n",
    "        if tsne:\n",
    "            val_loss, tsne = evaluate_model(model, val_loader, criterion, device, tsne)\n",
    "        else:\n",
    "            val_loss = evaluate_model(model, val_loader, criterion, device)\n",
    "        results[domain] = val_loss\n",
    "    return (results, tsne) if tsne else results\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    # metrics_list: list of dicts, each dict contains metrics for a batch\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    avg_metrics = {}\n",
    "    for k in keys:\n",
    "        avg_metrics[k] = float(np.mean([m[k] for m in metrics_list if k in m]))\n",
    "    return avg_metrics\n",
    "\n",
    "def collect_tsne_features(model, loader, device):\n",
    "    model.eval()\n",
    "    all_inv, all_spec, all_domains = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for domain, loaders in domain_dataloaders.items():\n",
    "            loader = loaders['val']\n",
    "            for x, _, d in loader:\n",
    "                x = x.to(device)\n",
    "                out = model(x)\n",
    "                all_inv.append(out['invariant_feats'].cpu())\n",
    "                all_domains += list(d)\n",
    "    inv_feats = torch.cat(all_inv, dim=0).numpy()\n",
    "    return inv_feats, all_domains\n",
    "\n",
    "\n",
    "def collect_gradients(model):\n",
    "    grad_norms = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and not name.startswith(\"backbone\"):\n",
    "            module = name.split('.')[0]\n",
    "            norm = param.grad.norm(2).item()\n",
    "            if module not in grad_norms:\n",
    "                grad_norms[module] = []\n",
    "            grad_norms[module].append(norm)\n",
    "    # Take mean per module\n",
    "    grad_norms = {k: float(np.mean(v)) for k, v in grad_norms.items()}\n",
    "    return grad_norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "def unified_train_loop(\n",
    "    model, domains, domain_dataloaders, buffer, optimizer, writer, device,\n",
    "    batch_fn, batch_kwargs, loss_params, num_epochs=5, exp_name=\"exp\", gradient_clipping=False, detach_base=False, binary=False, full_replay=False, collect_tsne_data=False, restart={}, eval_buffer=False\n",
    "):\n",
    "    start_domain_idx = 0\n",
    "    global_step = 0\n",
    "    history = {\n",
    "        'train_epoch_loss': [],\n",
    "        'val_epoch_loss': [],\n",
    "        'val_buffer_epoch_loss': [],\n",
    "        'train_epoch_metrics': [],\n",
    "        'cross_domain_val': [],\n",
    "        'grad_norms': [],\n",
    "    }\n",
    "    \n",
    "    if restart:\n",
    "        # Populate history\n",
    "        global_step = restart['global_step']\n",
    "        history = restart['history']\n",
    "        # Populate buffer\n",
    "        start_domain_idx = np.where(domains == restart['domain'])[0][0]\n",
    "        for domain_idx, current_domain in enumerate(domains[:start_domain_idx]):\n",
    "            buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset) \n",
    "        print(f\"Restarting from domain {restart['domain']} index {start_domain_idx}\")\n",
    "        print(f\"Buffer: {buffer.get_domain_distribution()}\")         \n",
    "        \n",
    "\n",
    "    for domain_idx, current_domain in enumerate(tqdm(domains[start_domain_idx:], desc=f\"Total training\"), start=start_domain_idx):\n",
    "        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        if eval_buffer:\n",
    "            eval_loader = eval_buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['val'])\n",
    "        else:\n",
    "            eval_loader = domain_dataloaders[current_domain]['val']\n",
    "        len_dataloader = len(train_loader)\n",
    "        \n",
    "        for epoch in trange(num_epochs, desc=f\"Current domain {current_domain}\"):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            samples = 0\n",
    "            batch_metrics_list = []\n",
    "            \n",
    "            # for batch_idx, batch in enumerate(train_loader):\n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Current epoch {epoch}\", leave=False)):\n",
    "                if not batch_kwargs['alpha']:\n",
    "                    p = (epoch * len_dataloader + batch_idx) / (num_epochs * len_dataloader)\n",
    "                    alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "                else:\n",
    "                    alpha = batch_kwargs['alpha']\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss, metrics = batch_fn(model, batch, device, detach_base, binary, full_replay, loss_params, **{**batch_kwargs, 'current_domain': current_domain, 'alpha':alpha})\n",
    "                if gradient_clipping:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                batch_size = batch[0].size(0)\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                samples += batch_size\n",
    "                global_step += 1\n",
    "                batch_metrics_list.append(metrics)\n",
    "                # # TensorBoard logging (every 10 batches)\n",
    "                # if writer and batch_idx % 10 == 0:\n",
    "                #     writer.add_scalar(f'{exp_name}/train_loss', loss.item(), global_step)\n",
    "                #     for k, v in metrics.items():\n",
    "                #         writer.add_scalar(f'{exp_name}/train_{k}', v, global_step)\n",
    "            avg_epoch_loss = epoch_loss / samples\n",
    "            # writer.add_scalar(f'{exp_name}/train_epoch_loss', avg_epoch_loss, global_step)\n",
    "            history['train_epoch_loss'].append(avg_epoch_loss)\n",
    "            # Average batch metrics for this epoch\n",
    "            avg_metrics = average_metrics(batch_metrics_list)\n",
    "            history['train_epoch_metrics'].append(avg_metrics)\n",
    "\n",
    "            # Collect gradients\n",
    "            grad_norms = collect_gradients(model)\n",
    "            history['grad_norms'].append(grad_norms)\n",
    "\n",
    "            # Validation on current domain\n",
    "            val_loss = evaluate_model(model, domain_dataloaders[current_domain]['val'], batch_kwargs['mse_criterion'], device)\n",
    "            val_loss_buffer = evaluate_model(model, eval_loader, batch_kwargs['mse_criterion'], device)\n",
    "            # writer.add_scalar(f'{exp_name}/val_epoch_loss', val_loss, global_step)\n",
    "            history['val_epoch_loss'].append(val_loss)\n",
    "            history['val_buffer_epoch_loss'].append(val_loss_buffer)\n",
    "\n",
    "            # Collect data for t-SNE domain separation graphs\n",
    "            if collect_tsne_data:\n",
    "                inv_feats, domain_labels = collect_tsne_features(model, domain_dataloaders, device)\n",
    "                tsne_data = {\n",
    "                    'inv_feats': inv_feats,\n",
    "                    'domain_labels': domain_labels\n",
    "                }\n",
    "            else:\n",
    "                tsne_data = None\n",
    "\n",
    "            # Cross-domain validation (after each domain)\n",
    "            if epoch == num_epochs-1:\n",
    "                if collect_tsne_data:\n",
    "                    tsne = {'social': [], 'env': [], 'domains': []}\n",
    "                    cross_val, tsne_data = cross_domain_validation(model, domain_dataloaders, batch_kwargs['mse_criterion'], device, tsne)\n",
    "                else:\n",
    "                    cross_val = cross_domain_validation(model, domain_dataloaders, batch_kwargs['mse_criterion'], device)\n",
    "                history['cross_domain_val'].append(cross_val)\n",
    "\n",
    "                # Only save last model per domain to save space\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'history': history,\n",
    "                    'tsne' : tsne_data,\n",
    "                }, f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pt\")\n",
    "            # else:\n",
    "            #     # Save metrics\n",
    "            #     torch.save({\n",
    "            #         # 'model_state_dict': model.state_dict(),\n",
    "            #         # 'optimizer_state_dict': optimizer.state_dict(),\n",
    "            #         'history': history,\n",
    "            #         'tsne' : tsne_data,\n",
    "            #     }, f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pt\")\n",
    "            with open(f\"../checkpoints/{exp_name}_history.pkl\", \"wb\") as f:\n",
    "                pickle.dump(history, f)\n",
    "            \n",
    "        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n",
    "        eval_buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['val'].dataset)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1584bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a91eb94",
   "metadata": {},
   "source": [
    "### runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973cdbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "# # For baseline model\n",
    "# baseline_model = LGRBaseline().to(device)\n",
    "# optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "# buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "# exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "# baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "# unified_train_loop(\n",
    "#     model=baseline_model,\n",
    "#     domains=domains,\n",
    "#     domain_dataloaders=domain_dataloaders,\n",
    "#     buffer=buffer,\n",
    "#     optimizer=optimizer,\n",
    "#     writer=writer,\n",
    "#     device=device,\n",
    "#     batch_fn=baseline_batch,\n",
    "#     batch_kwargs=baseline_kwargs,\n",
    "#     num_epochs=10,\n",
    "#     exp_name=exp_name\n",
    "# )\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9dfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_norm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86449b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bbdetach_deep_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41678456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=False, layer_norm=False, detach_base=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bdetach_batch16_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(16, 64, 64), resize_img_to=(128, 128), seed=SEED)  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=True\n",
    ")\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128), seed=SEED)  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=True, freeze_base='partial').to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bdetach_pfrozen_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=True\n",
    ")\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full').to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f63e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_binary_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary = True,\n",
    "    full_replay = False\n",
    ")\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_binary_explicitgrl_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary=True,\n",
    "    full_replay = False\n",
    ")\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=True, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"freplay_ffrozen_binary_explicitgrl_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary=True,\n",
    "    full_replay = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'pretrained_backbone_cnn_branch': ('pretrained', 'simple', 'simple'), \n",
    "    # 'linear_branch': ('3conv', 'linear', 'simple'),\n",
    "    # 'cnn_branch': ('3conv', 'simple', 'simple'),\n",
    "    # 'adversarial': ('2conv', 'adversarial', 'adversarial'),\n",
    "    'cnn_specialised_branches': ('3conv', 'special', 'simple')\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=False,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'pretrained_simple': ('pretrained', 'simple', '3linear', False), \n",
    "    'pretrained_special': ('pretrained', 'special', '3linear', False),\n",
    "    '3conv_simple': ('3conv', 'simple', '3linear', True),\n",
    "    '3conv_special': ('3conv', 'special', '3linear', True),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end, detach_base) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'pretrained_simple_0.25branches': ('pretrained', 'simple', '3linear'), \n",
    "    # '3conv_adversarial': ('3conv', 'adversarial', 'adversarial'),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params = {'head': 0.5, 'social': 0.25, 'room': 0.25}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c84d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'pretrained_simple': ('pretrained', 'simple', '3linear'), \n",
    "    '3conv_adversarial': ('3conv', 'adversarial', 'adversarial'),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    fold_history = {}\n",
    "    for fold_num, domain_dataloaders in tqdm(fold_loaders.items()):\n",
    "        dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "        optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "        buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "        def cos_criterion(a, b):\n",
    "            return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "        exp_name = f\"fold{fold_num}_CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "        dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "        history = unified_train_loop(\n",
    "            model=dual_model,\n",
    "            domains=domains,\n",
    "            domain_dataloaders=domain_dataloaders,\n",
    "            buffer=buffer,\n",
    "            optimizer=optimizer,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            batch_fn=dualbranch_batch,\n",
    "            batch_kwargs=dualbranch_kwargs,\n",
    "            num_epochs=10,\n",
    "            exp_name=exp_name,\n",
    "            gradient_clipping=True,\n",
    "            detach_base=False,\n",
    "            binary = True,\n",
    "            full_replay = True,\n",
    "            collect_tsne_data=False,\n",
    "            loss_params = {'head': 0.5, 'social': 0.25, 'room': 0.25}\n",
    "        )\n",
    "        fold_history[fold_num] = history\n",
    "    with open(f\"../checkpoints/5foldcrossval_{exp_name}_history.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_DANN().to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "exp_name = f\"DANN_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "history = unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dann_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary = True,\n",
    "    full_replay = True,\n",
    "    collect_tsne_data=False,\n",
    "    loss_params = {'head': 1, 'social': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders, split_idx = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(512,288), seed=SEED, return_splits=True)  #512, 288 # 352,128 #LGR had 128,128 MobileNetv2 had 224, 224\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "baseline_model = LGRBaseline().to(device)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "unified_train_loop(\n",
    "    model=baseline_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=baseline_batch,\n",
    "    batch_kwargs=baseline_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    loss_params={}\n",
    ")\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders, split_idx = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(256,144), seed=SEED, return_splits=True)  #512, 288 # 352,128 #LGR had 128,128 MobileNetv2 had 224, 224\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "testing_scenarios = {\n",
    "    'simple': ('3conv', 'simple', False, 1000), \n",
    "    'simple_buffer05': ('3conv', 'simple', False, 500),\n",
    "    'special': ('3conv', 'special', False, 1000),\n",
    "    'special_bnorm': ('3conv', 'special', True, 1000)\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, batch_norm, buffer_size) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone=} {branch=} {batch_norm=} {buffer_size=}\")\n",
    "    dual_model = DualBranchNet_minimal(backbone_type=backbone, branch_type=branch, head_type='3layer', batch_norm=batch_norm).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"minimal_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=baseline_batch,\n",
    "        batch_kwargs=baseline_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dualbranch_CNN_1epoch': (DualBranchCNNNet('3conv', 'simple', 'simple'), 500, 1), \n",
    "    'minimal_absurd_buff500': (DualBranchNet_minimal(backbone_type='absurd', branch_type='absurd', head_type='absurd'), 500, 10),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=baseline_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24627314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'DANN_notrain1dom': (DualBranchNet_DANN(), 500, 10, 1)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dann_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':0.5, 'social':0.5}\n",
    "    )\n",
    "\n",
    "testing_scenarios = {\n",
    "    'dualbranch_CNN_1epoch': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='3linear'), 500, 1, 1), \n",
    "    'dualbranch_CNN_dynamicalpha': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='3linear'), 500, 10, 0), \n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':1, 'social':1, 'room':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    '3conv_simple_simple_bnorm_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, False), \n",
    "    '3conv_simple_simple_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=False), 500, 10, 1, False), \n",
    "    '3conv_simple_simple_bnorm': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, True),\n",
    "    '3conv_simple_3linear_bnorm_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, False),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, gradient_clip) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=gradient_clip,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':1, 'social':1, 'room':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e197d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(224,224), return_splits=False, double_img=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'deeplabv3mobilenetv3_dann': (DANN_classifier_poc(), 500, 30, 0, True),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, gradient_clip) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dann_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=gradient_clip,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=True,\n",
    "        loss_params={'head':1, 'social':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#didit work, the architecture is flawed. Assesing category of features based on catastrophic forgetting makes no sense, \n",
    "# the features are not experiencing CF, the weights are. \n",
    "# But passing weights to split branches is not justified.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(224,224), return_splits=False, double_img=False)\n",
    "\n",
    "# CORRECTED: Initialize model with proper parameter names\n",
    "cf_model = CatastrophicForgettingDisentanglementModel(\n",
    "    backbone_output_channels=640,\n",
    "    branch_hidden_channels=256,\n",
    "    branch_output_channels=128,\n",
    "    num_outputs=9\n",
    ").to(device)\n",
    "\n",
    "# CORRECTED: Optimizers with proper parameter paths\n",
    "main_optimizer = torch.optim.Adam([\n",
    "    {'params': cf_model.forgetting_adapter.adapter.parameters()},\n",
    "    {'params': cf_model.branch_forgetting.parameters()},\n",
    "    {'params': cf_model.branch_not_forgetting.parameters()},\n",
    "    {'params': cf_model.head.parameters()},\n",
    "    {'params': cf_model.fusion.parameters()}\n",
    "], lr=1e-3)\n",
    "\n",
    "gating_optimizer = torch.optim.Adam(\n",
    "    cf_model.forgetting_adapter.gating.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "restart = {\n",
    "'global_step':\n",
    "'history':\n",
    "'domain':\n",
    "}\n",
    "\n",
    "# Use your existing buffer and data\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "# Modified batch kwargs for our model\n",
    "cf_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "}\n",
    "\n",
    "# Run training with corrected functions\n",
    "exp_name = f\"forgetinggated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "history = catastrophic_forgetting_train_loop(\n",
    "    model=cf_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=main_optimizer,\n",
    "    gating_optimizer=gating_optimizer,\n",
    "    device=device,\n",
    "    batch_fn=catastrophic_forgetting_batch,\n",
    "    batch_kwargs=cf_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    loss_params={'main': 1.0, 'adapter': 0.5, 'gating': 0.1},\n",
    "    restart = restart\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42881614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(288,512), return_splits=False, double_img=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dann_mobilenet': (DualBranchNet_DANN(backbone_type='mobilenet'), 500, 10, 0, False, dann_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 500, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 500, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_linear_simple': (DualBranchCNNNet(9,6,'pretrained', 'linear', 'simple'), 500, 10, 0, False, dualbranch_batch),\n",
    "    'baseline': (LGRBaseline(), 500, 10, 0, False, baseline_batch)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, detach_base, batch_fn) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=batch_fn,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df559160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(288,512), return_splits=False, double_img=False)\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dann_mobilenet_buff500': (DualBranchNet_DANN(backbone_type='mobilenet'), 500, 10, 0, False, dann_batch),\n",
    "    'dann_mobilenet_buff120': (DualBranchNet_DANN(backbone_type='mobilenet'), 120, 10, 0, False, dann_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach_buff500': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 500, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach_buff120': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 120, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear_buff500': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 500, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear_buff120': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 120, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_linear_simple_buff120': (DualBranchCNNNet(9,6,'pretrained', 'linear', 'simple'), 120, 10, 0, False, dualbranch_batch),\n",
    "    # 'baseline_buff500': (LGRBaseline(), 500, 10, 0, False, baseline_batch),\n",
    "    # 'baseline_buff120': (LGRBaseline(), 120, 10, 0, False, baseline_batch)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, detach_base, batch_fn) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=batch_fn,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5}\n",
    "    )\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# model_names = [\n",
    "#     'dann_mobilenet_buff500',\n",
    "#     'dann_mobilenet_buff120',\n",
    "#     'dual_2conv_adversarial_3linear_detach_buff500',\n",
    "#     'dual_2conv_adversarial_3linear_detach_buff120',\n",
    "#     'dual_mobilenet_simple_3linear_buff500',\n",
    "#     'dual_mobilenet_simple_3linear_buff120',\n",
    "#     'dual_mobilenet_linear_simple_buff120',\n",
    "#     'baseline_buff500',\n",
    "#     'baseline_buff120'\n",
    "#  ]\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     print(f\"Running {model_name}\")\n",
    "#     result = subprocess.run(\n",
    "#         [\"python\", \"train_models.py\", \"--model_name\", model_name],\n",
    "#         capture_output=True, text=True\n",
    "#     )\n",
    "#     print(result.stdout)\n",
    "#     if result.stderr:\n",
    "#         print(\"Error:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2a457",
   "metadata": {},
   "source": [
    "### current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO tests: retrain the model with\n",
    "# 3 trains averaged\n",
    "# one branch, no mask\n",
    "# top branch + mask\n",
    "# bottom branch + mask\n",
    "# both branches + random masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502be9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_scenarios = {\n",
    "    # 'heuristic_small_env': (DualBranchModel(), [(288,512), (144,256)], False),\n",
    "    # 'heuristic_square_img': (DualBranchModel(), [(224,224)]*2, False),\n",
    "    'heuristic_eval_buffer': (DualBranchModel(), [(288,512)]*2, True)\n",
    "}\n",
    "\n",
    "for name, (model, img_size, eval_buffer) in testing_scenarios.items():\n",
    "\n",
    "    transform_soc = transforms.Compose([\n",
    "        transforms.Resize(img_size[0]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    transform_env = transforms.Compose([\n",
    "        transforms.Resize(img_size[1]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "    df['image_path_env'] = df['image_path'].apply(lambda p: str(Path('../data/masked/environment') / Path(p).name))\n",
    "    df['image_path_social'] = df['image_path'].apply(lambda p: str(Path('../data/masked/social') / Path(p).name))\n",
    "    domain_dataloaders = get_dataloader(df, batch_sizes=(16, 64, 64), return_splits=False, double_img=True, transforms=[transform_soc, transform_env], num_workers=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    domains = df['domain'].unique()\n",
    "    domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    # optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': dual_model.social_branch.parameters()},\n",
    "        {'params': dual_model.env_branch.parameters()},\n",
    "        {'params': dual_model.head.parameters()},\n",
    "    ], lr=1e-3)\n",
    "    classifier_optimizer = optim.Adam([ \n",
    "        {'params': dual_model.social_classifier.parameters()},\n",
    "        {'params': dual_model.env_classifier.parameters()}\n",
    "    ], lr=1e-3)\n",
    "\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "    if eval_buffer:\n",
    "        eval_buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': 0,\n",
    "            'class_optimizer': classifier_optimizer\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=heuristic_dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5},\n",
    "        eval_buffer=eval_buffer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "model_names = [\n",
    "    'heuristic_small_env',\n",
    "    'heuristic_square_img',\n",
    "    'heuristic_eval_buffer'\n",
    "]\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Running {model_name}\")\n",
    "    process = subprocess.Popen(\n",
    "        [\"python\", \"train_models.py\", \"--model_name\", model_name, \"--num_workers\", \"0\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36adb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for potential ways to combine networks\n",
    "residual = SpecificHead(base + invariant_feats)\n",
    "specific_feats = invariant_feats + residual\n",
    "\n",
    "gate = GateNet(base + invariant_feats)\n",
    "residual = SpecificHead(base)\n",
    "specific_feats = invariant_feats + gate * residual\n",
    "\n",
    "residual = Attention(invariant_feats, base)\n",
    "specific_feats = invariant_feats + residual\n",
    "\n",
    "gamma, beta = SpecificHead(base)\n",
    "specific_feats = gamma * invariant_feats + beta\n",
    "\n",
    "invariant_feats = InvariantHead(base)\n",
    "specific_feats = SpecificHead(base)\n",
    "cos_sim(invariant_feats, specific_feats) ~ 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726bd493",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d29f9",
   "metadata": {},
   "source": [
    "### files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e848bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different cos 1-abs fixed residual - fixedred_fixedcos_dualbranchmodel_20250604_014811_history\n",
    "#fixed res but old cosine ^2 - fixedred_dualbranchmodel_20250604_002951_history\n",
    "#broken baseline - baselinemodel_20250603_200948_history\n",
    "# recent dualbranch - dualbranchmodel_20250603_135728_history\n",
    "#normalised and standarised residual - dualbranchmodel_20250609_013632_history\n",
    "#standarised residual - nonorm_dualbranchmodel_20250609_131820_history\n",
    "#no residual connection - nores_dualbranchmodel_20250609_232842_history\n",
    "#same plus gradient clipping - nores_gradclip_dualbranchmodel_20250610_010647_history\n",
    "#same, no graidient clipping, weights initialisation and normalisation - nores_winit_wnorm_dualbranchmodel_20250610_024401_history\n",
    "#same, gradient clipping and weights optimisation - nores_gradclip_winit_wnorm_dualbranchmodel_20250610_042118_history\n",
    "#deeper invariant network - deep_norm_dualbranchmodel_20250610_223350_history\n",
    "#same but with weiths normalisation - deep_dualbranchmodel_20250610_205411_history\n",
    "\n",
    "with open('../checkpoints/nonorm_dualbranchmodel_20250609_131820_history.pkl', 'rb') as f:\n",
    "    nonorm = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_dualbranchmodel_20250609_232842_history.pkl', 'rb') as f:\n",
    "    lnorm_nores = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_gradclip_dualbranchmodel_20250610_010647_history.pkl', 'rb') as f:\n",
    "    lnorm_nores_gradclip = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/dualbranchmodel_20250609_013632_history.pkl', 'rb') as f:\n",
    "    lnorm = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_winit_wnorm_dualbranchmodel_20250610_024401_history.pkl', 'rb') as f:\n",
    "    wnorm_nores = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/nores_gradclip_winit_wnorm_dualbranchmodel_20250610_042118_history.pkl', 'rb') as f:\n",
    "    wnorm_nores_gradclip = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/deep_norm_dualbranchmodel_20250610_223350_history.pkl', 'rb') as f:\n",
    "    wnorm_gradclip_deep = pickle.load(f)\n",
    "\n",
    "with open('../checkpoints/deep_dualbranchmodel_20250610_205411_history.pkl', 'rb') as f:\n",
    "    lnorm_gradclip_deep = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21170cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all have gradient clipping\n",
    "# Either detach and skip connect base -> head\n",
    "# - detach reference - how does detachement work? can backbone learn through skip connection?\n",
    "# - detach smaller batch effect?\n",
    "# - detach how partial freeze backbone learns?\n",
    "# or\n",
    "# freeze backbone\n",
    "# - frozen backbone doesn't learn, branches learn solo\n",
    "# - frozen, branches solo learn but binary?\n",
    "# - frozen branches solo learn binary but explicit gradient reverse function rather than layer?\n",
    "# - add full replay? \n",
    "\n",
    "files = [\n",
    "    'bbdetach_deep_dualbranchmodel_20250613_155613_history',                 #deep weights_init=False, detach_base=True, - base for below  experiments\n",
    "    'bdetach_batch16_dualbranchmodel_20250615_025032_history', # deep unfrozen bb, detached bb, smaller 16 batch - did smaller batch improve stability, did skip connection improve head loss and total training/val loss?\n",
    "    'bdetach_pfrozen_dualbranchmodel_20250615_040906_history', # deep pfrozen bb, detached bb - training branches+last layer bb - did partially frozen pretrained backbone improve skip head connection?\n",
    "   \n",
    "    'ffrozen_dualbranchmodel_20250615_133027_history',                       #deep weights_init=False, detach_base=False, -how do deep linear branches train on their own?\n",
    "    'ffrozen_binary_dualbranchmodel_20250615_153755_history',                      #weights_init=False, detach_base=False, explicit_grl=False, full_replay = False - does binary improve anything?\n",
    "    'ffrozen_binary_explicitgrl_dualbranchmodel_20250615_162820_history',          #weights_init=False, detach_base=False, explicit_grl=False, full_replay = False - does the gradient reversal layer work better or function?\n",
    "    'freplay_ffrozen_binary_explicitgrl_dualbranchmodel_20250615_174623_history'  #weights_init=True,  detach_base=False, explicit_grl=True,  full_replay = True - does full repaly help the branches on their own?\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be80e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    # 'CNN_pretrained_simple_simple_dualbranchmodel_20250616_032555_history',  # 'pretrained_backbone_cnn_branch': ('pretrained', 'simple', 'simple'), \n",
    "    # 'CNN_diy_backbone_linear_branch_dualbranchmodel_20250616_105211_history', # (2layer convolution backbone, 'linear', 'simple'),\n",
    "    # 'CNN_linear_branch_dualbranchmodel_20250616_172252_history',  # 'linear_branch': ('3conv', 'linear', 'simple'),\n",
    "    'CNN_cnn_branch_dualbranchmodel_20250616_190434_history',      # 'cnn_branch': ('3conv', 'simple', 'simple'),\n",
    "    'CNN_adversarial_dualbranchmodel_20250616_201008_history',      # 'adversarial': ('2conv', 'adversarial', 'adversarial'),\n",
    "    'CNN_cnn_specialised_branches_dualbranchmodel_20250616_232330_history',      # 'cnn_specialised_branches': ('3conv', 'special', 'simple')\n",
    "    'CNN_pretrained_simple_0.25branches_dualbranchmodel_20250617_230422_history',\n",
    "    'CNN_pretrained_special_dualbranchmodel_20250617_032445_history'\n",
    "]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['CNN_pretrained_simple_dualbranchmodel_20250617_022356_history', #'pretrained_simple': ('pretrained', 'simple', '3linear', detach_base=False), \n",
    "'CNN_pretrained_special_dualbranchmodel_20250617_032445_history', #    'pretrained_special': ('pretrained', 'special', '3linear', False),\n",
    "'CNN_3conv_simple_dualbranchmodel_20250617_042555_history', #    '3conv_simple': ('3conv', 'simple', '3linear', True),\n",
    "'CNN_3conv_special_dualbranchmodel_20250617_052951_history' #    '3conv_special': ('3conv', 'special', '3linear', True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['CNN_pretrained_simple_dualbranchmodel_20250617_022356_history',\n",
    "'CNN_pretrained_simple_0.25branches_dualbranchmodel_20250617_230422_history' #change the loss proportions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd198495",
   "metadata": {},
   "outputs": [],
   "source": [
    "files =['5foldcrossval_fold4_CNN_3conv_adversarial_dualbranchmodel_20250618_121607_history',\n",
    "'5foldcrossval_fold4_CNN_pretrained_simple_dualbranchmodel_20250618_064848_history'\n",
    "]\n",
    "files = [\n",
    "    'CNN_pretrained_simple_0.25branches_dualbranchmodel_20250617_230422_history',\n",
    "    'CNN_adversarial_dualbranchmodel_20250616_201008_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e66c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['DANN_dualbranchmodel_20250618_152753_history',\n",
    "         'DANN_dynamicalpha_notrain1dom_20250620_201349_history',\n",
    "         'DANN_dynamicalpha_notrain1dom_20250620_232330_history'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "'baselinemodel_20250618_195623_history',\n",
    "'minimal_simple_dualbranchmodel_20250618_213930_history',\n",
    "'minimal_simple_buffer05_dualbranchmodel_20250618_224113_history',\n",
    "'minimal_special_dualbranchmodel_20250618_233832_history'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[\n",
    "'baselinemodel_20250603_200948_history',\n",
    "'baselinemodel_20250603_034233_history',\n",
    "'baselinemodel_20250618_195623_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'baselinemodel_20250603_200948_history',\n",
    "    'minimal_absurd_buff500_20250619_121348_history',\n",
    "    'minimal_simple_buffer05_dualbranchmodel_20250618_224113_history',\n",
    "    'DANN_dualbranchmodel_20250618_152753_history',\n",
    "    'CNN_pretrained_simple_0.25branches_dualbranchmodel_20250617_230422_history'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55861ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "'DANN_dynamicalpha_notrain1dom_20250620_232330_history',\n",
    "'DANN_notrain1dom_20250621_005208_history',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "'dualbranch_CNN_dynamicalpha_20250621_015610_history',\n",
    "'CNN_adversarial_dualbranchmodel_20250616_201008_history',\n",
    "'CNN_cnn_branch_dualbranchmodel_20250616_190434_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32da7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "files =[\n",
    "'dualbranch_CNN_1epoch_20250621_014841_history',\n",
    "'CNN_pretrained_simple_025branches_dualbranchmodel_20250617_230422_history',\n",
    "'CNN_pretrained_simple_dualbranchmodel_20250617_022356_history',\n",
    "'CNN_adversarial_dualbranchmodel_20250616_201008_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "'3conv_simple_3linear_bnorm_nogradclip_20250621_130858_history',\n",
    "'3conv_simple_simple_bnorm_20250621_121437_history',\n",
    "'3conv_simple_simple_bnorm_nogradclip_20250621_102634_history',\n",
    "'3conv_simple_simple_nogradclip_20250621_112040_history',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files =['mobinev2_dann_20250621_204449_history',\n",
    "        'deeplabv3mobilenetv3_dann_20250621_223924_history',\n",
    "        'deeplabv3mobilenetv3_dann_20250624_000526_history'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    # ('dann', 'dann_mobilenet_20250630_014020_history'), #smoother, stable disentagnlement but not significantly better results\n",
    "    # ('dann_old', 'mobinev2_dann_20250621_204449_history'), #old mobilenet dann\n",
    "    ('detached', 'dual_2conv_adversarial_3linear_detach_20250630_045130_history'),\n",
    "    ('detached_old', 'CNN_3conv_simple_dualbranchmodel_20250617_042555_history'), #detached old\n",
    "    ('trainable_old', 'CNN_adversarial_dualbranchmodel_20250616_201008_history'), #trainable backbone pretrained adv adv old\n",
    "    ('frozen', 'dual_mobilenet_simple_3linear_20250630_081954_history'),\n",
    "    ('frozen_old', 'CNN_pretrained_simple_dualbranchmodel_20250617_022356_history'), #pretrained simple, 3linear old\n",
    "    # 'dual_mobilenet_linear_simple_20250630_111817_history',  #performs poorly, no surprise taht linear layer canot extract meaningfull features from frozen backbone\n",
    "    # ('baseline', 'baseline_20250630_161803_history'), #more stable, similar results\n",
    "    # ('baseline_old', 'baselinemodel_20250618_195623_history'), #old baseline\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823265ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "('linear', 'dual_mobilenet_linear_simple_20250630_111817_history'),\n",
    "('linear_b120', 'dual_mobilenet_linear_simple_buff120_20250704_081958_history'), \n",
    "('base', 'baseline_20250630_161803_history'),\n",
    "('base_b500', 'baseline_buff500_20250704_085427_history'), \n",
    "('base_b120', 'baseline_buff120_20250704_113155_history'), \n",
    "('dann', 'dann_mobilenet_20250630_014020_history'),\n",
    "('dann_b500', 'dann_mobilenet_buff500_20250704_003743_history'), \n",
    "('dann_b120', 'dann_mobilenet_buff120_20250704_031423_history'), \n",
    "('detach', 'dual_2conv_adversarial_3linear_detach_20250630_045130_history'),\n",
    "('detach_b500', 'dual_2conv_adversarial_3linear_detach_buff500_20250704_041620_history'), \n",
    "('detach_b120', 'dual_2conv_adversarial_3linear_detach_buff120_20250704_054532_history'),\n",
    "('frozen', 'dual_mobilenet_simple_3linear_20250630_081954_history'), \n",
    "('frozen_b500', 'dual_mobilenet_simple_3linear_buff500_20250704_062411_history'), \n",
    "('frozen_b120', 'dual_mobilenet_simple_3linear_buff120_20250704_074516_history'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72973c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "('base_b500', 'baseline_buff500_20250704_085427_history'), \n",
    "('base_b120', 'baseline_buff120_20250704_113155_history'), \n",
    "('new_b500','heuristic_dualbranch_buff500_20250704_213101_history'),\n",
    "('new_b120','heuristic_dualbranch_buff120_20250705_031333_history'),\n",
    "('frozen_b500', 'dual_mobilenet_simple_3linear_buff500_20250704_062411_history'), \n",
    "('frozen_b120', 'dual_mobilenet_simple_3linear_buff120_20250704_074516_history'),\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dbe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    ('base_b120', 'baseline_buff120_20250704_113155_history'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "('base_b500', 'baseline_buff500_20250704_085427_history'), \n",
    "('base_b120', 'baseline_buff120_20250704_113155_history'), \n",
    "('new_b500','heuristic_dualbranch_buff500_20250704_213101_history'),\n",
    "('new_b120','heuristic_dualbranch_buff120_20250705_031333_history'),\n",
    "('frozen_b500', 'dual_mobilenet_simple_3linear_buff500_20250704_062411_history'), \n",
    "('frozen_b120', 'dual_mobilenet_simple_3linear_buff120_20250704_074516_history'),\n",
    "('old_b500', 'CNN_pretrained_simple_dualbranchmodel_20250617_022356_history'), \n",
    "('linear_b1000', 'dual_mobilenet_linear_simple_20250630_111817_history'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5082410",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    ('base','heuristic_dualbranch_buff120_20250705_031333_history'),\n",
    "    # ('small_env','heuristic_small_env_20250722_222924_history'),\n",
    "    # ('square','heuristic_square_img_20250722_233743_history'),\n",
    "    ('eval_buffer','heuristic_eval_buffer_20250723_144515_history'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pkl_files = [f for f in os.listdir('../checkpoints/') if f.endswith('.pkl')]\n",
    "for file in pkl_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee88438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "models = {}\n",
    "for i, file in enumerate(files):\n",
    "    file_name = ''\n",
    "    if isinstance(file, tuple):\n",
    "        file_name, file = file\n",
    "    with open(f'../checkpoints/{file}.pkl', 'rb') as f:\n",
    "        model_name = file_name or '_'.join(file.split('_')[:-3])+str(i)\n",
    "        models[model_name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c14e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fold histories\n",
    "for k,m in models.items():\n",
    "    models[k] = combine_fold_histories(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ec94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.update(models2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../checkpoints/bbdetach_deep_dualbranchmodel_20250613_155613_history.pkl', 'rb') as f:\n",
    "    bbdetach_deep = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_file = torch.load('../checkpoints/dualbranchmodel_20250609_013632_domainSmallOffice_epoch9_step1300.pt')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "model.load_state_dict(pt_file['model_state_dict'])\n",
    "history = pt_file['history']\n",
    "tsne_data = pt_file['tsne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.keys():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae0a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(models.keys())[1:]:\n",
    "    for j in models[i]['cross_domain_val']:\n",
    "        for key in j:\n",
    "            j[key] = j[key][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(models.keys())[1:]:\n",
    "    models[i]['val_epoch_loss'] = [j[0] for j in models[i]['val_epoch_loss']]\n",
    "    models[i]['val_buffer_epoch_loss'] = [j[0] for j in models[i]['val_buffer_epoch_loss']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f67f",
   "metadata": {},
   "source": [
    "### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf83e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ed5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'bbdetach_deep':bbdetach_deep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c761c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'nonorm': nonorm,\n",
    "'lnorm': lnorm,\n",
    "'lnorm_nores': lnorm_nores,\n",
    "'lnorm_nores_gradclip': lnorm_nores_gradclip,\n",
    "'wnorm_nores': wnorm_nores,\n",
    "'wnorm_nores_gradclip': wnorm_nores_gradclip,\n",
    "'wnorm_gradclip_deep': wnorm_gradclip_deep,\n",
    "'lnorm_gradclip_deep': lnorm_gradclip_deep\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfbf212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,m in models.items():\n",
    "#     filtered = [x for i, x in enumerate(m['cross_domain_val']) if i in [9, 19, 29, 39, 49, 58]]\n",
    "#     m['cross_domain_val'] = filtered\n",
    "\n",
    "for i,m in models.items():\n",
    "    print(len(m['cross_domain_val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickvals=[0, 10, 20, 30, 40, 50]\n",
    "np.multiply(np.array(tickvals), 2).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "traces = []\n",
    "for model_name, history in models.items():\n",
    "    traces.append(go.Scatter(\n",
    "        x=list(range(len(history['train_epoch_loss']))),\n",
    "        y=history['train_epoch_loss'],\n",
    "        mode='lines',\n",
    "        name=f'{model_name} Train Loss',\n",
    "        visible=False\n",
    "    ))\n",
    "    traces.append(go.Scatter(\n",
    "        x=list(range(len(history['val_epoch_loss']))),\n",
    "        y=history['val_epoch_loss'],\n",
    "        mode='lines',\n",
    "        name=f'{model_name} Val Loss',\n",
    "        visible=False\n",
    "    ))\n",
    "\n",
    "# Make the first model visible by default\n",
    "for i in range(2):\n",
    "    traces[i].visible = True\n",
    "\n",
    "buttons = []\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    visible = [False] * len(traces)\n",
    "    visible[2*i] = True\n",
    "    visible[2*i + 1] = True\n",
    "    buttons.append(dict(\n",
    "        label=model_name,\n",
    "        method='update',\n",
    "        args=[{'visible': visible}, {'title': f'Training and Validation Loss - {model_name}'}]\n",
    "    ))\n",
    "\n",
    "fig = go.Figure(data=traces)\n",
    "fig.update_layout(\n",
    "    updatemenus=[dict(\n",
    "        active=0,\n",
    "        buttons=buttons,\n",
    "        x=0.1,\n",
    "        y=1.15,\n",
    "        xanchor='left',\n",
    "        yanchor='top'\n",
    "    )],\n",
    "    title='Training and Validation Loss - Model A',\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='MSE Loss',\n",
    "    template='plotly_white',\n",
    "    yaxis=dict(range=[0, 1])\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name, history in models.items():\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history['train_epoch_loss']))),\n",
    "        y=history['train_epoch_loss'],\n",
    "        mode='lines',\n",
    "        name=f'{model_name} Train Loss'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(history['val_epoch_loss']))),\n",
    "        y=history['val_epoch_loss'],\n",
    "        mode='lines',\n",
    "        name=f'{model_name} Val Loss'\n",
    "    ))\n",
    "    try:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(history['val_buffer_epoch_loss']))),\n",
    "            y=history['val_buffer_epoch_loss'],\n",
    "            mode='lines',\n",
    "            name=f'{model_name} Buffer Val Loss'\n",
    "        ))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training and Validation Loss',\n",
    "    xaxis_title='Epochs',\n",
    "    yaxis_title='MSE Loss',\n",
    "    template='plotly_white',\n",
    "    yaxis=dict(range=[-0.0, 1]),\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 10, 20, 30, 40, 50],\n",
    "    # tickvals = np.multiply(np.array([0, 10, 20, 30, 40, 50]), 3).tolist(),\n",
    "    ticktext=list(df['domain'].unique())\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f792f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt\n",
    "plt.figure(figsize=(20,7))\n",
    "for name, model in models.items():\n",
    "    plt.plot(model['train_epoch_loss'], label=f'{name} Train Loss')\n",
    "    plt.plot(model['val_epoch_loss'], label=f'{name} Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(-0.01, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a1af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in models.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c42bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for name, model in models.items():\n",
    "    for i in range(6):\n",
    "        final += list(model['cross_domain_val'][i].values())\n",
    "max_loss = max(final)\n",
    "min_loss = min(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt\n",
    "history = models['history']['cross_domain_val']\n",
    "\n",
    "# Extract domain names\n",
    "domains = list(history[0].keys())\n",
    "\n",
    "# Prepare accuracy per domain over time\n",
    "domain_scores = {domain: [] for domain in domains}\n",
    "for snapshot in history:\n",
    "    for domain in domains:\n",
    "        domain_scores[domain].append(snapshot[domain])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for domain, scores in domain_scores.items():\n",
    "    plt.plot(domains[:len(scores)], scores, label=domain, marker='o')\n",
    "\n",
    "plt.xlabel(\"After training on domain X\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Domain-wise Accuracy Over Time\")\n",
    "# plt.ylim(min(-0.1, min_loss), max_loss)\n",
    "# plt.ylim(-0.05, 0.1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare data structure\n",
    "domain_data = {}\n",
    "for model_name, model_history in models.items():\n",
    "    history = model_history['cross_domain_val']\n",
    "    domains = list(history[0].keys())\n",
    "    domain_data[model_name] = {\n",
    "        domain: [snapshot[domain] for snapshot in history]\n",
    "        for domain in domains\n",
    "    }\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Color palette for domains\n",
    "domain_colors = {\n",
    "    domain: color for domain, color in zip(\n",
    "        domains, \n",
    "        ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', \"#05ffee\"]\n",
    "    )\n",
    "}\n",
    "\n",
    "# Add traces for each model and domain\n",
    "for model_idx, (model_name, domains) in enumerate(domain_data.items()):\n",
    "    for domain, scores in domains.items():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(scores))),\n",
    "            y=scores,\n",
    "            mode='lines+markers',\n",
    "            name=domain,\n",
    "            line=dict(color=domain_colors[domain], width=2),\n",
    "            marker=dict(size=8, symbol=model_idx+1),  # Unique symbol per model\n",
    "            legendgroup=model_name,\n",
    "            legendgrouptitle_text=model_name,\n",
    "            visible=True if model_idx == 0 else 'legendonly'  # Show first model by default\n",
    "        ))\n",
    "\n",
    "# Create model selection buttons\n",
    "buttons = [\n",
    "    dict(\n",
    "        label=model_name,\n",
    "        method='update',\n",
    "        args=[\n",
    "            {'visible': [m == model_name for m in domain_data.keys() for _ in domains]},\n",
    "            {'title': f'Domain Losses: {model_name}'}\n",
    "        ]\n",
    "    ) for model_name in domain_data.keys()\n",
    "]\n",
    "\n",
    "# Layout configuration\n",
    "fig.update_layout(\n",
    "    title='Validation Loss of Domain X After Training on Domain Y',\n",
    "    xaxis_title='After Training on Domain Y',\n",
    "    yaxis_title='MSE Loss',\n",
    "    legend=dict(\n",
    "        title='Domain X',\n",
    "        groupclick=\"toggleitem\",  # Allows group toggling while preserving individual control\n",
    "        itemsizing='constant'\n",
    "    ),\n",
    "    updatemenus=[{\n",
    "        'type': 'dropdown',\n",
    "        'direction': 'down',\n",
    "        'showactive': True,\n",
    "        'buttons': buttons,\n",
    "        'x': 1,\n",
    "        'xanchor': 'left',\n",
    "        'y': 1.1,\n",
    "        'yanchor': 'top'\n",
    "    }],\n",
    "    template='plotly_white',\n",
    "    # width=1200,\n",
    "    # height=700\n",
    "    yaxis=dict(range=[min(-0.01, min_loss), max_loss]),\n",
    "    # yaxis=dict(range=[-0.0, 0.8]),\n",
    "    \n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 1, 2, 3, 4, 5],\n",
    "    ticktext=list(df['domain'].unique())\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa569abd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072bf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt similarity\n",
    "h = models['deep_norm']\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot([m['similarity'] for m in h['train_epoch_metrics']])\n",
    "plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.title('cosine similarity of two branches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt\n",
    "h = models['history']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['inv_acc', 'spec_acc']:\n",
    "    plt.plot([m[metric] for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.title('Branch accuracies and their similarity')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce368a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name, history in models.items():\n",
    "    for metric in ['inv_acc', 'spec_acc']:\n",
    "        try:\n",
    "            y_values = [m[metric] for m in history['train_epoch_metrics']]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(len(y_values))),\n",
    "                y=y_values,\n",
    "                mode='lines+markers',\n",
    "                name=f'{model_name} - {metric}'\n",
    "            ))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Branch Accuracies',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Accuracy',\n",
    "    # xaxis=dict(tickmode='linear', tick0=0, dtick=1),\n",
    "    template='plotly_white',\n",
    "    # width=1000,\n",
    "    # height=400\n",
    "    yaxis=dict(range=[-0.0, 1]),\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 10, 20, 30, 40, 50],\n",
    "    ticktext=list(df['domain'].unique())\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt\n",
    "h = models['history']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['inv_domain', 'spec_domain', 'task_loss']:\n",
    "    plt.plot([m[metric] for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.ylim(-0.01, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50704b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name, history in models.items():\n",
    "    for metric in ['inv_domain', 'spec_domain', 'task_loss']:\n",
    "        try:\n",
    "            y_values = [m[metric] for m in history['train_epoch_metrics']]\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(len(y_values))),\n",
    "                y=y_values,\n",
    "                mode='lines+markers',\n",
    "                name=f'{model_name} - {metric}'\n",
    "            ))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "fig.update_layout(\n",
    "    title='CE loss - inv_domain, spec_domain, and MSE task_loss',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss',\n",
    "    template='plotly_white',\n",
    "    # width=1000,\n",
    "    # height=400\n",
    "    yaxis=dict(range=[-0.01, 0.12]),\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 10, 20, 30, 40, 50],\n",
    "    ticktext=list(df['domain'].unique())\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = models['history']\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "for metric in ['replay_count', 'current_count']:\n",
    "    plt.plot([m[metric]/32*100 for m in h['train_epoch_metrics']], label=metric)\n",
    "plt.title('Type of samples in batch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('% of batch')\n",
    "# plt.axhline(32, color='r')\n",
    "# plt.xticks(np.arange(0, 60, step=1))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d41c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "# for module in ['invariant', 'specific_residual', 'domain_classifier']:\n",
    "for module in h['grad_norms'][0].keys():\n",
    "    plt.plot([m[f'{module}'] for m in h['grad_norms']], label=module)\n",
    "plt.title('Gradient Norms by Module')\n",
    "# plt.ylim(-0.001, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Collect all module names across all models\n",
    "all_modules = set()\n",
    "for model_name, history in models.items():\n",
    "    if 'grad_norms' in history and len(history['grad_norms']) > 0:\n",
    "        for grad_dict in history['grad_norms']:\n",
    "            all_modules.update(grad_dict.keys())\n",
    "all_modules = sorted(all_modules)\n",
    "# all_modules = ['invariant', 'specific', 'head']\n",
    "\n",
    "\n",
    "# Add traces for each model and module, only show the first model by default\n",
    "trace_visibility = []\n",
    "for model_idx, (model_name, history) in enumerate(models.items()):\n",
    "    if 'grad_norms' not in history or len(history['grad_norms']) == 0:\n",
    "        continue\n",
    "    for grad_dict in history['grad_norms']:\n",
    "        for key in all_modules:\n",
    "            grad_dict.setdefault(key, 0)\n",
    "    for module in all_modules:\n",
    "        y_values = [m[module] for m in history['grad_norms']]\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=list(range(len(y_values))),\n",
    "            y=y_values,\n",
    "            mode='lines+markers',\n",
    "            name=f'{model_name} - {module}',\n",
    "            # visible=(model_idx == 0)\n",
    "        ))\n",
    "    trace_visibility.append((model_name, len(all_modules)))\n",
    "\n",
    "# Create dropdown buttons for each model\n",
    "buttons = []\n",
    "start_idx = 0\n",
    "for model_name, n_traces in trace_visibility:\n",
    "    visible = [False] * len(fig.data)\n",
    "    for i in range(start_idx, start_idx + n_traces):\n",
    "        visible[i] = True\n",
    "    buttons.append(dict(\n",
    "        label=model_name,\n",
    "        method='update',\n",
    "        args=[{'visible': visible}, {'title': f'Gradient Norms by Module - {model_name}'}]\n",
    "    ))\n",
    "    start_idx += n_traces\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Gradient Norms by Module - {list(models.keys())[0]}',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Gradient Norm',\n",
    "    template='plotly_white',\n",
    "    # width=1000,\n",
    "    # height=400,\n",
    "    updatemenus=[{\n",
    "        'buttons': buttons,\n",
    "        'direction': 'down',\n",
    "        'showactive': True,\n",
    "        'x': 1,\n",
    "        'xanchor': 'left',\n",
    "        'y': 1.2,\n",
    "        'yanchor': 'top'\n",
    "    }],\n",
    "    yaxis=dict(range=[-0.0, 0.2]),\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    tickvals=[0, 10, 20, 30, 40, 50],\n",
    "    ticktext=list(df['domain'].unique())\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97530f3f",
   "metadata": {},
   "source": [
    "### tsne projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdbdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "\n",
    "# 1. Gather all checkpoint files\n",
    "checkpoint_files = glob.glob(\"../checkpoints/dualbranchmodel_20250609_013632_*.pt\")\n",
    "\n",
    "# 2. Parse out the step value and sort\n",
    "pattern = re.compile(r\"_step(\\d+)\\.pt\")\n",
    "files_with_steps = []\n",
    "for f in checkpoint_files:\n",
    "    match = pattern.search(f)\n",
    "    if match:\n",
    "        step = int(match.group(1))\n",
    "        files_with_steps.append((step, f))\n",
    "files_with_steps.sort()  # Sort by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb858a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e574052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Precompute t-SNE for all checkpoints (0-59)\n",
    "tsne_projections = []\n",
    "for idx, (step, ckpt_file) in enumerate(tqdm(files_with_steps, desc=\"Processing checkpoints\")):\n",
    "    ckpt = torch.load(ckpt_file, map_location='cpu')\n",
    "    data = ckpt['tsne']\n",
    "    inv_feats = np.array(data['inv_feats'])\n",
    "    spec_feats = np.array(data['spec_feats'])\n",
    "    domain_labels = np.array(data['domain_labels'])\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    inv_2d = tsne.fit_transform(inv_feats)\n",
    "    spec_2d = tsne.fit_transform(spec_feats)\n",
    "\n",
    "    tsne_projections.append({\n",
    "        'timeline_idx': idx,  # 0 to 59\n",
    "        'inv_2d': inv_2d,\n",
    "        'spec_2d': spec_2d,\n",
    "        'domains': domain_labels,\n",
    "        'filename': ckpt_file\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# State variable for current index\n",
    "current_idx = 0\n",
    "\n",
    "# Output widget for the plot\n",
    "out = widgets.Output()\n",
    "\n",
    "# Buttons\n",
    "button_prev = widgets.Button(description=\"Previous\")\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "\n",
    "# Precompute limits\n",
    "all_x = np.concatenate([d['inv_2d'][:,0] for d in tsne_projections] + [d['spec_2d'][:,0] for d in tsne_projections])\n",
    "all_y = np.concatenate([d['inv_2d'][:,1] for d in tsne_projections] + [d['spec_2d'][:,1] for d in tsne_projections])\n",
    "x_min, x_max = all_x.min(), all_x.max()\n",
    "y_min, y_max = all_y.min(), all_y.max()\n",
    "\n",
    "def plot_epoch(timeline_idx):\n",
    "    data = tsne_projections[timeline_idx]\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), constrained_layout=True)\n",
    "    domain_to_int = {name: i for i, name in enumerate(domains)}\n",
    "    domain_ints = np.array([domain_to_int[name] for name in data['domains']])\n",
    "    scatter1 = ax1.scatter(data['inv_2d'][:,0], data['inv_2d'][:,1], \n",
    "                          c=domain_ints, cmap='tab10', alpha=0.7, vmin=0, vmax=len(domains)-1)\n",
    "    ax1.set_title(f\"Invariant Features - Timeline {timeline_idx}\")\n",
    "    ax1.set_xlim(x_min, x_max)\n",
    "    ax1.set_ylim(y_min, y_max)\n",
    "    scatter2 = ax2.scatter(data['spec_2d'][:,0], data['spec_2d'][:,1],\n",
    "                          c=domain_ints, cmap='tab10', alpha=0.7, vmin=0, vmax=len(domains)-1)\n",
    "    ax2.set_title(f\"Specific Features - Timeline {timeline_idx}\")\n",
    "    ax2.set_xlim(x_min, x_max)\n",
    "    ax2.set_ylim(y_min, y_max)\n",
    "    cbar = fig.colorbar(scatter1, ax=[ax1, ax2], label='Domain', \n",
    "                        ticks=np.arange(len(domains)), boundaries=np.arange(len(domains)+1)-0.5)\n",
    "    cbar.set_ticks(np.arange(len(domains)))\n",
    "    cbar.set_ticklabels(domains)\n",
    "    plt.show()\n",
    "\n",
    "def on_prev_clicked(b):\n",
    "    global current_idx\n",
    "    if current_idx > 0:\n",
    "        current_idx -= 1\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plot_epoch(current_idx)\n",
    "\n",
    "def on_next_clicked(b):\n",
    "    global current_idx\n",
    "    if current_idx < len(tsne_projections) - 1:\n",
    "        current_idx += 1\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            plot_epoch(current_idx)\n",
    "\n",
    "button_prev.on_click(on_prev_clicked)\n",
    "button_next.on_click(on_next_clicked)\n",
    "\n",
    "# Display everything\n",
    "display(widgets.HBox([button_prev, button_next]))\n",
    "display(out)\n",
    "\n",
    "# Initial plot\n",
    "with out:\n",
    "    plot_epoch(current_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870cd48",
   "metadata": {},
   "source": [
    "### single batch overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895de6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model output\n",
    "outputs = model(inputs)  # Should be in [1,5]\n",
    "print(f\"Output range: {outputs.min().item()}–{outputs.max().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2bbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = LGRBaseline().to(device)\n",
    "test_optimizer = optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels, _ = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)\n",
    "\n",
    "# %%\n",
    "num_test_epochs = 400\n",
    "for epoch in range(num_test_epochs):\n",
    "    test_optimizer.zero_grad()\n",
    "\n",
    "    outputs = test_model(inputs)\n",
    "    loss = criterion(outputs['output'], labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    test_optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Overfit Epoch {epoch+1}/{num_test_epochs} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# device = torch.device('cpu')torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "\n",
    "writer = SummaryWriter(\"visualisation/\")\n",
    "model = DualBranchNet().to(device)\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels, _ = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)\n",
    "# writer.add_graph(model, inputs)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_names = [\"image\"]\n",
    "output_names = [\"appropriateness scores\"]\n",
    "\n",
    "torch.onnx.export(model, inputs, \"model.onnx\", input_names=input_names, output_names=output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "y = model(inputs)\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cadf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Get a single batch from any domain's train loader\n",
    "domain = domains[0]\n",
    "single_batch = next(iter(domain_dataloaders[domain]['train']))\n",
    "\n",
    "\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "\n",
    "# 4. Overfit loop for both models\n",
    "def overfit_model(\n",
    "    model, optimizer, batch_fn, batch_kwargs, device, num_epochs=100, exp_name=\"overfit\"\n",
    "):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss, metrics = batch_fn(model, single_batch, device, **batch_kwargs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "    return losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Baseline model overfit\n",
    "baseline_model = LGRBaseline().to(device)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "baseline_losses = overfit_model(\n",
    "    baseline_model, optimizer, baseline_batch, {'mse_criterion': torch.nn.MSELoss()}, device\n",
    ")\n",
    "\n",
    "# 6. DualBranch model overfit\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': torch.nn.MSELoss(),\n",
    "    'ce_criterion': torch.nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': lambda a, b: (torch.nn.CosineSimilarity()(a, b) ** 2).mean(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'current_domain': domain\n",
    "}\n",
    "dualbranch_losses = overfit_model(\n",
    "    dual_model, optimizer, dualbranch_batch, dualbranch_kwargs, device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Plot the loss curves (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(baseline_losses, label='Baseline')\n",
    "plt.plot(dualbranch_losses, label='DualBranch')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Overfitting to a Single Batch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa9229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
