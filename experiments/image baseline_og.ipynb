{"cells":[{"cell_type":"markdown","id":"d72ee38c","metadata":{"id":"d72ee38c"},"source":["## Data preparation"]},{"cell_type":"code","source":["%cd experiments"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9BS6CMC6oS_9","executionInfo":{"status":"ok","timestamp":1748934819075,"user_tz":-120,"elapsed":17,"user":{"displayName":"Rafal Karpinski","userId":"08014100261962019500"}},"outputId":"b27b8d74-ebd9-49f9-b67b-d2cc667ee3fe"},"id":"9BS6CMC6oS_9","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'experiments'\n","/content/experiments\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jrrutFBKo8tc"},"id":"jrrutFBKo8tc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data processing"],"metadata":{"id":"XmtOHRqFo9GK"},"id":"XmtOHRqFo9GK"},{"cell_type":"code","execution_count":null,"id":"d2560ad3","metadata":{"id":"d2560ad3"},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","from pathlib import Path\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import random\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","DATASET_DIR = (Path(\"..\") / \"datasets\").resolve()\n","DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n","LABEL_COLS = [\n","    \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n","    \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n","    \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n","]"]},{"cell_type":"code","execution_count":null,"id":"cf9da04b","metadata":{"id":"cf9da04b"},"outputs":[],"source":["def process_csv(csv_path, dataset):\n","    \"\"\"Process individual CSV files\"\"\"\n","    df = pd.read_csv(csv_path)\n","    df = df.drop(columns=df.columns[-1])\n","\n","    # Extract metadata from first column\n","    first_col = df.columns[0]\n","    split_data = df[first_col].str.split('_', n=2, expand=True)\n","\n","    df[\"robot\"] = split_data[0]\n","    df[\"domain\"] = split_data[1]\n","    df[\"image_ref\"] = split_data[2].astype(int)\n","    df[\"dataset\"] = dataset\n","\n","    df = df.drop(columns=[first_col])\n","\n","    return df\n","\n","def consolidate_data(datasets):\n","    \"\"\"Aggregate all CSVs\"\"\"\n","    all_dfs = []\n","    for dataset in datasets:\n","        source_path = DATASET_DIR / dataset\n","\n","        for robot in [\"NAO\", \"Pepper\", \"PR2\"]:\n","            ann_dir = source_path / robot / \"Annotations\"\n","            if not ann_dir.exists():\n","                raise ValueError(f\"Labels csv file path ({ann_dir}) doesn't exist\")\n","\n","\n","            for csv_file in ann_dir.glob(\"*.csv\"):\n","                try:\n","                    df = process_csv(csv_file, dataset)\n","                    all_dfs.append(df)\n","                except Exception as e:\n","                    print(f\"Error processing {csv_file}: {str(e)}\")\n","\n","    df = pd.concat(all_dfs, ignore_index=True)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"53657ef9","metadata":{"id":"53657ef9"},"outputs":[],"source":["def validate_raw_data(df):\n","    \"\"\"Comprehensive data quality checks for raw annotation data\"\"\"\n","    required_columns = {'robot', 'domain', 'image_ref', 'dataset'}\n","\n","    # Check for any missing columns\n","    missing_cols = required_columns - set(df.columns)\n","    if missing_cols:\n","        raise ValueError(f\"Missing required columns: {missing_cols}\")\n","\n","    # Label value validation (should be between 1 and 5)\n","    for col in LABEL_COLS:\n","        if df[col].min() < 1 or df[col].max() > 5:\n","            raise ValueError(f\"Label {col} has invalid range [{df[col].min()}, {df[col].max()}]\")\n","\n","    # Null values check\n","    null_cols = df.columns[df.isnull().any()].tolist()\n","    if null_cols:\n","        raise ValueError(f\"Null values found in columns: {null_cols}\")\n","\n","    # Data type and value validation for image_ref\n","    if not pd.api.types.is_integer_dtype(df['image_ref']):\n","        raise TypeError(\"image_ref must be integer type\")\n","    if (df['image_ref'] < 0).any():\n","        raise ValueError(\"image_ref contains negative values, which is invalid\")\n","\n","    # Categorical value validation\n","    valid_robots = {'NAO', 'Pepper', 'PR2'}\n","    invalid_robots = set(df['robot']) - valid_robots\n","    if invalid_robots:\n","        raise ValueError(f\"Invalid robot values: {invalid_robots}\")\n","\n","    valid_sources = {'OFFICE-MANNERSDB', 'MANNERSDBPlus'}\n","    invalid_sources = set(df['dataset']) - valid_sources\n","    if invalid_sources:\n","        raise ValueError(f\"Invalid source directories: {invalid_sources}\")\n","\n","    return True\n"]},{"cell_type":"code","execution_count":null,"id":"4cb45d58","metadata":{"id":"4cb45d58"},"outputs":[],"source":["def aggregate_labels(df):\n","    \"\"\"Aggregate multiple annotations per image by image path\"\"\"\n","    agg_dict = {\n","        **{col: 'mean' for col in LABEL_COLS},\n","        **{col: 'first' for col in df.columns.difference(LABEL_COLS).tolist()},\n","    }\n","\n","    return df.groupby('image_path', as_index=False).agg(agg_dict)\n"]},{"cell_type":"code","execution_count":null,"id":"0e53d64d","metadata":{"id":"0e53d64d"},"outputs":[],"source":["def resolve_image_path(row):\n","    \"\"\"Robust path resolution with validation\"\"\"\n","    base_dir = DATASET_DIR / row['dataset'] / row['robot'] / \"Images\"\n","\n","    if row['dataset'] == \"OFFICE-MANNERSDB\":\n","        target = base_dir / f\"{row['domain']}_{row['image_ref']}.png\"\n","    else:\n","        target = next(base_dir.glob(f\"{row['image_ref']}_*.png\"), None)\n","\n","    if target and target.exists():\n","        return target.resolve()\n","    return None"]},{"cell_type":"code","execution_count":null,"id":"37acfe18","metadata":{"id":"37acfe18"},"outputs":[],"source":["class ImageLabelDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        if isinstance(idx, torch.Tensor):\n","            idx = idx.item()\n","\n","        img_path = str(self.df.at[idx, \"image_path\"])\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","        except Exception as e:\n","            raise RuntimeError(f\"Error loading {img_path}: {str(e)}\")\n","\n","        raw_labels = self.df.iloc[idx][LABEL_COLS].values.astype(np.float32)\n","        scaled_labels = (raw_labels - 1) / 4  # Convert 1-5 â†’ 0-1\n","        domain_labels = self.df.at[idx, 'domain']\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, torch.from_numpy(scaled_labels), domain_labels"]},{"cell_type":"code","execution_count":null,"id":"f915a512","metadata":{"id":"f915a512"},"outputs":[],"source":["def create_dataloaders(df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128)):\n","    \"\"\"Create train/val/test dataloaders using image_path as unique key\"\"\"\n","\n","    # Get image paths as indexing for split\n","    unique_images = df[['image_path']].reset_index(drop=True)\n","\n","    # Split using image_path as key #TODO is is spliting based on index or path strings? index faster\n","    train_paths, temp_paths = train_test_split(\n","        unique_images['image_path'],\n","        test_size=0.3,\n","        random_state=42\n","    )\n","    val_paths, test_paths = train_test_split(\n","        temp_paths,\n","        test_size=0.5,\n","        random_state=42\n","    )\n","\n","    # Create subsets\n","    train_df = df[df['image_path'].isin(train_paths)].reset_index(drop=True)\n","    val_df = df[df['image_path'].isin(val_paths)].reset_index(drop=True)\n","    test_df = df[df['image_path'].isin(test_paths)].reset_index(drop=True)\n","\n","    #TODO add coordinate values for spacialy aware CNN Uber's CoordNav\n","    # Define transforms\n","    transform = transforms.Compose([\n","        transforms.Resize(resize_img_to),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                            std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Create datasets\n","    train_dataset = ImageLabelDataset(train_df, transform)\n","    val_dataset = ImageLabelDataset(val_df, transform)\n","    test_dataset = ImageLabelDataset(test_df, transform)\n","\n","    # Create loaders\n","    num_workers = 0\n","    loaders = {\n","        'train': DataLoader(train_dataset, batch_size=batch_sizes[0], shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n","        'val': DataLoader(val_dataset, batch_size=batch_sizes[1], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n","        'test': DataLoader(test_dataset, batch_size=batch_sizes[2], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n","    }\n","\n","    return loaders\n"]},{"cell_type":"code","execution_count":null,"id":"9668e590","metadata":{"id":"9668e590"},"outputs":[],"source":["def validate_final_data(df):\n","    \"\"\"Final validation after aggregation\"\"\"\n","    # Missing image paths\n","    missing = df[df['image_path'].isnull()]\n","    if not missing.empty:\n","        raise FileNotFoundError(\n","            f\"{len(missing)} images missing after aggregation. Examples:\\n\"\n","            f\"{missing[['robot', 'domain', 'image_ref']].head()}\"\n","        )\n","\n","    # Null values check\n","    null_cols = df.columns[df.isnull().any()].tolist()\n","    if null_cols:\n","        raise ValueError(f\"Null values found in columns: {null_cols}\")\n","\n","    # Duplicate image paths\n","    duplicates = df[df.duplicated('image_path', keep=False)]\n","    if not duplicates.empty:\n","        raise RuntimeError(\n","            f\"Duplicate image paths after aggregation:\\n\"\n","            f\"{duplicates['image_path'].unique()}\"\n","        )\n","\n","    # Label validity (1-5)\n","    for col in LABEL_COLS:\n","        if df[col].min() < 1 or df[col].max() > 5:\n","            raise ValueError(\n","                f\"Aggregated label {col} out of range: \"\n","                f\"[{df[col].min()}, {df[col].max()}]\"\n","            )\n","\n","    return True\n"]},{"cell_type":"code","execution_count":null,"id":"8cfd34a8","metadata":{"id":"8cfd34a8"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"ac2a2a2b","metadata":{"id":"ac2a2a2b"},"source":["## --"]},{"cell_type":"code","execution_count":null,"id":"86f66130","metadata":{"id":"86f66130"},"outputs":[],"source":["try:\n","    raw_df = consolidate_data(DATASETS)\n","    validate_raw_data(raw_df)\n","    raw_df['image_path'] = raw_df.apply(resolve_image_path, axis=1)\n","    aggregated_df = aggregate_labels(raw_df)\n","    validate_final_data(aggregated_df)\n","except Exception as e:\n","    print(f\"Pipeline failed: {str(e)}\")\n","    raise\n","\n","aggregated_df.to_pickle(\"processed_all_data.pkl\")\n"]},{"cell_type":"markdown","id":"a43ca986","metadata":{"id":"a43ca986"},"source":["## CL Models"]},{"cell_type":"code","execution_count":null,"id":"b327a539","metadata":{"id":"b327a539"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","import numpy as np\n","from collections import deque\n","import random\n","import time\n","import torch.nn.functional as F\n","from torch.utils.data import Subset, ConcatDataset, DataLoader"]},{"cell_type":"code","execution_count":null,"id":"9ca1a5d4","metadata":{"id":"9ca1a5d4"},"outputs":[],"source":["class SocialContinualModel(nn.Module):\n","    def __init__(self, num_tasks=9):\n","        super().__init__()\n","        # Initialize the ResNet50 architecture with Places365 configuration\n","        self.backbone = models.resnet50(num_classes=365)\n","\n","        # get Places365 weights and fix their naming leftover from troch saving convention\n","        places365_weights = torch.load('resnet50_places365.pth.tar', weights_only=True)\n","        state_dict = places365_weights['state_dict']\n","        state_dict = {k.replace('module.', ''): v\n","                     for k, v in state_dict.items()}\n","\n","        # Load weights\n","        self.backbone.load_state_dict(state_dict)\n","\n","        # Remove classification head\n","        self.backbone.fc = nn.Identity()\n","\n","        #Freeze all params except last layer\n","        for name, param in self.backbone.named_parameters():\n","            if 'layer4' not in name:\n","                param.requires_grad_(False)\n","\n","        #TODO human mask average, std, quadrants, human in realtion to robot std\n","\n","\n","\n","        # Shared layers #TODO deeper shared space?\n","        self.shared_fc = nn.Sequential(\n","            nn.Linear(2048, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.3)\n","        )\n","\n","        # Task-specific heads with more expensive but finegrained GELU\n","        self.heads = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(1024, 512),\n","                nn.GELU(),\n","                nn.Linear(512, 1),\n","                nn.Sigmoid()\n","            ) for _ in range(num_tasks)\n","        ])\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        shared = self.shared_fc(features)\n","        outputs = [head(shared) for head in self.heads]\n","        return torch.cat(outputs, dim=1)\n"]},{"cell_type":"code","execution_count":null,"id":"971efe39","metadata":{"id":"971efe39"},"outputs":[],"source":["import torch.nn as nn\n","from torchvision import models\n","\n","class LGRBaseline(nn.Module):\n","    \"\"\"\n","    @misc{churamani_feature_2024,\n","\t\ttitle = {Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours},\n","\t\turl = {http://arxiv.org/abs/2405.15773},\n","\t\tdoi = {10.48550/arXiv.2405.15773},\n","\t\tnumber = {{arXiv}:2405.15773},\n","\t\tpublisher = {{arXiv}},\n","\t\tauthor = {Churamani, Nikhil and Checker, Saksham and Chiang, Hao-Tien Lewis and Gunes, Hatice},\n","\t\turldate = {2025-01-30},\n","\t\tdate = {2024-03-16},\n","\t}\n","    \"\"\"\n","    def __init__(self, num_classes=9):\n","        super(LGRBaseline, self).__init__()\n","\n","        # MobileNetV2 backbone\n","        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n","\n","        # Backbone feature processing\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.flatten = nn.Flatten()\n","\n","        # Regression head\n","        self.fc1_bn = nn.BatchNorm1d(1280)\n","        self.fc2 = nn.Linear(1280, 32)\n","        # self.fc2_bn = nn.BatchNorm1d(128)\n","\t\t# self.fc3 = nn.Linear(128, 32)\n","        self.fc4 = nn.Linear(32, num_classes)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = self.backbone(x)\n","\n","        # Spatial reduction\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","\n","        # Regression\n","        x = self.fc1_bn(x)\n","        x = self.fc2(x)\n","        # x = self.fc2_bn(x)\n","\t\t# x = self.fc3(x)\n","        x = self.fc4(x)\n","\n","        return {'output': x}\n"]},{"cell_type":"code","execution_count":null,"id":"33a7176e","metadata":{"id":"33a7176e"},"outputs":[],"source":["import torch\n","import random\n","import pandas as pd\n","\n","class ReservoirBuffer:\n","    def __init__(self, capacity=1000, replay_ratio=0.2, input_shape=(3, 384, 216), label_shape=(9,), device=torch.device('cpu')):\n","        self.capacity = capacity\n","        self.inputs = torch.empty((capacity, *input_shape), dtype=torch.float32, device=device)\n","        self.labels = torch.empty((capacity, *label_shape), dtype=torch.float32, device=device)\n","        self.domains = [None] * capacity\n","        self.size = 0          # Number of samples currently in buffer\n","        self.num_seen = 0      # Total samples seen\n","        self.replay_ratio = replay_ratio\n","\n","    def add(self, new_samples):\n","        for sample in new_samples:\n","            self.num_seen += 1\n","            if self.size < self.capacity:\n","                idx = self.size\n","                self.size += 1\n","            else:\n","                idx = random.randint(0, self.num_seen - 1)\n","                if idx >= self.capacity:\n","                    continue\n","            self.inputs[idx].copy_(sample[0])\n","            self.labels[idx].copy_(sample[1])\n","            self.domains[idx] = sample[2]\n","\n","    def sample(self, batch_size):\n","        if self.size == 0:\n","            return []\n","        indices = torch.randint(0, self.size, (batch_size,))\n","        return [(self.inputs[i], self.labels[i], self.domains[i]) for i in indices]\n","\n","    def __len__(self):\n","        return self.size\n","\n","    def get_domain_distribution(self):\n","        return pd.Series(self.domains[:self.size]).value_counts()\n"]},{"cell_type":"code","execution_count":null,"id":"4eb2f843","metadata":{"id":"4eb2f843"},"outputs":[],"source":["class NaiveRehearsalBuffer:\n","    \"\"\"\n","    @inproceedings{Hsu18_EvalCL,\n","        title={Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},\n","        author={Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},\n","        booktitle={NeurIPS Continual learning Workshop },\n","        year={2018},\n","        url={https://arxiv.org/abs/1810.12488}\n","    }\n","    \"\"\"\n","    def __init__(self, buffer_size=1000):\n","        self.buffer_size = buffer_size\n","        self.domain_buffer = {}\n","\n","    def update_buffer(self, domain, dataset):\n","        # Add/overwrite current domain\n","        self.domain_buffer[domain] = Subset(dataset, torch.arange(len(dataset)))\n","\n","        # Recalculate quota\n","        num_domains = len(self.domain_buffer)\n","        buffer_quota_per_domain = self.buffer_size // num_domains\n","\n","        # Reduce all domains (including current)\n","        for domain in self.domain_buffer:\n","            subset = self.domain_buffer[domain]\n","            max_safe_samples_to_overwrite = min(buffer_quota_per_domain, len(subset.dataset))\n","            rand_indices = torch.randperm(len(dataset))[:max_safe_samples_to_overwrite].numpy()\n","            self.domain_buffer[domain] = Subset(dataset, rand_indices)\n","\n","    def get_loader_with_replay(self, current_domain, current_loader):\n","        current_dataset = current_loader.dataset\n","        replay_datasets = [dataset for domain, dataset in self.domain_buffer.items() if domain != current_domain]\n","\n","        #Enforces 1:1 ratio when current â‰¥ buffer\n","        total_replay = sum(len(dataset) for dataset in replay_datasets)\n","        if total_replay > 0:\n","            K = max(len(current_dataset) // total_replay, 1)\n","            replay_datasets = replay_datasets * K\n","\n","        combined_dataset =  ConcatDataset(replay_datasets + [current_dataset])\n","        combined_dataset = DataLoader(\n","            combined_dataset,\n","            batch_size=current_loader.batch_size,\n","            shuffle=True,\n","            num_workers=current_loader.num_workers,\n","            pin_memory=current_loader.pin_memory,\n","            drop_last=current_loader.drop_last\n","        )\n","        return combined_dataset\n","\n","    def get_domain_distribution(self):\n","        \"\"\"Returns {domain: num_samples} without needing Storage\"\"\"\n","        return {domain: len(subset) for domain, subset in self.domain_buffer.items()}\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ba7c71f1","metadata":{"id":"ba7c71f1"},"outputs":[],"source":["class GradientReversalFunction(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x):\n","        return x\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        return -grad_output\n","\n","class GradientReversal(nn.Module):\n","    def forward(self, x):\n","        return GradientReversalFunction.apply(x)\n","\n","\n","class DualBranchNet(nn.Module):\n","    def __init__(self, num_outputs=9, num_domains=6):\n","        super().__init__()\n","\n","        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n","        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.feature_dim = 1280\n","\n","        self.invariant = nn.Sequential(\n","            nn.Linear(self.feature_dim, 256),\n","            GradientReversal(),\n","            nn.ReLU(),\n","            nn.Linear(256, 256)\n","        )\n","\n","        self.domain_classifier = nn.Sequential(\n","            nn.Linear(256, num_domains)\n","        )\n","\n","        self.specific_residual = nn.Sequential(\n","            nn.Linear(256, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, 256)\n","        )\n","\n","        self.specific_domain_classifier = nn.Sequential(\n","            nn.Linear(256, num_domains)\n","        )\n","\n","        self.head = nn.Sequential(\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, num_outputs)\n","        )\n","\n","    def forward(self, x):\n","        base = self.backbone(x)\n","        base = self.pool(base).view(x.size(0), -1)\n","\n","        invariant_feats = self.invariant(base)\n","        invariant_domain_pred = self.domain_classifier(invariant_feats)\n","\n","        residual = self.specific_residual(invariant_feats)\n","        specific_feats = invariant_feats + residual\n","        specific_domain_pred = self.specific_domain_classifier(specific_feats)\n","\n","        combined = torch.cat([invariant_feats, specific_feats], dim=1)\n","        scores = self.head(combined)\n","\n","        return {\n","            'output': scores,\n","            'invariant_domain': invariant_domain_pred,\n","            'specific_domain': specific_domain_pred,\n","            'invariant_feats': invariant_feats,\n","            'specific_feats': specific_feats\n","        }\n"]},{"cell_type":"code","execution_count":null,"id":"e871f944","metadata":{"id":"e871f944"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"5849e928","metadata":{"id":"5849e928"},"source":["## ---"]},{"cell_type":"code","execution_count":null,"id":"29827e78","metadata":{"id":"29827e78"},"outputs":[],"source":["def evaluate_model(model, dataloader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for inputs, labels, _ in dataloader:\n","            inputs = inputs.to(device, dtype=torch.float32)\n","            labels = labels.to(device, dtype=torch.float32)\n","            outputs = model(inputs)['output']\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item() * inputs.size(0)\n","            total_samples += inputs.size(0)\n","    return total_loss / total_samples"]},{"cell_type":"code","execution_count":null,"id":"965dd01c","metadata":{"id":"965dd01c"},"outputs":[],"source":["df = pd.read_pickle(\"processed_all_data.pkl\")\n","\n","# Create domain-specific dataloaders\n","domains = df['domain'].unique()\n","domain_dataloaders = {}\n","for domain in domains:\n","    domain_df = df[df['domain'] == domain]\n","    #domain_df = domain_df.sample(frac=0.5, random_state=42)\n","    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128))  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n","    domain_dataloaders[domain] = loaders"]},{"cell_type":"markdown","id":"9a453cfb","metadata":{"id":"9a453cfb"},"source":["## Train functions"]},{"cell_type":"code","execution_count":null,"id":"d8caf0d5","metadata":{"id":"d8caf0d5"},"outputs":[],"source":["metrics = {\n","    'train_loss': [],\n","    'val_loss': [],\n","    'test_loss': [],\n","    'domain_performance': {domain: [] for domain in domains}\n","}"]},{"cell_type":"code","execution_count":null,"id":"811fbaac","metadata":{"id":"811fbaac"},"outputs":[],"source":["import time\n","from torch.utils.tensorboard import SummaryWriter\n","\n","def train_model(model, domains, domain_dataloaders, buffer, optimizer, writer, device, criterion, num_epochs=5):\n","    \"\"\"Main training function with integrated TensorBoard logging\"\"\"\n","    # Initialize tracking components\n","    writer = writer\n","    optimizer = optimizer\n","    global_step = 0\n","\n","    # Training loop through domains\n","    for domain_idx, current_domain in enumerate(domains):\n","        domain_start_time = time.time()\n","        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n","\n","        # Domain training\n","        for epoch in range(num_epochs):\n","            model.train()\n","            epoch_loss = 0.0\n","            samples_processed = 0\n","\n","            for batch_idx, (inputs, labels, _) in enumerate(train_loader):\n","                # Forward/backward pass\n","                inputs, labels = inputs.to(device), labels.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(inputs)['output']\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Update tracking\n","                epoch_loss += loss.item() * inputs.size(0)\n","                samples_processed += inputs.size(0)\n","                global_step += 1\n","\n","                # Batch logging (every 50 batches)\n","                if batch_idx % 50 == 0:\n","                    writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n","\n","            # Epoch summary\n","            avg_epoch_loss = epoch_loss / samples_processed\n","            writer.add_scalar('Loss/train_epoch', avg_epoch_loss, global_step)\n","\n","            # Validation\n","            val_loss = evaluate_model(model, domain_dataloaders[current_domain]['val'], criterion, device)\n","            writer.add_scalar('Loss/val_domain', val_loss, global_step)\n","\n","            print(f\"[{current_domain}][Epoch {epoch+1}] Train: {avg_epoch_loss:.4f} | Val: {val_loss:.4f}\")\n","\n","            torch.save(model.state_dict(), f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pth\")\n","\n","        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n","\n","        # Cross-domain evaluation\n","        for eval_domain in domains:\n","            eval_loss = evaluate_model(model, domain_dataloaders[eval_domain]['val'], criterion, device)\n","            writer.add_scalar(f'CrossVal/{eval_domain}', eval_loss, global_step)\n","\n","        print(f\"Domain {current_domain} completed in {time.time()-domain_start_time:.1f}s\")\n","\n","    writer.close()\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"571e6dbf","metadata":{"id":"571e6dbf"},"outputs":[],"source":["def train_domain(domain_idx, domain, num_epochs=5):\n","    global global_step\n","\n","    current_domain = domain\n","    loaders = domain_dataloaders[domain]\n","    train_loader = buffer.get_loader_with_replay(current_domain, loaders['train'])\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","\n","        for batch_idx, (inputs, labels, domain_labels) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","            domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n","\n","            # Split batch into current and replay samples\n","            current_mask = (domain_labels == domain_to_idx[domain])\n","            replay_mask = ~current_mask\n","\n","            # 1. Process CURRENT SAMPLES and update all parameters\n","            if current_mask.any():\n","                inputs_current = inputs[current_mask]\n","                labels_current = labels[current_mask]\n","                domain_labels_current = domain_labels[current_mask]\n","\n","                outputs_current = model(inputs_current)\n","                inv_feats = outputs_current['invariant_feats']\n","                spec_feats = outputs_current['specific_feats']\n","\n","                # Losses\n","                task_loss = mse_criterion(outputs_current['output'], labels_current)\n","                inv_domain_loss = ce_criterion(outputs_current['invariant_domain'], domain_labels_current)\n","                spec_domain_loss = ce_criterion(outputs_current['specific_domain'], domain_labels_current)\n","                similarity_loss = cos_criterion(inv_feats, spec_feats)\n","\n","                total_loss = (task_loss +\n","                              0.5 * inv_domain_loss +\n","                              0.2 * spec_domain_loss +\n","                              0.1 * similarity_loss\n","                )\n","                total_loss.backward()\n","\n","                # Metrics\n","                inv_pred = outputs_current['invariant_domain'].argmax(1)\n","                spec_pred = outputs_current['specific_domain'].argmax(1)\n","                inv_acc = (inv_pred == domain_labels_current).float().mean().item()\n","                spec_acc = (spec_pred == domain_labels_current).float().mean().item()\n","            else:\n","                inv_acc = 0.0\n","                spec_acc = 0.0\n","                task_loss = torch.tensor(0.0)\n","\n","            # 2. Process REPLAY SAMPLES  and update only specific branch + head)\n","            if replay_mask.any():\n","                inputs_replay = inputs[replay_mask]\n","                labels_replay = labels[replay_mask]\n","                domain_labels_replay = domain_labels[replay_mask]\n","\n","                # No gradients for backbone and invariant branch\n","                with torch.no_grad():\n","                    base_replay = model.backbone(inputs_replay)\n","                    base_replay = model.pool(base_replay).flatten(1)\n","                    inv_feats_replay = model.invariant(base_replay)\n","\n","                # Normal gradient for the rest\n","                residual = model.specific_residual(inv_feats_replay)\n","                spec_feats_replay = inv_feats_replay + residual\n","                spec_domain_pred = model.specific_domain_classifier(spec_feats_replay)\n","\n","                combined = torch.cat([inv_feats_replay, spec_feats_replay], dim=1)\n","                scores = model.head(combined)\n","\n","                # Losses\n","                task_loss_replay = mse_criterion(scores, labels_replay)\n","                spec_domain_loss_replay = ce_criterion(spec_domain_pred, domain_labels_replay)\n","                total_loss_replay = task_loss_replay + 0.2 * spec_domain_loss_replay\n","\n","                total_loss_replay.backward()\n","\n","            optimizer.step()\n","\n","            writer.add_scalar('Loss/train', task_loss.item(), global_step)\n","            writer.add_scalar('Loss/inv_domain', inv_domain_loss.item(), global_step)\n","            writer.add_scalar('Loss/spec_domain', spec_domain_loss.item(), global_step)\n","            writer.add_scalar('Loss/similarity', similarity_loss.item(), global_step)\n","            writer.add_scalar('Accuracy/invariant', inv_acc, global_step)\n","            writer.add_scalar('Accuracy/specific', spec_acc, global_step)\n","            writer.add_scalar('Replay/replay_count', replay_mask.sum().item(), global_step)\n","            writer.add_scalar('Replay/current_count', current_mask.sum().item(), global_step)\n","\n","            if batch_idx % 10 == 0:\n","                print(f\"Epoch {epoch} Batch {batch_idx} | \"\n","                    f\"Task: {task_loss.item():.4f} | \"\n","                    f\"InvAcc: {inv_acc:.2%} | SpecAcc: {spec_acc:.2%} | \"\n","                    f\"Sim: {similarity_loss.item():.4f} | \"\n","                    f\"Replay: {replay_mask.sum().item()} | \"\n","                    f\"Current: {current_mask.sum().item()}\")\n","\n","            global_step += 1\n","\n","        # Validation\n","        val_loss = evaluate_model(model, loaders['val'], mse_criterion, device)\n","\n","        writer.add_scalar('Loss/val', val_loss, global_step)\n","        print(f\"Domain {current_domain} | Epoch {epoch+1} | Val Loss: {val_loss:.4f}\")\n","        torch.save(model.state_dict(), f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pth\")\n","\n","    buffer.update_buffer(domain, loaders['train'].dataset)\n","\n"]},{"cell_type":"markdown","id":"1f961279","metadata":{"id":"1f961279"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"id":"048e529e","metadata":{"id":"048e529e"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = DualBranchNet(num_domains=len(domains)).to(device)\n","buffer = NaiveRehearsalBuffer(buffer_size=1000)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","mse_criterion = nn.MSELoss()\n","ce_criterion = nn.CrossEntropyLoss()\n","def cos_criterion(a, b):\n","    criterion = nn.CosineSimilarity()\n","    return (criterion(a, b) ** 2).mean()\n","\n","domain_to_idx = {d: i for i, d in enumerate(domains)}"]},{"cell_type":"code","execution_count":null,"id":"7dd2a55d","metadata":{"id":"7dd2a55d"},"outputs":[],"source":["import datetime\n","from torch.utils.tensorboard import SummaryWriter\n","\n","exp_name = f\"dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","writer = SummaryWriter(log_dir=f\"tensorboard/{exp_name}\")\n","\n","global_step = 0\n","\n","for domain_idx, domain in enumerate(domains):\n","    train_domain(domain_idx, domain, num_epochs=10)\n","\n","    for eval_domain in domains[:domain_idx+1]:\n","        loader = domain_dataloaders[eval_domain]['val']\n","        loss = evaluate_model(model, loader, mse_criterion, device)\n","        print(f\"Domain {eval_domain} | Val Loss: {loss:.4f}\")\n","\n","writer.close()\n","\n","\n","\n","exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","writer = SummaryWriter(log_dir=f\"tensorboard/{exp_name}\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = LGRBaseline().to(device)\n","buffer = NaiveRehearsalBuffer(buffer_size=1000)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()\n","\n","train_model(model, domains, domain_dataloaders, buffer, optimizer, writer, device, criterion, num_epochs=10)\n","\n","writer.close()\n","\n"]},{"cell_type":"markdown","id":"1f8bf879","metadata":{"id":"1f8bf879"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"id":"6678f170","metadata":{"id":"6678f170"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(metrics['val_loss'])\n","plt.plot(metrics['train_loss'])\n","plt.xlabel('Epoch')\n","plt.ylabel('Validation Loss')\n","plt.title('Validation Loss Over Epochs')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"1960d6fc","metadata":{"id":"1960d6fc"},"outputs":[],"source":["for domain, losses in metrics['domain_performance'].items():\n","    plt.plot(losses, label=domain)\n","plt.xlabel('Domain Training Step')\n","plt.ylabel('Loss')\n","plt.title('Domain Performance Over Training')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"d878b1dc","metadata":{"id":"d878b1dc"},"outputs":[],"source":["for domain, losses in metrics['domain_performance'].items():\n","    print(f\"{domain}: Initial = {losses[0]:.4f}, Final = {losses[-1]:.4f}, Change = {losses[-1] - losses[0]:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"895de6b8","metadata":{"id":"895de6b8"},"outputs":[],"source":["# After model output\n","outputs = model(inputs)  # Should be in [1,5]\n","print(f\"Output range: {outputs.min().item()}â€“{outputs.max().item()}\")\n"]},{"cell_type":"code","execution_count":null,"id":"1580d671","metadata":{"id":"1580d671"},"outputs":[],"source":["#Single Batch sanity check for dual branch model\n","\n","# Get first batch\n","single_batch = next(iter(domain_dataloaders[domain]['train']))\n","inputs, labels, domain_labels = single_batch\n","inputs, labels = inputs.to(device), labels.to(device)\n","domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n","\n","# Overfit test\n","for epoch in range(100):\n","    optimizer.zero_grad()\n","\n","    # Forward\n","    outputs = model(inputs)\n","    inv_feats = outputs['invariant_feats']\n","    spec_feats = outputs['specific_feats']\n","\n","    # Losses\n","    task_loss = mse_criterion(outputs['output'], labels)\n","    inv_domain_loss = ce_criterion(outputs['invariant_domain'], domain_labels)\n","    spec_domain_loss = ce_criterion(outputs['specific_domain'], domain_labels)\n","    similarity_loss = cos_criterion(inv_feats, spec_feats)\n","    total_loss = task_loss + 0.5*inv_domain_loss + 0.2*spec_domain_loss + 0.1*similarity_loss\n","\n","    # Backward\n","    total_loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch}: Loss {total_loss.item():.4f}\")\n","\n","    # Early exit if loss < 0.001\n","    if total_loss < 0.001:\n","        break\n"]}],"metadata":{"kernelspec":{"display_name":"socialsense","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}