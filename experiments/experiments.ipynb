{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  # Enforce deterministic algorithms\n",
    "        torch.backends.cudnn.benchmark = False     # Disable benchmark for reproducibility\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)       # Seed Python hashing, which can affect ordering\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d0cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in dataloader:\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs)['output']\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c68e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"processed_all_data.pkl\")\n",
    "\n",
    "# Create domain-specific dataloaders\n",
    "domains = df['domain'].unique()\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128))  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a5b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Reservoire Buffer per batch\n",
    "# for batch_idx, (inputs, labels, _) in enumerate(train_loader):\n",
    "#             batch_start = time.time()    \n",
    "\n",
    "#             inputs = inputs.to(device, dtype=torch.float32)\n",
    "#             labels = labels.to(device, dtype=torch.float32)\n",
    "\n",
    "#             # ReservoirBuffer # Sample the buffer and add replay samples to training\n",
    "#             # if buffer and random.random() < buffer.replay_ratio:\n",
    "#             #     batch_size = inputs.size(0)\n",
    "#             #     replay_batch = buffer.sample(int(batch_size * 0.25))\n",
    "#             #     if replay_batch:\n",
    "#             #         replay_inputs, replay_labels, _ = zip(*replay_batch)\n",
    "#             #         replay_inputs = torch.stack(replay_inputs)\n",
    "#             #         replay_labels = torch.stack(replay_labels)\n",
    "#             #         inputs = torch.cat([inputs, replay_inputs])\n",
    "#             #         labels = torch.cat([labels, replay_labels])\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # ReservoirBuffer # Add current batch to buffer\n",
    "#             # with torch.no_grad():\n",
    "#             #     samples = [(img.detach(), label.detach(), domain) for img, label in zip(inputs, labels)]\n",
    "#             #     buffer.add(samples)\n",
    "\n",
    "            \n",
    "#             batch_time = time.time() - batch_start\n",
    "                \n",
    "#             running_loss += loss.item() * inputs.size(0)\n",
    "#             total_train_samples += inputs.size(0)\n",
    "\n",
    "#             if batch_idx % 10 == 9:               \n",
    "#                 avg_loss = running_loss / 10\n",
    "#                 batch_time = time.time() - batch_start  # ← Keep this\n",
    "#                 print(\n",
    "#                     f\"Domain: {domain} | Epoch: {epoch+1} | \"\n",
    "#                     f\"Batch: {batch_idx+1}/{total_batches} | \"\n",
    "#                     f\"Avg Loss: {avg_loss:.4f} | \"  # Changed to Avg Loss\n",
    "#                     f\"Time/batch: {batch_time:.2f}s\"\n",
    "#                 )\n",
    "#                 running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa5c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_model(model, domains, domain_dataloaders, buffer, optimizer, writer, device, criterion, num_epochs=5):\n",
    "    \"\"\"Main training function with integrated TensorBoard logging\"\"\"\n",
    "    # Initialize tracking components\n",
    "    writer = writer\n",
    "    optimizer = optimizer\n",
    "    global_step = 0\n",
    "\n",
    "    # Training loop through domains\n",
    "    for domain_idx, current_domain in enumerate(domains):\n",
    "        domain_start_time = time.time()\n",
    "        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        \n",
    "        # Domain training\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            samples_processed = 0\n",
    "            \n",
    "            for batch_idx, (inputs, labels, _) in enumerate(train_loader):\n",
    "                # Forward/backward pass\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)['output']\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update tracking\n",
    "                epoch_loss += loss.item() * inputs.size(0)\n",
    "                samples_processed += inputs.size(0)\n",
    "                global_step += 1\n",
    "\n",
    "                # Batch logging (every 50 batches)\n",
    "                if batch_idx % 50 == 0:\n",
    "                    writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n",
    "\n",
    "            # Epoch summary\n",
    "            avg_epoch_loss = epoch_loss / samples_processed\n",
    "            writer.add_scalar('Loss/train_epoch', avg_epoch_loss, global_step)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss = evaluate_model(model, domain_dataloaders[current_domain]['val'], criterion, device)\n",
    "            writer.add_scalar('Loss/val_domain', val_loss, global_step)\n",
    "            \n",
    "            print(f\"[{current_domain}][Epoch {epoch+1}] Train: {avg_epoch_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), f\"model_checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pth\") \n",
    "        \n",
    "        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n",
    "         \n",
    "        # Cross-domain evaluation\n",
    "        for eval_domain in domains:\n",
    "            eval_loss = evaluate_model(model, domain_dataloaders[eval_domain]['val'], criterion, device)\n",
    "            writer.add_scalar(f'CrossVal/{eval_domain}', eval_loss, global_step)\n",
    "        \n",
    "        print(f\"Domain {current_domain} completed in {time.time()-domain_start_time:.1f}s\")\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d0cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_domain(domain_idx, domain, num_epochs=5):\n",
    "    global global_step\n",
    "\n",
    "    current_domain = domain\n",
    "    loaders = domain_dataloaders[domain]\n",
    "    train_loader = buffer.get_loader_with_replay(current_domain, loaders['train'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (inputs, labels, domain_labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "            \n",
    "            # Split batch into current and replay samples\n",
    "            current_mask = (domain_labels == domain_to_idx[domain])\n",
    "            replay_mask = ~current_mask\n",
    "\n",
    "            # 1. Process CURRENT SAMPLES and update all parameters\n",
    "            if current_mask.any():\n",
    "                inputs_current = inputs[current_mask]\n",
    "                labels_current = labels[current_mask]\n",
    "                domain_labels_current = domain_labels[current_mask]\n",
    "                \n",
    "                outputs_current = model(inputs_current)\n",
    "                inv_feats = outputs_current['invariant_feats']\n",
    "                spec_feats = outputs_current['specific_feats']\n",
    "                \n",
    "                # Losses\n",
    "                task_loss = mse_criterion(outputs_current['output'], labels_current)\n",
    "                inv_domain_loss = ce_criterion(outputs_current['invariant_domain'], domain_labels_current)\n",
    "                spec_domain_loss = ce_criterion(outputs_current['specific_domain'], domain_labels_current)\n",
    "                similarity_loss = cos_criterion(inv_feats, spec_feats)\n",
    "                \n",
    "                total_loss = (task_loss + \n",
    "                              0.5 * inv_domain_loss + \n",
    "                              0.2 * spec_domain_loss + \n",
    "                              0.1 * similarity_loss\n",
    "                )\n",
    "                total_loss.backward()\n",
    "\n",
    "                # Metrics\n",
    "                inv_pred = outputs_current['invariant_domain'].argmax(1)\n",
    "                spec_pred = outputs_current['specific_domain'].argmax(1)\n",
    "                inv_acc = (inv_pred == domain_labels_current).float().mean().item()\n",
    "                spec_acc = (spec_pred == domain_labels_current).float().mean().item()\n",
    "            else:\n",
    "                inv_acc = 0.0\n",
    "                spec_acc = 0.0\n",
    "                task_loss = torch.tensor(0.0)\n",
    "\n",
    "            # 2. Process REPLAY SAMPLES  and update only specific branch + head)\n",
    "            if replay_mask.any():\n",
    "                inputs_replay = inputs[replay_mask]\n",
    "                labels_replay = labels[replay_mask]\n",
    "                domain_labels_replay = domain_labels[replay_mask]\n",
    "                \n",
    "                # No gradients for backbone and invariant branch\n",
    "                with torch.no_grad():\n",
    "                    base_replay = model.backbone(inputs_replay)\n",
    "                    base_replay = model.pool(base_replay).flatten(1)\n",
    "                    inv_feats_replay = model.invariant(base_replay)\n",
    "                \n",
    "                # Normal gradient for the rest\n",
    "                residual = model.specific_residual(inv_feats_replay)\n",
    "                spec_feats_replay = inv_feats_replay + residual\n",
    "                spec_domain_pred = model.specific_domain_classifier(spec_feats_replay)\n",
    "                \n",
    "                combined = torch.cat([inv_feats_replay, spec_feats_replay], dim=1)\n",
    "                scores = model.head(combined)\n",
    "\n",
    "                # Losses\n",
    "                task_loss_replay = mse_criterion(scores, labels_replay)\n",
    "                spec_domain_loss_replay = ce_criterion(spec_domain_pred, domain_labels_replay)\n",
    "                total_loss_replay = task_loss_replay + 0.2 * spec_domain_loss_replay\n",
    "                \n",
    "                total_loss_replay.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            writer.add_scalar('Loss/train', task_loss.item(), global_step)\n",
    "            writer.add_scalar('Loss/inv_domain', inv_domain_loss.item(), global_step)\n",
    "            writer.add_scalar('Loss/spec_domain', spec_domain_loss.item(), global_step)\n",
    "            writer.add_scalar('Loss/similarity', similarity_loss.item(), global_step)\n",
    "            writer.add_scalar('Accuracy/invariant', inv_acc, global_step)\n",
    "            writer.add_scalar('Accuracy/specific', spec_acc, global_step)\n",
    "            writer.add_scalar('Replay/replay_count', replay_mask.sum().item(), global_step)\n",
    "            writer.add_scalar('Replay/current_count', current_mask.sum().item(), global_step)  \n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch} Batch {batch_idx} | \"\n",
    "                    f\"Task: {task_loss.item():.4f} | \"\n",
    "                    f\"InvAcc: {inv_acc:.2%} | SpecAcc: {spec_acc:.2%} | \"\n",
    "                    f\"Sim: {similarity_loss.item():.4f} | \"\n",
    "                    f\"Replay: {replay_mask.sum().item()} | \"\n",
    "                    f\"Current: {current_mask.sum().item()}\")\n",
    "                \n",
    "            global_step += 1 \n",
    "    \n",
    "        # Validation\n",
    "        val_loss = evaluate_model(model, loaders['val'], mse_criterion, device)\n",
    "        \n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        print(f\"Domain {current_domain} | Epoch {epoch+1} | Val Loss: {val_loss:.4f}\")\n",
    "        torch.save(model.state_dict(), f\"model_checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pth\")  \n",
    "\n",
    "    buffer.update_buffer(domain, loaders['train'].dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e27944",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'test_loss': [],\n",
    "    'domain_performance': {domain: [] for domain in domains}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d3a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "ce_criterion = nn.CrossEntropyLoss()\n",
    "def cos_criterion(a, b):\n",
    "    criterion = nn.CosineSimilarity()\n",
    "    return (criterion(a, b) ** 2).mean()\n",
    "\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "exp_name = f\"dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"tensorboard/{exp_name}\")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for domain_idx, domain in enumerate(domains):\n",
    "    train_domain(domain_idx, domain, num_epochs=10)\n",
    "    \n",
    "    for eval_domain in domains[:domain_idx+1]:\n",
    "        loader = domain_dataloaders[eval_domain]['val']\n",
    "        loss = evaluate_model(model, loader, mse_criterion, device)\n",
    "        print(f\"Domain {eval_domain} | Val Loss: {loss:.4f}\")\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"tensorboard/{exp_name}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LGRBaseline().to(device)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_model(model, domains, domain_dataloaders, buffer, optimizer, writer, device, criterion, num_epochs=10)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08c284",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2770643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metrics['val_loss'])\n",
    "plt.plot(metrics['train_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acd44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for domain, losses in metrics['domain_performance'].items():\n",
    "    plt.plot(losses, label=domain)\n",
    "plt.xlabel('Domain Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Domain Performance Over Training')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abda19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, losses in metrics['domain_performance'].items():\n",
    "    print(f\"{domain}: Initial = {losses[0]:.4f}, Final = {losses[-1]:.4f}, Change = {losses[-1] - losses[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model output\n",
    "outputs = model(inputs)  # Should be in [1,5]\n",
    "print(f\"Output range: {outputs.min().item()}–{outputs.max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0563710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddb49c1d",
   "metadata": {},
   "source": [
    "## Overfitting Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6476e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Batch sanity check for dual branch model\n",
    "\n",
    "# Get first batch\n",
    "single_batch = next(iter(domain_dataloaders[domain]['train']))\n",
    "inputs, labels, domain_labels = single_batch\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "domain_labels = torch.tensor([domain_to_idx[d] for d in domain_labels], device=device)\n",
    "\n",
    "# Overfit test\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward\n",
    "    outputs = model(inputs)\n",
    "    inv_feats = outputs['invariant_feats']\n",
    "    spec_feats = outputs['specific_feats']\n",
    "    \n",
    "    # Losses\n",
    "    task_loss = mse_criterion(outputs['output'], labels)\n",
    "    inv_domain_loss = ce_criterion(outputs['invariant_domain'], domain_labels)\n",
    "    spec_domain_loss = ce_criterion(outputs['specific_domain'], domain_labels)\n",
    "    similarity_loss = cos_criterion(inv_feats, spec_feats)\n",
    "    total_loss = task_loss + 0.5*inv_domain_loss + 0.2*spec_domain_loss + 0.1*similarity_loss\n",
    "    \n",
    "    # Backward\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss {total_loss.item():.4f}\")\n",
    "    \n",
    "    # Early exit if loss < 0.001\n",
    "    if total_loss < 0.001:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ab1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfit Single Batch Check\n",
    "\n",
    "test_model = DualBranchNet().to(device)\n",
    "test_optimizer = optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)\n",
    "\n",
    "# %%\n",
    "num_test_epochs = 500\n",
    "for epoch in range(num_test_epochs):\n",
    "    test_optimizer.zero_grad()\n",
    "\n",
    "    outputs = test_model(inputs)\n",
    "    loss = criterion(outputs['output'], labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    test_optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Overfit Epoch {epoch+1}/{num_test_epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
