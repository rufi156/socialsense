{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc356782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stored here just for reference. \n",
    "# those are the scripts that I used to runn all past models.\n",
    "# their run history and checkpoints can be found in checkpoints dir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f82ee9a",
   "metadata": {},
   "source": [
    "### runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "# # For baseline model\n",
    "# baseline_model = LGRBaseline().to(device)\n",
    "# optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "# buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "# exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "# writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "# baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "# unified_train_loop(\n",
    "#     model=baseline_model,\n",
    "#     domains=domains,\n",
    "#     domain_dataloaders=domain_dataloaders,\n",
    "#     buffer=buffer,\n",
    "#     optimizer=optimizer,\n",
    "#     writer=writer,\n",
    "#     device=device,\n",
    "#     batch_fn=baseline_batch,\n",
    "#     batch_kwargs=baseline_kwargs,\n",
    "#     num_epochs=10,\n",
    "#     exp_name=exp_name\n",
    "# )\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(num_domains=len(domains)).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=False\n",
    ")\n",
    "\n",
    "# For DualBranchNet\n",
    "dual_model = DualBranchNet(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"nores_gradclip_winit_wnorm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"deep_norm_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bbdetach_deep_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ff55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=True, weights_norm=False, layer_norm=False, detach_base=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bdetach_batch16_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(16, 64, 64), resize_img_to=(128, 128), seed=SEED)  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=True\n",
    ")\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128), seed=SEED)  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=True, freeze_base='partial').to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"bdetach_pfrozen_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=True\n",
    ")\n",
    "\n",
    "dual_model = DualBranchNet_deep(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full').to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_binary_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary = True,\n",
    "    full_replay = False\n",
    ")\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=False, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=False).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"ffrozen_binary_explicitgrl_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary=True,\n",
    "    full_replay = False\n",
    ")\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_binary(weights_init=True, weights_norm=False, layer_norm=False, detach_base=False, freeze_base='full', explicit_grl=True).to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    # return (nn.CosineSimilarity()(a, b) ** 2).mean()\n",
    "\n",
    "exp_name = f\"freplay_ffrozen_binary_explicitgrl_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'ce_criterion': nn.CrossEntropyLoss(),\n",
    "    'cos_criterion': cos_criterion,\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dualbranch_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary=True,\n",
    "    full_replay = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce351ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'pretrained_backbone_cnn_branch': ('pretrained', 'simple', 'simple'), \n",
    "    # 'linear_branch': ('3conv', 'linear', 'simple'),\n",
    "    # 'cnn_branch': ('3conv', 'simple', 'simple'),\n",
    "    # 'adversarial': ('2conv', 'adversarial', 'adversarial'),\n",
    "    'cnn_specialised_branches': ('3conv', 'special', 'simple')\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=False,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'pretrained_simple': ('pretrained', 'simple', '3linear', False), \n",
    "    'pretrained_special': ('pretrained', 'special', '3linear', False),\n",
    "    '3conv_simple': ('3conv', 'simple', '3linear', True),\n",
    "    '3conv_special': ('3conv', 'special', '3linear', True),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end, detach_base) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'pretrained_simple_0.25branches': ('pretrained', 'simple', '3linear'), \n",
    "    # '3conv_adversarial': ('3conv', 'adversarial', 'adversarial'),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "    exp_name = f\"CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    dualbranch_kwargs = {\n",
    "        'mse_criterion': nn.MSELoss(),\n",
    "        'ce_criterion': nn.CrossEntropyLoss(),\n",
    "        'cos_criterion': cos_criterion,\n",
    "        'domain_to_idx': domain_to_idx,\n",
    "        'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "    }\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params = {'head': 0.5, 'social': 0.25, 'room': 0.25}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac16b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'pretrained_simple': ('pretrained', 'simple', '3linear'), \n",
    "    '3conv_adversarial': ('3conv', 'adversarial', 'adversarial'),\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, end) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone} + {branch} + {end}\")\n",
    "    fold_history = {}\n",
    "    for fold_num, domain_dataloaders in tqdm(fold_loaders.items()):\n",
    "        dual_model = DualBranchCNNNet(backbone_type=backbone, branch_type=branch, end_type=end, batch_norm=True).to(device)\n",
    "        optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "        buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "        def cos_criterion(a, b):\n",
    "            return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "        exp_name = f\"fold{fold_num}_CNN_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "        dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "        history = unified_train_loop(\n",
    "            model=dual_model,\n",
    "            domains=domains,\n",
    "            domain_dataloaders=domain_dataloaders,\n",
    "            buffer=buffer,\n",
    "            optimizer=optimizer,\n",
    "            writer=writer,\n",
    "            device=device,\n",
    "            batch_fn=dualbranch_batch,\n",
    "            batch_kwargs=dualbranch_kwargs,\n",
    "            num_epochs=10,\n",
    "            exp_name=exp_name,\n",
    "            gradient_clipping=True,\n",
    "            detach_base=False,\n",
    "            binary = True,\n",
    "            full_replay = True,\n",
    "            collect_tsne_data=False,\n",
    "            loss_params = {'head': 0.5, 'social': 0.25, 'room': 0.25}\n",
    "        )\n",
    "        fold_history[fold_num] = history\n",
    "    with open(f\"../checkpoints/5foldcrossval_{exp_name}_history.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fold_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "\n",
    "dual_model = DualBranchNet_DANN().to(device)\n",
    "optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "def cos_criterion(a, b):\n",
    "    return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "\n",
    "exp_name = f\"DANN_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "dualbranch_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "    'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "}\n",
    "history = unified_train_loop(\n",
    "    model=dual_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=dann_batch,\n",
    "    batch_kwargs=dualbranch_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    detach_base=False,\n",
    "    binary = True,\n",
    "    full_replay = True,\n",
    "    collect_tsne_data=False,\n",
    "    loss_params = {'head': 1, 'social': 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e11dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders, split_idx = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(512,288), seed=SEED, return_splits=True)  #512, 288 # 352,128 #LGR had 128,128 MobileNetv2 had 224, 224\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "baseline_model = LGRBaseline().to(device)\n",
    "optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "exp_name = f\"baselinemodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "unified_train_loop(\n",
    "    model=baseline_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=optimizer,\n",
    "    writer=writer,\n",
    "    device=device,\n",
    "    batch_fn=baseline_batch,\n",
    "    batch_kwargs=baseline_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    loss_params={}\n",
    ")\n",
    "\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    loaders, split_idx = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(256,144), seed=SEED, return_splits=True)  #512, 288 # 352,128 #LGR had 128,128 MobileNetv2 had 224, 224\n",
    "    domain_dataloaders[domain] = loaders\n",
    "\n",
    "testing_scenarios = {\n",
    "    'simple': ('3conv', 'simple', False, 1000), \n",
    "    'simple_buffer05': ('3conv', 'simple', False, 500),\n",
    "    'special': ('3conv', 'special', False, 1000),\n",
    "    'special_bnorm': ('3conv', 'special', True, 1000)\n",
    "}\n",
    "\n",
    "for name, (backbone, branch, batch_norm, buffer_size) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {backbone=} {branch=} {batch_norm=} {buffer_size=}\")\n",
    "    dual_model = DualBranchNet_minimal(backbone_type=backbone, branch_type=branch, head_type='3layer', batch_norm=batch_norm).to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"minimal_{name}_dualbranchmodel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=baseline_batch,\n",
    "        batch_kwargs=baseline_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87cb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dualbranch_CNN_1epoch': (DualBranchCNNNet('3conv', 'simple', 'simple'), 500, 1), \n",
    "    'minimal_absurd_buff500': (DualBranchNet_minimal(backbone_type='absurd', branch_type='absurd', head_type='absurd'), 500, 10),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    baseline_kwargs = {'mse_criterion': nn.MSELoss()}\n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss()\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=baseline_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f09eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'DANN_notrain1dom': (DualBranchNet_DANN(), 500, 10, 1)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dann_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':0.5, 'social':0.5}\n",
    "    )\n",
    "\n",
    "testing_scenarios = {\n",
    "    'dualbranch_CNN_1epoch': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='3linear'), 500, 1, 1), \n",
    "    'dualbranch_CNN_dynamicalpha': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='3linear'), 500, 10, 0), \n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':1, 'social':1, 'room':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04508d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    '3conv_simple_simple_bnorm_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, False), \n",
    "    '3conv_simple_simple_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=False), 500, 10, 1, False), \n",
    "    '3conv_simple_simple_bnorm': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, True),\n",
    "    '3conv_simple_3linear_bnorm_nogradclip': (DualBranchCNNNet(backbone_type='3conv', branch_type='simple', end_type='simple', batch_norm=True), 500, 10, 1, False),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, gradient_clip) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=gradient_clip,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head':1, 'social':1, 'room':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f51030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(224,224), return_splits=False, double_img=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    'deeplabv3mobilenetv3_dann': (DANN_classifier_poc(), 500, 30, 0, True),\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, gradient_clip) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None #SummaryWriter(log_dir=f\"../tensorboard/{exp_name}\")\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=dann_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=gradient_clip,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=True,\n",
    "        loss_params={'head':1, 'social':1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd642e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#didit work, the architecture is flawed. Assesing category of features based on catastrophic forgetting makes no sense, \n",
    "# the features are not experiencing CF, the weights are. \n",
    "# But passing weights to split branches is not justified.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(224,224), return_splits=False, double_img=False)\n",
    "\n",
    "# CORRECTED: Initialize model with proper parameter names\n",
    "cf_model = CatastrophicForgettingDisentanglementModel(\n",
    "    backbone_output_channels=640,\n",
    "    branch_hidden_channels=256,\n",
    "    branch_output_channels=128,\n",
    "    num_outputs=9\n",
    ").to(device)\n",
    "\n",
    "# CORRECTED: Optimizers with proper parameter paths\n",
    "main_optimizer = torch.optim.Adam([\n",
    "    {'params': cf_model.forgetting_adapter.adapter.parameters()},\n",
    "    {'params': cf_model.branch_forgetting.parameters()},\n",
    "    {'params': cf_model.branch_not_forgetting.parameters()},\n",
    "    {'params': cf_model.head.parameters()},\n",
    "    {'params': cf_model.fusion.parameters()}\n",
    "], lr=1e-3)\n",
    "\n",
    "gating_optimizer = torch.optim.Adam(\n",
    "    cf_model.forgetting_adapter.gating.parameters(),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# restart = {\n",
    "# 'global_step':\n",
    "# 'history':\n",
    "# 'domain':\n",
    "# }\n",
    "\n",
    "# Use your existing buffer and data\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)\n",
    "\n",
    "# Modified batch kwargs for our model\n",
    "cf_kwargs = {\n",
    "    'mse_criterion': nn.MSELoss(),\n",
    "    'domain_to_idx': domain_to_idx,\n",
    "}\n",
    "\n",
    "# Run training with corrected functions\n",
    "exp_name = f\"forgetinggated_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "history = catastrophic_forgetting_train_loop(\n",
    "    model=cf_model,\n",
    "    domains=domains,\n",
    "    domain_dataloaders=domain_dataloaders,\n",
    "    buffer=buffer,\n",
    "    optimizer=main_optimizer,\n",
    "    gating_optimizer=gating_optimizer,\n",
    "    device=device,\n",
    "    batch_fn=catastrophic_forgetting_batch,\n",
    "    batch_kwargs=cf_kwargs,\n",
    "    num_epochs=10,\n",
    "    exp_name=exp_name,\n",
    "    gradient_clipping=True,\n",
    "    loss_params={'main': 1.0, 'adapter': 0.5, 'gating': 0.1},\n",
    "    restart = restart\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(288,512), return_splits=False, double_img=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dann_mobilenet': (DualBranchNet_DANN(backbone_type='mobilenet'), 500, 10, 0, False, dann_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 500, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 500, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_linear_simple': (DualBranchCNNNet(9,6,'pretrained', 'linear', 'simple'), 500, 10, 0, False, dualbranch_batch),\n",
    "    'baseline': (LGRBaseline(), 500, 10, 0, False, baseline_batch)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, detach_base, batch_fn) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=batch_fn,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c53431",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "domains = df['domain'].unique()\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "domain_dataloaders = get_dataloader(df, batch_sizes=(32, 64, 64), resize_img_to=(288,512), return_splits=False, double_img=False)\n",
    "\n",
    "testing_scenarios = {\n",
    "    # 'dann_mobilenet_buff500': (DualBranchNet_DANN(backbone_type='mobilenet'), 500, 10, 0, False, dann_batch),\n",
    "    'dann_mobilenet_buff120': (DualBranchNet_DANN(backbone_type='mobilenet'), 120, 10, 0, False, dann_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach_buff500': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 500, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_2conv_adversarial_3linear_detach_buff120': (DualBranchCNNNet(9,6,'2conv', 'adversarial', '3linear'), 120, 10, 0, True, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear_buff500': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 500, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_simple_3linear_buff120': (DualBranchCNNNet(9,6,'pretrained', 'simple', '3linear'), 120, 10, 0, False, dualbranch_batch),\n",
    "    # 'dual_mobilenet_linear_simple_buff120': (DualBranchCNNNet(9,6,'pretrained', 'linear', 'simple'), 120, 10, 0, False, dualbranch_batch),\n",
    "    # 'baseline_buff500': (LGRBaseline(), 500, 10, 0, False, baseline_batch),\n",
    "    # 'baseline_buff120': (LGRBaseline(), 120, 10, 0, False, baseline_batch)\n",
    "}\n",
    "\n",
    "for name, (model, buffer_size, epochs, alpha, detach_base, batch_fn) in testing_scenarios.items():\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=buffer_size)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': alpha,\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=batch_fn,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=epochs,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=detach_base,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5}\n",
    "    )\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# model_names = [\n",
    "#     'dann_mobilenet_buff500',\n",
    "#     'dann_mobilenet_buff120',\n",
    "#     'dual_2conv_adversarial_3linear_detach_buff500',\n",
    "#     'dual_2conv_adversarial_3linear_detach_buff120',\n",
    "#     'dual_mobilenet_simple_3linear_buff500',\n",
    "#     'dual_mobilenet_simple_3linear_buff120',\n",
    "#     'dual_mobilenet_linear_simple_buff120',\n",
    "#     'baseline_buff500',\n",
    "#     'baseline_buff120'\n",
    "#  ]\n",
    "\n",
    "# for model_name in model_names:\n",
    "#     print(f\"Running {model_name}\")\n",
    "#     result = subprocess.run(\n",
    "#         [\"python\", \"train_models.py\", \"--model_name\", model_name],\n",
    "#         capture_output=True, text=True\n",
    "#     )\n",
    "#     print(result.stdout)\n",
    "#     if result.stderr:\n",
    "#         print(\"Error:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59703cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing_scenarios = {\n",
    "    # 'heuristic_small_env': (DualBranchModel(), [(288,512), (144,256)], False),\n",
    "    # 'heuristic_square_img': (DualBranchModel(), [(224,224)]*2, False),\n",
    "    'heuristic_eval_buffer': (DualBranchModel(), [(288,512)]*2, True)\n",
    "}\n",
    "\n",
    "for name, (model, img_size, eval_buffer) in testing_scenarios.items():\n",
    "\n",
    "    transform_soc = transforms.Compose([\n",
    "        transforms.Resize(img_size[0]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    transform_env = transforms.Compose([\n",
    "        transforms.Resize(img_size[1]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "    df['image_path_env'] = df['image_path'].apply(lambda p: str(Path('../data/masked/environment') / Path(p).name))\n",
    "    df['image_path_social'] = df['image_path'].apply(lambda p: str(Path('../data/masked/social') / Path(p).name))\n",
    "    domain_dataloaders = get_dataloader(df, batch_sizes=(16, 64, 64), return_splits=False, double_img=True, transforms=[transform_soc, transform_env], num_workers=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    domains = df['domain'].unique()\n",
    "    domain_to_idx = {d: i for i, d in enumerate(domains)}\n",
    "\n",
    "\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    # optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': dual_model.social_branch.parameters()},\n",
    "        {'params': dual_model.env_branch.parameters()},\n",
    "        {'params': dual_model.head.parameters()},\n",
    "    ], lr=1e-3)\n",
    "    classifier_optimizer = optim.Adam([ \n",
    "        {'params': dual_model.social_classifier.parameters()},\n",
    "        {'params': dual_model.env_classifier.parameters()}\n",
    "    ], lr=1e-3)\n",
    "\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "    if eval_buffer:\n",
    "        eval_buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    writer = None\n",
    "\n",
    "    def cos_criterion(a, b):\n",
    "        return (1 - torch.abs(nn.CosineSimilarity()(a, b))).mean()\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'cos_criterion': cos_criterion,\n",
    "            'domain_to_idx': domain_to_idx,\n",
    "            'bce_criterion': nn.BCEWithLogitsLoss(),\n",
    "            'alpha': 0,\n",
    "            'class_optimizer': classifier_optimizer\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        writer=writer,\n",
    "        device=device,\n",
    "        batch_fn=heuristic_dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        detach_base=False,\n",
    "        binary = True,\n",
    "        full_replay = True,\n",
    "        collect_tsne_data=False,\n",
    "        loss_params={'head': 1, 'social': 1, 'room': 0.5},\n",
    "        eval_buffer=eval_buffer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: heuristic__small_imgs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27295f2e8ce34379b914a9093c16cb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total training:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e74d3fff6de495bbea8c0017edcac32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain Home:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3360a9deeb45e5ae4e11c2c8c3f0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adb3f7545564be9b003c7d96807a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d88193e16f84d7bb0c6bcbf1a31e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af57e69f3f64aa689e0c5e68f8caac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e988b198be7c4c6dbc4bd32a5fcfb108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2f213d9b254dc189203ae9f9defc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066668a5a9de42b9aa932b2ef01568c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc1c9bf8c1548528ebac755c216d705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f3c9aa722f476d9518e4e4dff29877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cce3d5a7634167b36a41eb38082deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17f461b6ce44a5fb2aa712a7b5a6d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain BigOffice-2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f13ce6a5d794bc5bb9f8b00e11b261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb48f31546f449a5b6966c1eb9274ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48005fc8799422f8d89a8968cd4781f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d982aba2a944980ad6f384ba4b104f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc35bd93ec140f9bccbe20c6ad1679b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8707bd9a4d45c5ba71e5bf80e73b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b7b389c3ce46c0b36be9c26b6ac627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b29c1d7bf844175b8dde94940e6fbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e5ba0c18c047bd869fb4ce12725c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb822709dde4935a7c09274b21b5a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a060ff180ba4ab69e403c994febf6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain BigOffice-3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cada9650c44e708c9959d2f41126a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1c14b3e60d42b8b5b27bdc8bb94088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea01c65c1d94a39b47829f2bac76486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77055446f3443f6bdc3744ee5515b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82196c5b13184b23b08d4fc51c8622f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5ec42fb58740b9a5e5da8d3844d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8917639b1e3a415dbf0db69adee43e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e53a68b0d94004b2c4961a675b5caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffffa72b434b4e35a152604375fc40fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d482f06cc42068ce259bcd5b2ad88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a0d978c2a14a4e914c1bd595cc8694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain Hallway:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f6d7cce4e4412f8a0c77b223b1998c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf9b10dd8fb44788183b6f923b8a510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf6eeb409d44911963bfa4b75a834d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a15b562b2c4474c914840f78d0dd02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4021142813a4aba82332ded41eb3ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf08a5ed84d0444691aef8a2c59e1c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17be14109c664687bcece21ed6ef25b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e533de2ba6b34ef59a09de0f41d99520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7744840ebe4cd1b233722806c7c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1e1ddd20134bb781e3ed483eb52bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eace3c8165457792987a37b9ebbd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain MeetingRoom:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c61239c4e64c1198760dd4ed5d9049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701732142af04800859313a5130a5db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a1e396398843a69ac87a4d12be497e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d068efcf3fc64d5ead0c71cdc57544c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43b5f7681e240cc89292490e3eb5823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618fdfd0b165491c946d5283ce9d5ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658a295c0dd44bb2909867a83fa09fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a935879e7a44f6b3e6e8847d121866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f402229647241d183d91f14db538006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4321fc7158db44fa8eb0168cabd92940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a977092634417bac29ef3c02dad8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current domain SmallOffice:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4f59a999cd4fc89de7a488646564e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 0:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1555a8bf3684b349a897fa0a647b18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 1:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf18a9802e0d48c9bf33935190d49315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 2:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf60044ea40451e8673d536c1feda95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 3:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c464803a1a7049b9b000410c66a06777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 4:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94fdb3751932415b8a2bf6d278c943d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 5:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4342b3d3ffbd4f3fb3b582a572acc130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 6:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da692fe153a34ed2a83f97103a6208af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 7:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28c1e9a3fbd49268b0689e7bf1f9d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 8:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47dec78b43349f6b223baa644767119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current epoch 9:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "testing_scenarios = {\n",
    "    'heuristic__small_imgs': (DualBranchModel(), [(144,256)]*2)\n",
    "}\n",
    "\n",
    "for name, (model, img_size) in testing_scenarios.items():\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((144,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    df = pd.read_pickle(\"../data/pepper_data.pkl\")\n",
    "    df['image_path_env'] = df['image_path'].apply(lambda p: str(Path('../data/masked/environment') / Path(p).name))\n",
    "    df['image_path_social'] = df['image_path'].apply(lambda p: str(Path('../data/masked/social') / Path(p).name))\n",
    "    domain_dataloaders = get_dataloader(df, batch_sizes=(16, 64, 64), return_splits=False, double_img=True, transforms=[transform]*2, num_workers=0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    domains = ['Home', 'BigOffice-2', 'BigOffice-3', 'Hallway', 'MeetingRoom', 'SmallOffice']\n",
    "\n",
    "\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    dual_model = model.to(device)\n",
    "    # optimizer = torch.optim.Adam(dual_model.parameters(), lr=1e-3)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': dual_model.social_branch.parameters()},\n",
    "        {'params': dual_model.env_branch.parameters()},\n",
    "        {'params': dual_model.head.parameters()},\n",
    "    ], lr=1e-3)\n",
    "    classifier_optimizer = optim.Adam([ \n",
    "        {'params': dual_model.social_classifier.parameters()},\n",
    "        {'params': dual_model.env_classifier.parameters()}\n",
    "    ], lr=1e-3)\n",
    "\n",
    "    buffer = NaiveRehearsalBuffer(buffer_size=120)\n",
    "\n",
    "    exp_name = f\"{name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    dualbranch_kwargs = {\n",
    "            'mse_criterion': nn.MSELoss(),\n",
    "            'ce_criterion': nn.CrossEntropyLoss(),\n",
    "            'class_optimizer': classifier_optimizer\n",
    "        }\n",
    "    \n",
    "    unified_train_loop(\n",
    "        model=dual_model,\n",
    "        domains=domains,\n",
    "        domain_dataloaders=domain_dataloaders,\n",
    "        buffer=buffer,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        batch_fn=heuristic_dualbranch_batch,\n",
    "        batch_kwargs=dualbranch_kwargs,\n",
    "        num_epochs=10,\n",
    "        exp_name=exp_name,\n",
    "        gradient_clipping=True,\n",
    "        collect_tsne_data=False,\n",
    "        checkpoint_dir=\"../checkpoints\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
