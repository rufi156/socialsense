{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72ee38c",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2560ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "DATASET_DIR = (Path(\"..\") / \"datasets\").resolve()\n",
    "DATASETS = [\"OFFICE-MANNERSDB\", \"MANNERSDBPlus\"]\n",
    "LABEL_COLS = [\n",
    "    \"Vaccum Cleaning\", \"Mopping the Floor\", \"Carry Warm Food\",\n",
    "    \"Carry Cold Food\", \"Carry Drinks\", \"Carry Small Objects\",\n",
    "    \"Carry Large Objects\", \"Cleaning\", \"Starting a conversation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9da04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_path, dataset):\n",
    "    \"\"\"Process individual CSV files\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.drop(columns=df.columns[-1])\n",
    "    \n",
    "    # Extract metadata from first column\n",
    "    first_col = df.columns[0]\n",
    "    split_data = df[first_col].str.split('_', n=2, expand=True)\n",
    "    \n",
    "    df[\"robot\"] = split_data[0]\n",
    "    df[\"domain\"] = split_data[1]\n",
    "    df[\"image_ref\"] = split_data[2].astype(int)\n",
    "    df[\"dataset\"] = dataset\n",
    "\n",
    "    df = df.drop(columns=[first_col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def consolidate_data(datasets):\n",
    "    \"\"\"Aggregate all CSVs\"\"\"\n",
    "    all_dfs = []\n",
    "    for dataset in datasets:\n",
    "        source_path = DATASET_DIR / dataset\n",
    "        \n",
    "        for robot in [\"NAO\", \"Pepper\", \"PR2\"]:\n",
    "            ann_dir = source_path / robot / \"Annotations\"\n",
    "            if not ann_dir.exists():\n",
    "                raise ValueError(f\"Labels csv file path ({ann_dir}) doesn't exist\")\n",
    "                \n",
    "            \n",
    "            for csv_file in ann_dir.glob(\"*.csv\"):\n",
    "                try:\n",
    "                    df = process_csv(csv_file, dataset)\n",
    "                    all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {csv_file}: {str(e)}\")\n",
    "    \n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53657ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_raw_data(df):\n",
    "    \"\"\"Comprehensive data quality checks for raw annotation data\"\"\"\n",
    "    required_columns = {'robot', 'domain', 'image_ref', 'dataset'}\n",
    "\n",
    "    # Check for any missing columns\n",
    "    missing_cols = required_columns - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "\n",
    "    # Label value validation (should be between 1 and 5)\n",
    "    for col in LABEL_COLS:\n",
    "        if df[col].min() < 1 or df[col].max() > 5:\n",
    "            raise ValueError(f\"Label {col} has invalid range [{df[col].min()}, {df[col].max()}]\")\n",
    "\n",
    "    # Null values check\n",
    "    null_cols = df.columns[df.isnull().any()].tolist()\n",
    "    if null_cols:\n",
    "        raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
    "\n",
    "    # Data type and value validation for image_ref\n",
    "    if not pd.api.types.is_integer_dtype(df['image_ref']):\n",
    "        raise TypeError(\"image_ref must be integer type\")\n",
    "    if (df['image_ref'] < 0).any():\n",
    "        raise ValueError(\"image_ref contains negative values, which is invalid\")\n",
    "\n",
    "    # Categorical value validation\n",
    "    valid_robots = {'NAO', 'Pepper', 'PR2'}\n",
    "    invalid_robots = set(df['robot']) - valid_robots\n",
    "    if invalid_robots:\n",
    "        raise ValueError(f\"Invalid robot values: {invalid_robots}\")\n",
    "\n",
    "    valid_sources = {'OFFICE-MANNERSDB', 'MANNERSDBPlus'}\n",
    "    invalid_sources = set(df['dataset']) - valid_sources\n",
    "    if invalid_sources:\n",
    "        raise ValueError(f\"Invalid source directories: {invalid_sources}\")\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb45d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_labels(df):\n",
    "    \"\"\"Aggregate multiple annotations per image by image path\"\"\"    \n",
    "    agg_dict = {\n",
    "        **{col: 'mean' for col in LABEL_COLS},\n",
    "        **{col: 'first' for col in df.columns.difference(LABEL_COLS).tolist()},\n",
    "    }\n",
    "    \n",
    "    return df.groupby('image_path', as_index=False).agg(agg_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_image_path(row):\n",
    "    \"\"\"Robust path resolution with validation\"\"\"\n",
    "    base_dir = DATASET_DIR / row['dataset'] / row['robot'] / \"Images\"\n",
    "    \n",
    "    if row['dataset'] == \"OFFICE-MANNERSDB\":\n",
    "        target = base_dir / f\"{row['domain']}_{row['image_ref']}.png\"\n",
    "    else:\n",
    "        target = next(base_dir.glob(f\"{row['image_ref']}_*.png\"), None)\n",
    "    \n",
    "    if target and target.exists():\n",
    "        return target.resolve()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acfe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.item()\n",
    "            \n",
    "        img_path = str(self.df.at[idx, \"image_path\"])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading {img_path}: {str(e)}\")\n",
    "        \n",
    "        raw_labels = self.df.iloc[idx][LABEL_COLS].values.astype(np.float32)\n",
    "        scaled_labels = (raw_labels - 1) / 4  # Convert 1-5 → 0-1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.from_numpy(scaled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128)):\n",
    "    \"\"\"Create train/val/test dataloaders using image_path as unique key\"\"\"\n",
    "    \n",
    "    # Get image paths as indexing for split\n",
    "    unique_images = df[['image_path']].reset_index(drop=True)\n",
    "    \n",
    "    # Split using image_path as key #TODO is is spliting based on index or path strings? index faster\n",
    "    train_paths, temp_paths = train_test_split(\n",
    "        unique_images['image_path'], \n",
    "        test_size=0.3, \n",
    "        random_state=42\n",
    "    )\n",
    "    val_paths, test_paths = train_test_split(\n",
    "        temp_paths,\n",
    "        test_size=0.5, \n",
    "        random_state=42\n",
    "    )\n",
    "   \n",
    "    # Create subsets\n",
    "    train_df = df[df['image_path'].isin(train_paths)].reset_index(drop=True)\n",
    "    val_df = df[df['image_path'].isin(val_paths)].reset_index(drop=True)\n",
    "    test_df = df[df['image_path'].isin(test_paths)].reset_index(drop=True)\n",
    "    \n",
    "    #TODO add coordinate values for spacialy aware CNN Uber's CoordNav\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resize_img_to),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ImageLabelDataset(train_df, transform)\n",
    "    val_dataset = ImageLabelDataset(val_df, transform)\n",
    "    test_dataset = ImageLabelDataset(test_df, transform)\n",
    "    \n",
    "    # Create loaders\n",
    "    num_workers = 0\n",
    "    loaders = {\n",
    "        'train': DataLoader(train_dataset, batch_size=batch_sizes[0], shuffle=True, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n",
    "        'val': DataLoader(val_dataset, batch_size=batch_sizes[1], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available()),\n",
    "        'test': DataLoader(test_dataset, batch_size=batch_sizes[2], shuffle=False, num_workers=num_workers, pin_memory=torch.cuda.is_available())\n",
    "    }\n",
    "    \n",
    "    return loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9668e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_final_data(df):\n",
    "    \"\"\"Final validation after aggregation\"\"\"\n",
    "    # Missing image paths\n",
    "    missing = df[df['image_path'].isnull()]\n",
    "    if not missing.empty:\n",
    "        raise FileNotFoundError(\n",
    "            f\"{len(missing)} images missing after aggregation. Examples:\\n\"\n",
    "            f\"{missing[['robot', 'domain', 'image_ref']].head()}\"\n",
    "        )\n",
    "    \n",
    "    # Null values check\n",
    "    null_cols = df.columns[df.isnull().any()].tolist()\n",
    "    if null_cols:\n",
    "        raise ValueError(f\"Null values found in columns: {null_cols}\")\n",
    "\n",
    "    # Duplicate image paths\n",
    "    duplicates = df[df.duplicated('image_path', keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"Duplicate image paths after aggregation:\\n\"\n",
    "            f\"{duplicates['image_path'].unique()}\"\n",
    "        )\n",
    "\n",
    "    # Label validity (1-5)\n",
    "    for col in LABEL_COLS:\n",
    "        if df[col].min() < 1 or df[col].max() > 5:\n",
    "            raise ValueError(\n",
    "                f\"Aggregated label {col} out of range: \"\n",
    "                f\"[{df[col].min()}, {df[col].max()}]\"\n",
    "            )\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd34a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2a2a2b",
   "metadata": {},
   "source": [
    "## --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f66130",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_df = consolidate_data(DATASETS)\n",
    "    validate_raw_data(raw_df)\n",
    "    raw_df['image_path'] = raw_df.apply(resolve_image_path, axis=1)   \n",
    "    aggregated_df = aggregate_labels(raw_df)\n",
    "    validate_final_data(aggregated_df) \n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "aggregated_df.to_pickle(\"processed_all_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27a27f",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raw_df = consolidate_data(DATASETS)\n",
    "    validate_raw_data(raw_df)\n",
    "except Exception as e:\n",
    "    print(f\"Pipeline failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "entries_per_image = raw_df.groupby(['robot', 'domain', 'image_ref']).size().reset_index(name='num_entries')\n",
    "domain_counts = entries_per_image.groupby(['domain', 'num_entries']).size().reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = sns.barplot(\n",
    "    data=domain_counts,\n",
    "    x='num_entries',\n",
    "    y='count',\n",
    "    hue='domain',\n",
    "    hue_order = ['Home', 'BigOffice-2', 'BigOffice-3', 'Hallway', 'MeetingRoom',\n",
    "       'SmallOffice'],\n",
    "    palette='viridis'\n",
    ")\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2.,\n",
    "            height + 0.5,  # Slightly above the bar\n",
    "            int(height),\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=11\n",
    "        )\n",
    "ax.set_ylim(0, domain_counts['count'].sum())\n",
    "\n",
    "plt.xlabel('Number of Entries per Image')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Distribution of Number of Entries per Image by Domain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28016faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "duplicated_rows = raw_df.duplicated(keep=False)\n",
    "duplicates_per_index = duplicated_rows.groupby([raw_df['robot'], raw_df['domain'], raw_df['image_ref']]).sum()\n",
    "duplicates_per_index = duplicates_per_index.apply(lambda x: max(x - 1, 0))\n",
    "\n",
    "duplicates_per_index = duplicates_per_index.reset_index(name='num_duplicates')\n",
    "domain_counts = duplicates_per_index.groupby(['domain', 'num_duplicates']).size().reset_index(name='count')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(\n",
    "    data=domain_counts,\n",
    "    x='num_duplicates',\n",
    "    y='count',\n",
    "    hue='domain',\n",
    "    hue_order = ['Home', 'BigOffice-2', 'BigOffice-3', 'Hallway', 'MeetingRoom',\n",
    "       'SmallOffice'],\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    if height > 0:\n",
    "        ax.text(\n",
    "            p.get_x() + p.get_width() / 2.,\n",
    "            height + 0.5,\n",
    "            int(height),\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=11\n",
    "        )\n",
    "\n",
    "# Set y-axis max to total number of duplicates\n",
    "ax.set_ylim(0, domain_counts['count'].sum())\n",
    "\n",
    "plt.xlabel('Number of Duplicates per Image')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Distribution of Number of Duplicates per Image by Domain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90680d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43ca986",
   "metadata": {},
   "source": [
    "## CL Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset, ConcatDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialContinualModel(nn.Module):\n",
    "    def __init__(self, num_tasks=9):\n",
    "        super().__init__()\n",
    "        # Initialize the ResNet50 architecture with Places365 configuration\n",
    "        self.backbone = models.resnet50(num_classes=365)\n",
    "        \n",
    "        # get Places365 weights and fix their naming leftover from troch saving convention\n",
    "        places365_weights = torch.load('resnet50_places365.pth.tar', weights_only=True)\n",
    "        state_dict = places365_weights['state_dict']\n",
    "        state_dict = {k.replace('module.', ''): v \n",
    "                     for k, v in state_dict.items()}\n",
    "        \n",
    "        # Load weights\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "        # Remove classification head\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        #Freeze all params except last layer\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if 'layer4' not in name:\n",
    "                param.requires_grad_(False)\n",
    "        \n",
    "        #TODO human mask average, std, quadrants, human in realtion to robot std\n",
    "        \n",
    "\n",
    "\n",
    "        # Shared layers #TODO deeper shared space?\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Task-specific heads with more expensive but finegrained GELU\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(512, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_tasks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        shared = self.shared_fc(features)\n",
    "        outputs = [head(shared) for head in self.heads]\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971efe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class LGRBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    @misc{churamani_feature_2024,\n",
    "\t\ttitle = {Feature Aggregation with Latent Generative Replay for Federated Continual Learning of Socially Appropriate Robot Behaviours},\n",
    "\t\turl = {http://arxiv.org/abs/2405.15773},\n",
    "\t\tdoi = {10.48550/arXiv.2405.15773},\n",
    "\t\tnumber = {{arXiv}:2405.15773},\n",
    "\t\tpublisher = {{arXiv}},\n",
    "\t\tauthor = {Churamani, Nikhil and Checker, Saksham and Chiang, Hao-Tien Lewis and Gunes, Hatice},\n",
    "\t\turldate = {2025-01-30},\n",
    "\t\tdate = {2024-03-16},\n",
    "\t}\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(LGRBaseline, self).__init__()\n",
    "        \n",
    "        # MobileNetV2 backbone\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        # Backbone feature processing\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Regression head\n",
    "        self.fc1_bn = nn.BatchNorm1d(1280)\n",
    "        self.fc2 = nn.Linear(1280, 32)\n",
    "        # self.fc2_bn = nn.BatchNorm1d(128)\n",
    "\t\t# self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        # Spatial reduction\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Regression\n",
    "        x = self.fc1_bn(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.fc2_bn(x)\n",
    "\t\t# x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class ReservoirBuffer:\n",
    "    def __init__(self, capacity=1000, replay_ratio=0.2, input_shape=(3, 384, 216), label_shape=(9,), device=torch.device('cpu')):\n",
    "        self.capacity = capacity\n",
    "        self.inputs = torch.empty((capacity, *input_shape), dtype=torch.float32, device=device)\n",
    "        self.labels = torch.empty((capacity, *label_shape), dtype=torch.float32, device=device)\n",
    "        self.domains = [None] * capacity\n",
    "        self.size = 0          # Number of samples currently in buffer\n",
    "        self.num_seen = 0      # Total samples seen\n",
    "        self.replay_ratio = replay_ratio\n",
    "\n",
    "    def add(self, new_samples):\n",
    "        for sample in new_samples:\n",
    "            self.num_seen += 1\n",
    "            if self.size < self.capacity:\n",
    "                idx = self.size\n",
    "                self.size += 1\n",
    "            else:\n",
    "                idx = random.randint(0, self.num_seen - 1)\n",
    "                if idx >= self.capacity:\n",
    "                    continue\n",
    "            self.inputs[idx].copy_(sample[0])\n",
    "            self.labels[idx].copy_(sample[1])\n",
    "            self.domains[idx] = sample[2]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.size == 0:\n",
    "            return []\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return [(self.inputs[i], self.labels[i], self.domains[i]) for i in indices]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def get_domain_distribution(self):\n",
    "        return pd.Series(self.domains[:self.size]).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, tag):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.tag = tag  # 0=current, 1=replay\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base_dataset[idx]\n",
    "        return x, y, self.tag\n",
    "\n",
    "class NaiveRehearsalBuffer:\n",
    "    \"\"\"\n",
    "    @inproceedings{Hsu18_EvalCL,\n",
    "        title={Re-evaluating Continual Learning Scenarios: A Categorization and Case for Strong Baselines},\n",
    "        author={Yen-Chang Hsu and Yen-Cheng Liu and Anita Ramasamy and Zsolt Kira},\n",
    "        booktitle={NeurIPS Continual learning Workshop },\n",
    "        year={2018},\n",
    "        url={https://arxiv.org/abs/1810.12488}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size=1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.domain_buffer = {}\n",
    "\n",
    "    def update_buffer(self, domain, dataset):\n",
    "        # Add/overwrite current domain\n",
    "        self.domain_buffer[domain] = Subset(dataset, torch.arange(len(dataset)))\n",
    "        \n",
    "        # Recalculate quota\n",
    "        num_domains = len(self.domain_buffer)\n",
    "        buffer_quota_per_domain = self.buffer_size // num_domains\n",
    "        \n",
    "        # Reduce all domains (including current)\n",
    "        for domain in self.domain_buffer:\n",
    "            subset = self.domain_buffer[domain]\n",
    "            max_safe_samples_to_overwrite = min(buffer_quota_per_domain, len(subset.dataset))\n",
    "            rand_indices = torch.randperm(len(dataset))[:max_safe_samples_to_overwrite].numpy()\n",
    "            self.domain_buffer[domain] = Subset(dataset, rand_indices)\n",
    "\n",
    "    def get_loader_with_replay(self, current_domain, current_loader):\n",
    "        current_dataset = TaggedDataset(current_loader.dataset, tag=0)\n",
    "        replay_datasets = [TaggedDataset(dataset, tag=1) for domain, dataset in self.domain_buffer.items() if domain != current_domain]\n",
    "\n",
    "        #Enforces 1:1 ratio when current ≥ buffer\n",
    "        total_replay = sum(len(dataset) for dataset in replay_datasets)\n",
    "        if total_replay > 0:\n",
    "            K = max(len(current_dataset) // total_replay, 1)\n",
    "            replay_datasets = replay_datasets * K\n",
    "\n",
    "        combined_dataset =  ConcatDataset(replay_datasets + [current_dataset])\n",
    "        combined_dataset = DataLoader(\n",
    "            combined_dataset,\n",
    "            batch_size=current_loader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=current_loader.num_workers,\n",
    "            pin_memory=current_loader.pin_memory,\n",
    "            drop_last=current_loader.drop_last\n",
    "        )\n",
    "        return combined_dataset\n",
    "    \n",
    "    def get_domain_distribution(self):\n",
    "        \"\"\"Returns {domain: num_samples} without needing Storage\"\"\"\n",
    "        return {domain: len(subset) for domain, subset in self.domain_buffer.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)  # Scale by batch size\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    average_loss_per_sample = total_loss / total_samples\n",
    "\n",
    "    return average_loss_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871f944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a453cfb",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"processed_all_data.pkl\")\n",
    "\n",
    "# Create domain-specific dataloaders\n",
    "domains = df['domain'].unique()\n",
    "domain_dataloaders = {}\n",
    "for domain in domains:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    #domain_df = domain_df.sample(frac=0.5, random_state=42)\n",
    "    loaders = create_dataloaders(domain_df, batch_sizes=(32, 64, 64), resize_img_to=(128, 128))  #TODO should be (384, 216) to retain scale or (224, 224) for best performance on MobileNet\n",
    "    domain_dataloaders[domain] = loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f1f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LGRBaseline().to(device)\n",
    "#buffer = ReservoirBuffer(capacity=1000, replay_ration=0.2, input_shape=(3, 384, 216), label_shape=(9,), device=device)\n",
    "buffer = NaiveRehearsalBuffer(buffer_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8caf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#TODO separate optimisers for shared and heads?\n",
    "criterion = nn.MSELoss()\n",
    "#TODO potential loss optimisations like unccertainty loss, \n",
    "\n",
    "num_epochs=5\n",
    "\n",
    "metrics = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'test_loss': [],\n",
    "    'domain_performance': {domain: [] for domain in domains}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcdfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='runs/naive_rehearsal_buffer')  # You can change the log_dir name\n",
    "\n",
    "\n",
    "\n",
    "for domain_idx, domain in enumerate(domains):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n=== Training Domain {domain_idx+1}/{len(domains)}: {domain} ===\")\n",
    "\n",
    "    original_train_loader = domain_dataloaders[domain]['train']\n",
    "    original_train_dataset = original_train_loader.dataset\n",
    "    \n",
    "    train_loader = buffer.get_loader_with_replay(domain, original_train_loader)\n",
    "    \n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    print(f\"Training on {str(device).upper()} | Batch Size: {train_loader.batch_size}\")\n",
    "    print(f\"Total Batches per Epoch: {total_batches}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels, _) in enumerate(train_loader):\n",
    "            batch_start = time.time()    \n",
    "\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "\n",
    "            # ReservoirBuffer # Sample the buffer and add replay samples to training\n",
    "            # if buffer and random.random() < buffer.replay_ratio:\n",
    "            #     batch_size = inputs.size(0)\n",
    "            #     replay_batch = buffer.sample(int(batch_size * 0.25))\n",
    "            #     if replay_batch:\n",
    "            #         replay_inputs, replay_labels, _ = zip(*replay_batch)\n",
    "            #         replay_inputs = torch.stack(replay_inputs)\n",
    "            #         replay_labels = torch.stack(replay_labels)\n",
    "            #         inputs = torch.cat([inputs, replay_inputs])\n",
    "            #         labels = torch.cat([labels, replay_labels])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ReservoirBuffer # Add current batch to buffer\n",
    "            # with torch.no_grad():\n",
    "            #     samples = [(img.detach(), label.detach(), domain) for img, label in zip(inputs, labels)]\n",
    "            #     buffer.add(samples)\n",
    "\n",
    "            \n",
    "            batch_time = time.time() - batch_start\n",
    "                \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            total_train_samples += inputs.size(0)\n",
    "\n",
    "            if batch_idx % 10 == 9:               \n",
    "                avg_loss = running_loss / 10\n",
    "                batch_time = time.time() - batch_start  # ← Keep this\n",
    "                print(\n",
    "                    f\"Domain: {domain} | Epoch: {epoch+1} | \"\n",
    "                    f\"Batch: {batch_idx+1}/{total_batches} | \"\n",
    "                    f\"Avg Loss: {avg_loss:.4f} | \"  # Changed to Avg Loss\n",
    "                    f\"Time/batch: {batch_time:.2f}s\"\n",
    "                )\n",
    "                running_loss = 0.0\n",
    "    \n",
    "        # Epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_train_loss = running_loss / total_train_samples\n",
    "        metrics['train_loss'].append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1} Time: {epoch_time//60:.0f}m {epoch_time%60:.0f}s\")     \n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "       \n",
    "        # Validation\n",
    "        val_loss = evaluate_model(model, domain_dataloaders[domain]['val'], criterion, device)\n",
    "        metrics['val_loss'].append(val_loss)\n",
    "        print(f\"Domain {domain} | Epoch {epoch+1} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch) \n",
    "    \n",
    "    buffer.update_buffer(domain, original_train_dataset)\n",
    "\n",
    "    print(\"\\nBuffer after domain:\", domain)\n",
    "    dist = buffer.get_domain_distribution()\n",
    "    for d, counts in dist.items():\n",
    "        print(f\"- {d}: {sum(counts.values()) if isinstance(counts, dict) else counts} samples\")\n",
    "    \n",
    "    # Evaluate on all domains\n",
    "    for eval_domain in domains:\n",
    "        domain_loss = evaluate_model(model, domain_dataloaders[eval_domain]['val'], criterion, device)\n",
    "        metrics['domain_performance'][eval_domain].append(domain_loss)\n",
    "        print(f\"Performance on {eval_domain} after {domain}: {domain_loss:.4f}\")\n",
    "        writer.add_scalar(f'Loss/val_{eval_domain}', domain_loss, epoch)\n",
    "    \n",
    "\n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'metrics': metrics,\n",
    "    'epoch': epoch + 1,\n",
    "    'domain': domain\n",
    "}\n",
    "torch.save(checkpoint, 'base_model_naiverehearsal.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011d46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6678f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(metrics['val_loss'])\n",
    "plt.plot(metrics['train_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Over Epochs')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, losses in metrics['domain_performance'].items():\n",
    "    plt.plot(losses, label=domain)\n",
    "plt.xlabel('Domain Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Domain Performance Over Training')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for domain, losses in metrics['domain_performance'].items():\n",
    "    print(f\"{domain}: Initial = {losses[0]:.4f}, Final = {losses[-1]:.4f}, Change = {losses[-1] - losses[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895de6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model output\n",
    "outputs = model(inputs)  # Should be in [1,5]\n",
    "print(f\"Output range: {outputs.min().item()}–{outputs.max().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ced18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfit Single Batch Check\n",
    "\n",
    "test_model = LGRBaseline().to(device)\n",
    "test_optimizer = optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "buffer = NaiveRehearsalBuffer(0)\n",
    "\n",
    "first_domain = domains[0]\n",
    "train_loader = domain_dataloaders[first_domain]['train']\n",
    "single_batch = next(iter(train_loader))\n",
    "inputs, labels = single_batch\n",
    "inputs = inputs.to(device, dtype=torch.float32)\n",
    "labels = labels.to(device, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367720f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_epochs = 100\n",
    "for epoch in range(num_test_epochs):\n",
    "    test_optimizer.zero_grad()\n",
    "\n",
    "    outputs = test_model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    test_optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Overfit Epoch {epoch+1}/{num_test_epochs} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8715d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9743dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
