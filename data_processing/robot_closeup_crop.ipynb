{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fdb3de",
   "metadata": {},
   "source": [
    "### Dataset analysis for zoomin method\n",
    "find robot in image, zoomin on the robot: image 1-robot close up, image 2- full image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08521110",
   "metadata": {},
   "source": [
    "#### Get robot bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bz2\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('../data/autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    suffix=\"home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e26a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899eff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('../data/autodistill_dataset_office.pbz2', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    suffix=\"office\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019f0532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_robots(detections_dataset, confidence_threshold=0.3):\n",
    "    robot_counts_per_image = []\n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/images\") / Path(item[0]).name\n",
    "        detections = item[-1]\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        detected_classes = detections.class_id[keep_indices]\n",
    "\n",
    "        robot_count = np.count_nonzero(detected_classes == 1)\n",
    "        robot_counts_per_image.append(robot_count)  # Aggregate per image\n",
    "    return robot_counts_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_counts_per_image = count_robots(dataset)\n",
    "unique_counts, counts = np.unique(robot_counts_per_image, return_counts=True)\n",
    "for count, num_images in zip(unique_counts, counts):\n",
    "    print(f\"{count}: {num_images}\")\n",
    "\n",
    "#Home\n",
    "# 0: 9\n",
    "# 1: 388\n",
    "# 2: 386\n",
    "# 3: 159\n",
    "# 4: 49\n",
    "# 5: 8\n",
    "# 6: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_counts_per_image = count_robots(dataset)\n",
    "unique_counts, counts = np.unique(robot_counts_per_image, return_counts=True)\n",
    "for count, num_images in zip(unique_counts, counts):\n",
    "    print(f\"{count}: {num_images}\")\n",
    "\n",
    "#Office\n",
    "# 0: 24\n",
    "# 1: 805\n",
    "# 2: 163\n",
    "# 3: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d90ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_labels = np.repeat(np.arange(5), 200)\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Initialize a dictionary to hold counts per category\n",
    "category_robot_counts = defaultdict(list)\n",
    "\n",
    "for item, label in zip(dataset_office, domain_labels):\n",
    "    detections = item[1]\n",
    "    keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= 0.3]\n",
    "    detected_classes = detections.class_id[keep_indices]\n",
    "    robot_count = np.count_nonzero(detected_classes == 1)\n",
    "    \n",
    "    # Append count to the relevant category list\n",
    "    category_robot_counts[label].append(robot_count)\n",
    "\n",
    "# Now compute value counts per category\n",
    "for category, counts in category_robot_counts.items():\n",
    "    counter = Counter(counts)\n",
    "    print(f\"\\nCategory {category}:\")\n",
    "    for count, num_images in sorted(counter.items()):\n",
    "        print(f\"Robots: {count} | Images: {num_images}\")\n",
    "\n",
    "# Per domain\n",
    "# Category 0:\n",
    "# Robots: 0 | Images: 4\n",
    "# Robots: 1 | Images: 172\n",
    "# Robots: 2 | Images: 23\n",
    "# Robots: 3 | Images: 1\n",
    "\n",
    "# Category 1:\n",
    "# Robots: 0 | Images: 2\n",
    "# Robots: 1 | Images: 177\n",
    "# Robots: 2 | Images: 19\n",
    "# Robots: 3 | Images: 2\n",
    "\n",
    "# Category 2:\n",
    "# Robots: 0 | Images: 5\n",
    "# Robots: 1 | Images: 174\n",
    "# Robots: 2 | Images: 20\n",
    "# Robots: 3 | Images: 1\n",
    "\n",
    "# Category 3:\n",
    "# Robots: 0 | Images: 13\n",
    "# Robots: 1 | Images: 167\n",
    "# Robots: 2 | Images: 20\n",
    "\n",
    "# Category 4:\n",
    "# Robots: 1 | Images: 115\n",
    "# Robots: 2 | Images: 81\n",
    "# Robots: 3 | Images: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329785f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robot_locations(detections_dataset, confidence_threshold=0.3):\n",
    "    robot_bboxes = []\n",
    "\n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/images\") / Path(item[0]).name\n",
    "        detections = item[-1]\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        detected_classes = detections.class_id[keep_indices]\n",
    "\n",
    "        # Save bounding boxes of detected robots (class_id == 1)\n",
    "        robot_indices = [i for i, c in zip(keep_indices, detected_classes) if c == 1]\n",
    "        if len(robot_indices) == 1:\n",
    "            i = robot_indices[0]\n",
    "            bbox = detections.xyxy[i]  # bbox: [x1, y1, x2, y2]\n",
    "            robot_bboxes.append({\n",
    "                'image_path': image_path.as_posix(),\n",
    "                'bbox': bbox\n",
    "            })\n",
    "    return robot_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d42fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_bboxes = robot_locations(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_bboxes = [{'image_path': i['image_path'], 'bbox': i['bbox'].tolist()} for i in robot_bboxes]\n",
    "with open(f\"robot_bboxes_office.json\", \"w\") as f:\n",
    "    json.dump(robot_bboxes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526eeb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"robot_bboxes_office.json\", \"r\") as f:\n",
    "    robot_bboxes = json.load(f)\n",
    "print(len(robot_bboxes), \"\\n\", robot_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda94d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"robot_bboxes_home.json\", \"r\") as f:\n",
    "    robot_bboxes = json.load(f)\n",
    "print(len(robot_bboxes), \"\\n\", robot_bboxes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14bc602",
   "metadata": {},
   "source": [
    "#### Prepare cropped robot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0739e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"robot_bboxes_home.json\", \"r\") as f:\n",
    "    robot_bboxes = json.load(f)\n",
    "with open(\"robot_bboxes_office.json\", \"r\") as f:\n",
    "    robot_bboxes += json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(robot_bboxes), robot_bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d91107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude images that incorrectly labeled the robot - find them manually\n",
    "exclude = [\n",
    "    \"Hallway_205\", \n",
    "    \"Hallway_273\", \n",
    "    \"Hallway_38\",\n",
    "    \"38_1_0_4_3_0.7022904_0_1_1_2.686175_0.7022905_0.7022906_0.7022906_220_340_100_0_0_1_0_1_1_1_0_50_0_0_5_Pepper_TV\",\n",
    "    \"4_1_0_6_4_0.5346667_0_1_1_2.360057_0.5346668_0.5346668_0.5346668_300_30_120_0_0_1_0_1_1_1_0_50_0_0_7_Pepper_TV\",\n",
    "    \"576_1_0_3_5_0.6744272_0_1_1_1.406374_0.674427_0.674427_0.6744272_150_222_6_0_0_0_1_1_1_0_0_50_0_1_7_Pepper_TV\",\n",
    "    \"SmallOffice_236\",\n",
    "    \"62_1_0_4_3_0.59108_0_1_1_1.457013_0.59108_0.59108_0.59108_354_114_234_0_1_0_0_1_1_1_0_50_1_0_6_Pepper_TV\",\n",
    "    \"440_1_0_5_3_0.7460655_0_1_1_2.220734_0.7460653_0.7460654_0.7460655_158_38.00001_278_0_0_1_0_1_0_0_1_1.334754_0_0_6_Pepper_TV\",\n",
    "    \"Hallway_44\",\n",
    "    \"Hallway_76\",\n",
    "    \"MeetingRoom_47\",\n",
    "    \"MeetingRoom_69\",\n",
    "    ]\n",
    "robot_bboxes = [\n",
    "    e for e in robot_bboxes\n",
    "    if not any(k in e[\"image_path\"] for k in exclude)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect faulty bounding boxes manually\n",
    "\n",
    "# output_dir = Path(\"./bbox_inspection\")\n",
    "# output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# target_size = (720, 405)  # Resize for inspection\n",
    "\n",
    "# for entry in tqdm(robot_bboxes):\n",
    "#     img_path = entry['image_path']\n",
    "#     bbox = entry['bbox']  # bbox is a list [x1, y1, x2, y2]\n",
    "#     x1, y1, x2, y2 = map(int, bbox)\n",
    "\n",
    "#     # Open image\n",
    "#     img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "#     # Draw green bounding box\n",
    "#     draw = ImageDraw.Draw(img)\n",
    "#     draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=3)\n",
    "\n",
    "#     # Resize image\n",
    "#     img_resized = img.resize(target_size, Image.BILINEAR)\n",
    "\n",
    "#     # Save image\n",
    "#     save_path = output_dir / Path(img_path).name\n",
    "#     img_resized.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e63b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_crop_window(robot_bboxes):\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    max_bbox_img=''\n",
    "\n",
    "    for entry in robot_bboxes:\n",
    "        bbox_img = entry['image_path']\n",
    "        bbox = entry['bbox']\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "\n",
    "        if w > max_width:\n",
    "            max_width = w\n",
    "            max_bbox_img = bbox_img\n",
    "        if h > max_height:\n",
    "            max_height = h\n",
    "            max_bbox_img = bbox_img\n",
    "    return (round(max_width), round(max_height)), max_bbox_img\n",
    "\n",
    "\n",
    "# Find 16:9 crop window enclosing the biggest bbox without cropping it (expand bbox to 16:9)\n",
    "def expand_bbox_to_aspect(bbox_w, bbox_h, target_aspect=16/9):\n",
    "    # Start from bbox size\n",
    "    w, h = bbox_w, bbox_h\n",
    "    current_aspect = w / h\n",
    "\n",
    "    if current_aspect > target_aspect:\n",
    "        # Too wide: increase height\n",
    "        new_h = w / target_aspect\n",
    "        new_w = w\n",
    "    else:\n",
    "        # Too tall: increase width\n",
    "        new_w = h * target_aspect\n",
    "        new_h = h\n",
    "    return int(np.ceil(new_w)), int(np.ceil(new_h))\n",
    "\n",
    "max_size, img_path = get_max_crop_window(robot_bboxes)\n",
    "print(f\"Biggest bbox: {max_size} {img_path}\")\n",
    "max_crop_width, max_crop_height = expand_bbox_to_aspect(*max_size)\n",
    "print(f\"Absolute crop window size (closest 16:9 enclosing biggest bbox): {max_crop_width}x{max_crop_height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_window(image_width, image_height, bbox, absolute_crop_size=None):\n",
    "    \"\"\"\n",
    "    Returns a crop window (x1, y1, x2, y2).\n",
    "    - If absolute_crop_size is given (width, height), center crop window of that size on bbox center, clamped inside image.\n",
    "    - Otherwise, calculate relative crop window where bbox width/height is 50% of crop window.\n",
    "\n",
    "    bbox = [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    bbox_w, bbox_h = x2 - x1, y2 - y1\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    if absolute_crop_size is not None:\n",
    "        crop_w, crop_h = absolute_crop_size\n",
    "    else:\n",
    "        crop_w = crop_h = max(bbox_w, bbox_h) * 2 \n",
    "\n",
    "    # Crop box coords\n",
    "    left = int(round(cx - crop_w / 2))\n",
    "    upper = int(round(cy - crop_h / 2))\n",
    "    right = left + crop_w\n",
    "    lower = upper + crop_h\n",
    "\n",
    "    # Clamp inside image bounds\n",
    "    if left < 0:\n",
    "        right -= left  # Move right boundary\n",
    "        left = 0\n",
    "    if upper < 0:\n",
    "        lower -= upper\n",
    "        upper = 0\n",
    "    if right > image_width:\n",
    "        left -= (right - image_width)\n",
    "        right = image_width\n",
    "        if left < 0:\n",
    "            left = 0\n",
    "    if lower > image_height:\n",
    "        upper -= (lower - image_height)\n",
    "        lower = image_height\n",
    "        if upper < 0:\n",
    "            upper = 0\n",
    "\n",
    "    return left, upper, right, lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (128,128) #(224,224)\n",
    "\n",
    "absolute_crop_size_side = max(*get_max_crop_window(robot_bboxes)[0])\n",
    "absolute_crop_size = (absolute_crop_size_side, absolute_crop_size_side)\n",
    "\n",
    "output_dir = Path(\"../data/resized_images\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for entry in tqdm(robot_bboxes):\n",
    "    img_path = entry['image_path']\n",
    "    bbox = entry['bbox']\n",
    "\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    iw, ih = img.size\n",
    "\n",
    "    # Choose crop window mode: absolute or relative\n",
    "    # crop_box = get_crop_window(iw, ih, bbox, absolute_crop_size=absolute_crop_size)  # absolute\n",
    "    crop_box = get_crop_window(iw, ih, bbox, absolute_crop_size=None)  # relative with bbox half crop\n",
    "\n",
    "    cropped_img = img.crop(crop_box)\n",
    "    resized_img = cropped_img.resize(target_size, Image.BILINEAR)\n",
    "    # resized_img = img.resize((256,144), Image.BILINEAR)\n",
    "\n",
    "    output_path = output_dir / Path(img_path).name\n",
    "    resized_img.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb247dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
