{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f4ae1",
   "metadata": {},
   "source": [
    "### ForgettingGatedSplitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ultralytics import YOLO\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Tuple, Optional\n",
    "from collections import deque\n",
    "\n",
    "sys.path.append('D:/projects/Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67631ca",
   "metadata": {},
   "source": [
    "#### backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLO11FeatureExtractor(nn.Module):\n",
    "    \"\"\"Extract features from YOLO11-seg backbone (layers 0-8)\"\"\"\n",
    "    def __init__(self, model_path='../models/yolo11n-seg.pt'):\n",
    "        super().__init__()\n",
    "        yolo = YOLO(model_path)\n",
    "\n",
    "        # Extract layers 0-8 as discussed (before SPPF to preserve spatial granularity)\n",
    "        self.backbone = nn.Sequential(*list(yolo.model.model[:9]))\n",
    "        del yolo\n",
    "        \n",
    "        # Freeze parameters\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)  # Output: [B, 256, H/32, W/32]\n",
    "\n",
    "class DepthAnythingFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_path='../models/depth_anything_v2_vits.pth'):\n",
    "        super().__init__()\n",
    "        model_configs = {\n",
    "            'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "        }\n",
    "        model = DepthAnythingV2(**model_configs['vits'])\n",
    "        model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "        \n",
    "        self.backbone = model.pretrained\n",
    "        del model\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Patch embedding\n",
    "            x = self.backbone.patch_embed(x)\n",
    "            \n",
    "            # Pass through transformer blocks\n",
    "            for block in self.backbone.blocks:\n",
    "                x = block(x)\n",
    "            \n",
    "            # Apply final norm\n",
    "            x = self.backbone.norm(x)\n",
    "            \n",
    "            # Remove CLS token and reshape to spatial format\n",
    "            if x.shape[1] > (H//14) * (W//14):\n",
    "                patch_tokens = x[:, 1:, :]  # Remove CLS token\n",
    "            else:\n",
    "                patch_tokens = x\n",
    "            \n",
    "            # Reshape to spatial format\n",
    "            patch_h, patch_w = H // 14, W // 14\n",
    "            spatial_features = patch_tokens.transpose(1, 2).reshape(B, 384, patch_h, patch_w)\n",
    "            \n",
    "            return spatial_features\n",
    "\n",
    "\n",
    "class DualBackboneFeatureExtractor(nn.Module):\n",
    "    \"\"\"Combines YOLO11 and DepthAnything feature extractors\"\"\"\n",
    "    def __init__(self, model_paths):\n",
    "        super().__init__()\n",
    "        # Individual feature extractors\n",
    "        self.yolo_extractor = YOLO11FeatureExtractor(model_paths['YOLO11n_seg'])\n",
    "        self.depth_extractor = DepthAnythingFeatureExtractor(model_paths['DepthAnythingV2_small'])\n",
    "        \n",
    "        # Feature projection layers (optional - can keep features as-is)\n",
    "        self.yolo_proj = nn.Conv2d(256, 256, 1)\n",
    "        self.depth_proj = nn.Conv2d(384, 384, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features from both backbones\n",
    "        yolo_features = self.yolo_extractor(x[0])  # [B, 256, H/32, W/32]\n",
    "        depth_features = self.depth_extractor(x[1])  # [B, 384, H/14, W/14]\n",
    "        \n",
    "        # Apply projections\n",
    "        yolo_features = self.yolo_proj(yolo_features)\n",
    "        depth_features = self.depth_proj(depth_features)\n",
    "        \n",
    "        # Downsample the depthanything features to match yolo\n",
    "        depth_features = F.adaptive_avg_pool2d(depth_features, (7, 7)) #image size /32 224,224 > 7,7\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([yolo_features, depth_features], dim=1)  # [B, 640, H/14, W/14] \n",
    "        \n",
    "        return combined_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35f86c",
   "metadata": {},
   "source": [
    "#### model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatastrophicForgettingAdapter(nn.Module):\n",
    "    \"\"\"Standalone adapter that measures forgetting and gates spatial features\"\"\"\n",
    "    def __init__(self, num_channels=640, num_outputs=9):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # Forgetting measurement storage (per channel)\n",
    "        self.previous_adapter_state = None\n",
    "        self.current_forgetting_scores = None\n",
    "        \n",
    "        # Adapter that predicts main task (for forgetting measurement)\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global average pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_channels, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_outputs)  # Predicts main task\n",
    "        )\n",
    "        \n",
    "        # Learnable gating network (operates on channel-wise forgetting scores)\n",
    "        self.gating = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // 4, num_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, backbone_features, use_gating=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            backbone_features: [B, 640, 36, 36] spatial features\n",
    "        Returns:\n",
    "            forgetting_features: [B, 640, 36, 36] features routed to forgetting branch\n",
    "            not_forgetting_features: [B, 640, 36, 36] features routed to stable branch\n",
    "            adapter_predictions: [B, 9] task predictions for forgetting measurement\n",
    "        \"\"\"\n",
    "        B, C, H, W = backbone_features.shape\n",
    "        \n",
    "        # Adapter makes predictions (for forgetting measurement)\n",
    "        adapter_predictions = self.adapter(backbone_features)\n",
    "        \n",
    "        if use_gating and self.current_forgetting_scores is not None:\n",
    "            # Apply channel-wise gating based on forgetting scores\n",
    "            forgetting_scores_batch = self.current_forgetting_scores.unsqueeze(0).expand(B, -1)  # [B, 640]\n",
    "            gating_mask = self.gating(forgetting_scores_batch)  # [B, 640]\n",
    "            \n",
    "            # Reshape for spatial broadcasting\n",
    "            gating_mask = gating_mask.unsqueeze(2).unsqueeze(3)  # [B, 640, 1, 1]\n",
    "            \n",
    "            # Route features channel-wise\n",
    "            forgetting_features = backbone_features * gating_mask\n",
    "            not_forgetting_features = backbone_features * (1 - gating_mask)\n",
    "        else:\n",
    "            # No gating - both branches get all features\n",
    "            forgetting_features = backbone_features\n",
    "            not_forgetting_features = backbone_features\n",
    "        \n",
    "        return forgetting_features, not_forgetting_features, adapter_predictions\n",
    "    \n",
    "    def store_adapter_state(self):\n",
    "        \"\"\"Store current adapter state for forgetting measurement\"\"\"\n",
    "        self.previous_adapter_state = {\n",
    "            name: param.clone().detach() \n",
    "            for name, param in self.adapter.named_parameters()\n",
    "        }\n",
    "    \n",
    "    def compute_forgetting_scores(self):\n",
    "        \"\"\"Compute channel-wise forgetting scores by comparing adapter states\"\"\"\n",
    "        # Get the device from model parameters\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        if self.previous_adapter_state is None:\n",
    "            # First domain - no forgetting to measure\n",
    "            self.current_forgetting_scores = torch.zeros(self.num_channels, device=device)\n",
    "            return\n",
    "        \n",
    "        # Compare adapter weights to measure which channels are most affected\n",
    "        forgetting_scores = []\n",
    "        \n",
    "        for name, current_param in self.adapter.named_parameters():\n",
    "            if name in self.previous_adapter_state:\n",
    "                previous_param = self.previous_adapter_state[name]\n",
    "                \n",
    "                if 'weight' in name and len(current_param.shape) == 2:\n",
    "                    # For first linear layer after pooling: [256, 640]\n",
    "                    if current_param.shape[1] == self.num_channels:\n",
    "                        # Compute change per input channel\n",
    "                        weight_change = torch.norm(current_param - previous_param, dim=0)\n",
    "                        forgetting_scores.append(weight_change)\n",
    "                        break  # Use only first layer that maps from channels\n",
    "        \n",
    "        # Use channel-wise forgetting scores\n",
    "        if forgetting_scores:\n",
    "            self.current_forgetting_scores = forgetting_scores[0].to(device)\n",
    "            \n",
    "            # Normalize to [0, 1]\n",
    "            if self.current_forgetting_scores.max() > 0:\n",
    "                self.current_forgetting_scores = (\n",
    "                    self.current_forgetting_scores / self.current_forgetting_scores.max()\n",
    "                )\n",
    "        else:\n",
    "            # Fallback if no forgetting scores computed\n",
    "            self.current_forgetting_scores = torch.zeros(self.num_channels, device=device)\n",
    "\n",
    "class ConvBranch(nn.Module):\n",
    "    \"\"\"Convolutional processing branch for spatial features\"\"\"\n",
    "    def __init__(self, input_channels=640, hidden_channels=256, output_channels=128):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Output conv block\n",
    "            nn.Conv2d(hidden_channels, output_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Global pooling to get fixed-size output\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()  # [B, output_channels]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "    \n",
    "class FeatureFusion(nn.Module):\n",
    "    \"\"\"Attention-based fusion of branch features\"\"\"\n",
    "    def __init__(self, feature_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(feature_dim * 2, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, high_features, low_features):\n",
    "        # Concatenate features\n",
    "        combined = torch.cat([high_features, low_features], dim=1)  # [B, 512]\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention(combined)  # [B, 2]\n",
    "        \n",
    "        # Apply attention\n",
    "        weighted_high = high_features * attention_weights[:, 0:1]\n",
    "        weighted_low = low_features * attention_weights[:, 1:2]\n",
    "        \n",
    "        # Combine with residual connection\n",
    "        fused = weighted_high + weighted_low\n",
    "        \n",
    "        return fused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e6e49",
   "metadata": {},
   "source": [
    "#### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatastrophicForgettingDisentanglementModel(nn.Module):\n",
    "    \"\"\"Updated model with spatial features and convolutional branches\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 backbone_output_channels: int = 640,\n",
    "                 branch_hidden_channels: int = 256,\n",
    "                 branch_output_channels: int = 128,\n",
    "                 num_outputs: int = 9):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"ðŸš€ Initializing Spatial Catastrophic Forgetting Model...\")\n",
    "        \n",
    "        # Dual backbone (frozen) - now outputs spatial features\n",
    "        model_paths={\n",
    "            'YOLO11n_seg': '../models/yolo11n-seg.pt', \n",
    "            'DepthAnythingV2_small': '../models/depth_anything_v2_vits.pth'\n",
    "            }\n",
    "        missing = [name for name, path in model_paths.items() if not os.path.exists(path)]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f\"Missing model files for: {', '.join(missing)}\")\n",
    "\n",
    "        self.backbone = DualBackboneFeatureExtractor(model_paths=model_paths)  # [B, 640, 36, 36]\n",
    "        \n",
    "        # Standalone forgetting adapter\n",
    "        self.forgetting_adapter = CatastrophicForgettingAdapter(\n",
    "            num_channels=backbone_output_channels,\n",
    "            num_outputs=num_outputs\n",
    "        )\n",
    "        \n",
    "        # Convolutional branches for spatial processing\n",
    "        self.branch_forgetting = ConvBranch(\n",
    "            backbone_output_channels, \n",
    "            branch_hidden_channels, \n",
    "            branch_output_channels\n",
    "        )\n",
    "        self.branch_not_forgetting = ConvBranch(\n",
    "            backbone_output_channels, \n",
    "            branch_hidden_channels, \n",
    "            branch_output_channels\n",
    "        )\n",
    "        \n",
    "        self.fusion = FeatureFusion(branch_output_channels)\n",
    "\n",
    "        # Final head combines branch outputs\n",
    "        self.head =  nn.Sequential(\n",
    "            nn.Linear(branch_output_channels, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, num_outputs)\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Spatial model initialized successfully!\")\n",
    "        \n",
    "    def forward(self, x, use_gating: bool = True):\n",
    "        # Extract spatial features from frozen backbone\n",
    "        backbone_features = self.backbone(x)  # [B, 640, 36, 36]\n",
    "        \n",
    "        # Process through forgetting adapter (gating + task prediction)\n",
    "        forgetting_features, not_forgetting_features, adapter_predictions = \\\n",
    "            self.forgetting_adapter(backbone_features, use_gating)\n",
    "        \n",
    "        # Process through convolutional branches\n",
    "        forgetting_output = self.branch_forgetting(forgetting_features)      # [B, 128]\n",
    "        not_forgetting_output = self.branch_not_forgetting(not_forgetting_features)  # [B, 128]\n",
    "        \n",
    "        # Combine branch outputs for final prediction\n",
    "        fused_features = self.fusion(forgetting_output, not_forgetting_output)\n",
    "        final_output = self.head(fused_features)\n",
    "        \n",
    "        return {\n",
    "            'output': final_output,\n",
    "            'adapter_output': adapter_predictions,\n",
    "            'backbone_features': backbone_features,\n",
    "            'forgetting_features': forgetting_features,\n",
    "            'not_forgetting_features': not_forgetting_features\n",
    "        }\n",
    "    \n",
    "    def store_adapter_state(self):\n",
    "        \"\"\"Delegate to forgetting adapter\"\"\"\n",
    "        self.forgetting_adapter.store_adapter_state()\n",
    "    \n",
    "    def compute_forgetting_scores(self):\n",
    "        \"\"\"Delegate to forgetting adapter\"\"\"\n",
    "        self.forgetting_adapter.compute_forgetting_scores()\n",
    "\n",
    "    @property\n",
    "    def current_forgetting_scores(self):\n",
    "        \"\"\"Access forgetting scores from adapter\"\"\"\n",
    "        return self.forgetting_adapter.current_forgetting_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf5f99",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize your model\n",
    "# cf_model = CatastrophicForgettingDisentanglementModel(\n",
    "#     backbone_output_channels=640,\n",
    "#     branch_hidden_channels=256,\n",
    "#     branch_output_channels=128,\n",
    "#     num_outputs=9\n",
    "# ).to(device)\n",
    "\n",
    "# # Set model to evaluation mode (important for testing)\n",
    "# cf_model.eval()\n",
    "\n",
    "# # Create dummy input tensor matching your expected input size\n",
    "# # Your model expects: [batch_size, 3, height, width]\n",
    "# # Using 224x224 as we discussed for memory efficiency\n",
    "# batch_size = 2\n",
    "# dummy1 = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "# dummy2 = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "# dummy_input = (dummy1, dummy2)\n",
    "\n",
    "# print(f\"Input shape: {dummy_input[0].shape}\")\n",
    "\n",
    "# # Test forward pass without gating (first domain)\n",
    "# with torch.no_grad():\n",
    "#     outputs_no_gating = cf_model(dummy_input, use_gating=False)\n",
    "\n",
    "# print(\"âœ… Forward pass without gating successful!\")\n",
    "# print(f\"Output shape: {outputs_no_gating['output'].shape}\")\n",
    "# print(f\"Adapter output shape: {outputs_no_gating['adapter_output'].shape}\")\n",
    "# print(f\"Backbone features shape: {outputs_no_gating['backbone_features'].shape}\")\n",
    "\n",
    "# # Test forward pass with gating (after first domain)\n",
    "# # First, simulate having forgetting scores\n",
    "# cf_model.forgetting_adapter.current_forgetting_scores = torch.randn(640).to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs_with_gating = cf_model(dummy_input, use_gating=True)\n",
    "\n",
    "# print(\"âœ… Forward pass with gating successful!\")\n",
    "# print(f\"Forgetting features shape: {outputs_with_gating['forgetting_features'].shape}\")\n",
    "# print(f\"Not forgetting features shape: {outputs_with_gating['not_forgetting_features'].shape}\")\n",
    "\n",
    "# # Test forgetting measurement functions\n",
    "# cf_model.store_adapter_state()\n",
    "# print(\"âœ… Adapter state stored successfully!\")\n",
    "\n",
    "# cf_model.compute_forgetting_scores()\n",
    "# print(\"âœ… Forgetting scores computed successfully!\")\n",
    "# print(f\"Forgetting scores shape: {cf_model.current_forgetting_scores.shape}\")\n",
    "\n",
    "# outputs = cf_model(dummy_input, use_gating=True)\n",
    "# print(outputs['output'].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb8d1c",
   "metadata": {},
   "source": [
    "#### helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a58316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_cf(model, dataloader, criterion, device):\n",
    "    \"\"\"Modified evaluation function for catastrophic forgetting model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for yolo_images, depth_images, labels, _ in dataloader:\n",
    "            yolo_images = yolo_images.to(device, dtype=torch.float32)\n",
    "            depth_images = depth_images.to(device, dtype=torch.float32)\n",
    "            inputs = (yolo_images, depth_images)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs, use_gating=True)  # Use gating during evaluation\n",
    "            loss = criterion(outputs['output'], labels)\n",
    "            total_loss += loss.item() * inputs[0].size(0)\n",
    "            total_samples += inputs[0].size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def cross_domain_validation_cf(model, domain_dataloaders, criterion, device):\n",
    "    \"\"\"Modified cross-domain validation for catastrophic forgetting model\"\"\"\n",
    "    results = {}\n",
    "    for domain, loaders in domain_dataloaders.items():\n",
    "        val_loader = loaders['val']\n",
    "        val_loss = evaluate_model_cf(model, val_loader, criterion, device)\n",
    "        results[domain] = val_loss\n",
    "    return results\n",
    "\n",
    "def average_metrics(metrics_list):\n",
    "    if not metrics_list:\n",
    "        return {}\n",
    "    keys = metrics_list[0].keys()\n",
    "    avg_metrics = {}\n",
    "    for k in keys:\n",
    "        avg_metrics[k] = float(np.mean([m[k] for m in metrics_list if k in m]))\n",
    "    return avg_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c437b",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gating_loss(forgetting_scores: torch.Tensor, \n",
    "                       gating_mask: torch.Tensor,\n",
    "                       lambda_balance: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"Compute loss for gating network\"\"\"\n",
    "    high_forgetting_features = forgetting_scores * gating_mask\n",
    "    low_forgetting_features = forgetting_scores * (1 - gating_mask)\n",
    "    \n",
    "    high_mean = high_forgetting_features.sum() / (gating_mask.sum() + 1e-8)\n",
    "    low_mean = low_forgetting_features.sum() / ((1 - gating_mask).sum() + 1e-8)\n",
    "    \n",
    "    split_loss = -(high_mean - low_mean)\n",
    "    balance_loss = torch.abs(gating_mask.mean() - 0.5)\n",
    "    \n",
    "    total_loss = split_loss + lambda_balance * balance_loss\n",
    "    return total_loss\n",
    "\n",
    "def catastrophic_forgetting_batch(model, batch, device, loss_params={'main': 1.0, 'adapter': 0.5, 'gating': 0.1}, **kwargs):\n",
    "    \"\"\"CORRECTED: Batch function with proper adapter access\"\"\"\n",
    "    yolo_images, depth_images, labels, domain_labels = batch\n",
    "    yolo_images = yolo_images.to(device)\n",
    "    depth_images = depth_images.to(device)\n",
    "    inputs = (yolo_images, depth_images)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    mse_criterion = kwargs['mse_criterion']\n",
    "    domain_idx = kwargs.get('domain_idx', 0)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs, use_gating=(domain_idx > 0))\n",
    "    \n",
    "    # Main task loss (from final head)\n",
    "    main_loss = mse_criterion(outputs['output'], labels)\n",
    "    \n",
    "    # Adapter loss (for monitoring forgetting)\n",
    "    adapter_loss = mse_criterion(outputs['adapter_output'], labels)\n",
    "    \n",
    "    # Main loss backpropagation\n",
    "    main_loss.backward(retain_graph=True) \n",
    "    \n",
    "    # CORRECTED: Gating loss with proper path access\n",
    "    gating_loss = torch.tensor(0.0, device=device)\n",
    "    if domain_idx > 0 and model.current_forgetting_scores is not None:\n",
    "        backbone_features = outputs['backbone_features']\n",
    "        batch_size = backbone_features.size(0)\n",
    "        forgetting_scores_batch = model.current_forgetting_scores.unsqueeze(0).expand(batch_size, -1).to(device)\n",
    "        \n",
    "        # FIXED: Use correct path to gating network\n",
    "        gating_mask = model.forgetting_adapter.gating(forgetting_scores_batch)\n",
    "        \n",
    "        gating_loss = compute_gating_loss(\n",
    "            model.current_forgetting_scores.to(device), \n",
    "            gating_mask.mean(dim=0)\n",
    "        )\n",
    "    \n",
    "    metrics = {\n",
    "        'main_loss': main_loss.item(),\n",
    "        'adapter_loss': adapter_loss.item(),\n",
    "        'gating_loss': gating_loss.item()\n",
    "    }\n",
    "    \n",
    "    return main_loss, gating_loss, metrics\n",
    "\n",
    "def catastrophic_forgetting_train_loop(\n",
    "    model, domains, domain_dataloaders, buffer, optimizer, gating_optimizer, device,\n",
    "    batch_fn, batch_kwargs, loss_params, num_epochs=5, exp_name=\"cf_exp\", \n",
    "    gradient_clipping=False, restart={}\n",
    "):\n",
    "    \"\"\"CORRECTED: Training loop with proper forgetting measurement\"\"\"\n",
    "    start_domain_idx = 0\n",
    "    global_step = 0\n",
    "    history = {\n",
    "        'train_epoch_loss': [],\n",
    "        'val_epoch_loss': [],\n",
    "        'train_epoch_metrics': [],\n",
    "        'cross_domain_val': [],\n",
    "        'grad_norms': [],\n",
    "        'forgetting_scores_history': [],\n",
    "        'gating_losses': []\n",
    "    }\n",
    "    \n",
    "    if restart:\n",
    "        global_step = restart['global_step']\n",
    "        history = restart['history']\n",
    "        start_domain_idx = np.where(domains == restart['domain'])[0][0]\n",
    "        for domain_idx, current_domain in enumerate(domains[:start_domain_idx]):\n",
    "            buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset) \n",
    "        print(f\"Restarting from domain {restart['domain']} index {start_domain_idx}\")\n",
    "        print(f\"Buffer: {buffer.get_domain_distribution()}\")         \n",
    "\n",
    "    for domain_idx, current_domain in enumerate(tqdm(domains[start_domain_idx:], desc=f\"Total training\"), start=start_domain_idx):\n",
    "        print(f\"\\n=== Training on Domain {domain_idx}: {current_domain} ===\")\n",
    "        \n",
    "        # Store adapter state before training (for forgetting measurement)\n",
    "        if domain_idx > 0:\n",
    "            model.store_adapter_state()\n",
    "        \n",
    "        train_loader = buffer.get_loader_with_replay(current_domain, domain_dataloaders[current_domain]['train'])\n",
    "        \n",
    "        for epoch in trange(num_epochs, desc=f\"Current domain {current_domain}\"):\n",
    "            model.train()\n",
    "            epoch_loss = 0.0\n",
    "            samples = 0\n",
    "            batch_metrics_list = []\n",
    "            \n",
    "            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Current epoch {epoch}\", leave=False)):\n",
    "                # Main model training\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                batch_kwargs_with_domain = {**batch_kwargs, 'current_domain': current_domain, 'domain_idx': domain_idx}\n",
    "                \n",
    "                main_loss, gating_loss, metrics = batch_fn(model, batch, device, loss_params, **batch_kwargs_with_domain)\n",
    "                \n",
    "                if gradient_clipping:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Gating training (in same batch, separate optimizer)\n",
    "                if domain_idx > 0 and gating_loss.item() > 0:\n",
    "                    gating_optimizer.zero_grad()\n",
    "                    gating_loss.backward()\n",
    "                    gating_optimizer.step()\n",
    "                \n",
    "                batch_size = batch[0].size(0)\n",
    "                epoch_loss += main_loss.item() * batch_size\n",
    "                samples += batch_size\n",
    "                global_step += 1\n",
    "                batch_metrics_list.append(metrics)\n",
    "                \n",
    "            avg_epoch_loss = epoch_loss / samples\n",
    "            history['train_epoch_loss'].append(avg_epoch_loss)\n",
    "            \n",
    "            avg_metrics = average_metrics(batch_metrics_list)\n",
    "            history['train_epoch_metrics'].append(avg_metrics)\n",
    "            \n",
    "            grad_norms = collect_gradients(model)\n",
    "            history['grad_norms'].append(grad_norms)\n",
    "            \n",
    "            # Validation on current domain\n",
    "            val_loss = evaluate_model_cf(model, domain_dataloaders[current_domain]['val'], batch_kwargs['mse_criterion'], device)\n",
    "            history['val_epoch_loss'].append(val_loss)\n",
    "            \n",
    "            # Cross-domain validation (after each domain)\n",
    "            if epoch == num_epochs-1:\n",
    "                cross_val = cross_domain_validation_cf(model, domain_dataloaders, batch_kwargs['mse_criterion'], device)\n",
    "                history['cross_domain_val'].append(cross_val)\n",
    "\n",
    "                # Compute forgetting scores after training on domain (for next domain)\n",
    "                model.compute_forgetting_scores()\n",
    "                history['forgetting_scores_history'].append(\n",
    "                    model.current_forgetting_scores.clone() if model.current_forgetting_scores is not None else None\n",
    "                )\n",
    "                if model.current_forgetting_scores is not None:\n",
    "                    print(f\"Forgetting scores computed. Mean: {model.current_forgetting_scores.mean():.4f}, \"\n",
    "                        f\"Std: {model.current_forgetting_scores.std():.4f}\")\n",
    "                \n",
    "\n",
    "                # Save checkpoint\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'gating_optimizer_state_dict': gating_optimizer.state_dict(),\n",
    "                    'history': history,\n",
    "                    'forgetting_scores': model.current_forgetting_scores,\n",
    "                }, f\"../checkpoints/{exp_name}_domain{current_domain}_epoch{epoch}_step{global_step}.pt\")\n",
    "            \n",
    "            with open(f\"../checkpoints/{exp_name}_history.pkl\", \"wb\") as f:\n",
    "                pickle.dump(history, f)\n",
    "        \n",
    "        buffer.update_buffer(current_domain, domain_dataloaders[current_domain]['train'].dataset)\n",
    "        print(f\"Domain {domain_idx} completed. Buffer: {buffer.get_domain_distribution()}\")\n",
    "    \n",
    "    return history\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
