{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  # Enforce deterministic algorithms\n",
    "        torch.backends.cudnn.benchmark = False     # Disable benchmark for reproducibility\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)       # Seed Python hashing, which can affect ordering\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb683b",
   "metadata": {},
   "source": [
    "### Explicit Heuristic Split Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361082c6",
   "metadata": {},
   "source": [
    "#### ZoeDepth - HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "# from torchvision import transforms\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# output_dir = Path('../data/depth')\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", use_fast=True)\n",
    "# model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(device).eval()\n",
    "\n",
    "# # Prepare image paths list\n",
    "# image_paths = df['image_path'].tolist()\n",
    "\n",
    "# batch_size = 10  # or whatever batch size you want\n",
    "# for batch_idx in tqdm(range(0, len(image_paths), batch_size)):\n",
    "#     batch_paths = image_paths[batch_idx:batch_idx + batch_size]\n",
    "    \n",
    "#     # Load images as PIL Images (no manual transform)\n",
    "#     batch_images = [Image.open(img_path).convert(\"RGB\") for img_path in batch_paths]\n",
    "    \n",
    "#     # Preprocess with ZoeDepth image processor\n",
    "#     inputs = image_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # Post-process depth maps to original sizes\n",
    "#     source_sizes = [(img.height, img.width) for img in batch_images]\n",
    "#     post_processed = image_processor.post_process_depth_estimation(\n",
    "#         outputs,\n",
    "#         source_sizes=source_sizes\n",
    "#     )\n",
    "    \n",
    "#     for i, depth_dict in enumerate(post_processed):\n",
    "#         # Get raw depth map\n",
    "#         depth_array = depth_dict[\"predicted_depth\"].cpu().numpy()\n",
    "#         img_stem = Path(batch_paths[i]).stem\n",
    "#         np.save(output_dir / f\"{img_stem}.npy\", depth_array)\n",
    "#         # Save visualization PNG\n",
    "#         depth_norm = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min())\n",
    "#         depth_img = Image.fromarray((depth_norm * 255).astype(np.uint8))\n",
    "#         depth_img.save(output_dir / f\"{img_stem}_depth.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a104c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c64a710",
   "metadata": {},
   "source": [
    "#### segmentation generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043b1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "from autodistill.utils import plot\n",
    "import cv2\n",
    "import pickle\n",
    "import bz2\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an ontology to map class names to our GroundedSAM prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = GroundedSAM(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"human . child . person\": \"human\",\n",
    "            \"robot\": \"robot\",\n",
    "            \"dog\": \"dog\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# human : human, anima: animal, robot:robot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bad2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run inference on a single image\n",
    "# results = base_model.predict(df['image_path'].iloc[0])\n",
    "\n",
    "# plot(\n",
    "#     image=cv2.imread(df['image_path'].iloc[0]),\n",
    "#     classes=base_model.ontology.classes(),\n",
    "#     detections=results\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e939f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "    dataset_home = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d117674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bz2.BZ2File('autodistill_dataset.pbz2', 'wb') as f:\n",
    "#     pickle.dump(dataset, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394547bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_home = base_model.label(\"../../socialsense/data/images/home\", extension=\".png\")\n",
    "# with bz2.BZ2File('autodistill_dataset_home.pbz2', 'wb') as f:\n",
    "#     pickle.dump(dataset_home, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"autodistill_temp_dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8220ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import os\n",
    "import supervision as sv\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Define a color palette for your classes\n",
    "# Use RGBA for masks (with transparency), RGB for boxes (solid)\n",
    "class_colors = {\n",
    "    0: (255, 0, 0, 100),    # Red for class 0 (human)\n",
    "    1: (0, 255, 0, 100),    # Green for class 1 (robot)\n",
    "    2: (0, 0, 255, 100),    # Blue for class 2 (animal)\n",
    "    3: (122, 122, 0, 100),\n",
    "    # Add more if you have more classes\n",
    "}\n",
    "\n",
    "def visualize_and_save_pil_colored(dataset, confidence_threshold=0.3, output_dir=\"../data/temp_masks_coloured\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for idx, (path, image, detections) in enumerate(tqdm(dataset)):\n",
    "        # Convert BGR to RGB for PIL (if your image is BGR)\n",
    "        rgb_image = image[:, :, ::-1]\n",
    "        pil_img = Image.fromarray(rgb_image)\n",
    "        draw = ImageDraw.Draw(pil_img, \"RGBA\")\n",
    "        \n",
    "        # Filter detections by confidence\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        if not keep_indices:\n",
    "            print(f\"No detections above threshold for image {path}\")\n",
    "            continue\n",
    "\n",
    "        filtered_boxes = detections.xyxy[keep_indices]\n",
    "        filtered_masks = detections.mask[keep_indices]\n",
    "        filtered_confidences = detections.confidence[keep_indices]\n",
    "        filtered_class_ids = detections.class_id[keep_indices]\n",
    "        \n",
    "        # Overlay masks with transparency and class colors\n",
    "        for i, (mask, conf) in enumerate(zip(filtered_masks, filtered_confidences)):\n",
    "            class_id = filtered_class_ids[i] if filtered_class_ids is not None else 0\n",
    "            color = class_colors.get(class_id, (255, 255, 255, 100))  # Default white if class unknown\n",
    "            # Create a colored mask with alpha\n",
    "            mask_img = Image.fromarray((mask * 255).astype(np.uint8), mode=\"L\")\n",
    "            colored_mask = Image.new(\"RGBA\", pil_img.size, color)\n",
    "            # Composite the colored mask onto the image with transparency\n",
    "            pil_img = Image.alpha_composite(pil_img.convert(\"RGBA\"), Image.composite(colored_mask, Image.new(\"RGBA\", pil_img.size), mask_img))\n",
    "        \n",
    "        draw = ImageDraw.Draw(pil_img)\n",
    "        \n",
    "        # Draw bounding boxes and confidence with class colors\n",
    "        for i, (box, conf) in enumerate(zip(filtered_boxes, filtered_confidences)):\n",
    "            class_id = filtered_class_ids[i] if filtered_class_ids is not None else 0\n",
    "            color = class_colors[class_id][:3]  # Use RGB for boxes (no alpha)\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=color, width=2)\n",
    "            draw.text((x1, y1 - 10*class_id), f\"{class_id}: {conf:.2f}\", fill=color)\n",
    "        \n",
    "        # Save the image as PNG\n",
    "        filename = os.path.basename(path)\n",
    "        save_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \".png\")\n",
    "        pil_img.convert(\"RGB\").save(save_path)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from supervision import Detections\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union for two boxes [x1,y1,x2,y2]\"\"\"\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2_inter - x1_inter) * max(0, y2_inter - y1_inter)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    return inter_area / (box1_area + box2_area - inter_area + 1e-6)\n",
    "\n",
    "def remove_duplicates_any_class(detections: Detections, iou_threshold=0.9) -> Detections:\n",
    "    \"\"\"\n",
    "    Remove duplicate detections using IoU and confidence, regardless of class.\n",
    "    \n",
    "    Args:\n",
    "        detections: supervision.Detections object\n",
    "        iou_threshold: IoU threshold for considering duplicates\n",
    "        \n",
    "    Returns:\n",
    "        Filtered Detections object\n",
    "    \"\"\"\n",
    "    # Extract components from Detections\n",
    "    xyxy = detections.xyxy\n",
    "    confidence = detections.confidence\n",
    "    class_id = detections.class_id\n",
    "    mask = detections.mask\n",
    "\n",
    "    # Convert to list of dicts for processing\n",
    "    detections_list = [\n",
    "        {\n",
    "            'box': xyxy[i],\n",
    "            'mask': mask[i] if mask is not None else None,\n",
    "            'confidence': confidence[i],\n",
    "            'class_id': class_id[i]\n",
    "        }\n",
    "        for i in range(len(xyxy))\n",
    "    ]\n",
    "\n",
    "    # Sort by confidence (highest first)\n",
    "    detections_list.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Filter duplicates (do NOT check class_id)\n",
    "    keep = []\n",
    "    while detections_list:\n",
    "        current = detections_list.pop(0)\n",
    "        keep.append(current)\n",
    "        detections_list = [\n",
    "            d for d in detections_list\n",
    "            if calculate_iou(current['box'], d['box']) <= iou_threshold\n",
    "        ]\n",
    "\n",
    "    # Reconstruct Detections object\n",
    "    return Detections(\n",
    "        xyxy=np.array([d['box'] for d in keep]),\n",
    "        confidence=np.array([d['confidence'] for d in keep]),\n",
    "        class_id=np.array([d['class_id'] for d in keep]),\n",
    "        mask=np.array([d['mask'] for d in keep]) if mask is not None else None\n",
    "    )\n",
    "\n",
    "# Usage example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b23b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = []\n",
    "for path, image, detections in dataset_746:\n",
    "    filtered_detections = remove_duplicates_any_class(detections, iou_threshold=0.95)\n",
    "    filtered_dataset.append((path, image, filtered_detections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_and_save_pil_colored(dataset, confidence_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c106d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96dbcea4",
   "metadata": {},
   "source": [
    "#### depth mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_mean_std_for_npy(folder_path):\n",
    "    total_sum = 0\n",
    "    total_sum_sq = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # List all .npy files\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "    \n",
    "    # Wrap the loop with tqdm for progress bar\n",
    "    for filename in tqdm(files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        img = np.load(file_path).astype(np.float64)\n",
    "        total_sum += img.sum()\n",
    "        total_sum_sq += (img ** 2).sum()\n",
    "        total_count += img.size\n",
    "\n",
    "    mean = total_sum / total_count if total_count > 0 else None\n",
    "    variance = (total_sum_sq / total_count) - (mean ** 2) if total_count > 0 else None\n",
    "    std = np.sqrt(variance) if variance is not None else None\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7343985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_mean_std_for_npy('../data/depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996cb40",
   "metadata": {},
   "source": [
    "#### mask fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3625169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_segmentation_data(detections_dataset, output_dir, imagenet_mean=[0.485, 0.456, 0.406], confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process supervision Detections dataset to create social and environment images\n",
    "    \n",
    "    Args:\n",
    "        detections_dataset: List of tuples (image_path, Detection_object, ...)\n",
    "        output_dir: Base directory to save processed images\n",
    "        imagenet_mean: RGB mean values for filling masked areas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directories\n",
    "    social_dir = Path(output_dir) / \"social\"\n",
    "    env_dir = Path(output_dir) / \"environment\"\n",
    "    social_dir.mkdir(parents=True, exist_ok=True)\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = item[0]\n",
    "        detections = item[2]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = original_image.shape[:2]\n",
    "        \n",
    "        # Get filename for saving\n",
    "        filename = Path(image_path).name\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        filtered_masks = detections.mask[keep_indices]\n",
    "        \n",
    "        # Combine all masks into one\n",
    "        combined_mask = combine_masks(filtered_masks, height, width)\n",
    "        \n",
    "        # Create social image (only people and robot visible)\n",
    "        social_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=True\n",
    "        )\n",
    "        \n",
    "        # Create environment image (room only, people and robot masked out)\n",
    "        env_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=False\n",
    "        )\n",
    "        \n",
    "        # Save images\n",
    "        social_path = social_dir / filename\n",
    "        env_path = env_dir / filename\n",
    "        \n",
    "        save_image(social_image, social_path)\n",
    "        save_image(env_image, env_path)\n",
    "\n",
    "\n",
    "def combine_masks(masks, height, width, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Combine multiple masks into a single binary mask using union operation\n",
    "    \n",
    "    Args:\n",
    "        masks: Array of individual masks from Detection object\n",
    "        height, width: Dimensions of the original image\n",
    "        \n",
    "    Returns:\n",
    "        combined_mask: Single binary mask (1 = object, 0 = background)\n",
    "    \"\"\"\n",
    "    if masks is None or len(masks) == 0:\n",
    "        return np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Initialize combined mask\n",
    "    combined_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Union all individual masks using maximum function (as shown in search results)\n",
    "    for mask in masks:\n",
    "        # Ensure mask is the right size\n",
    "        if mask.shape != (height, width):\n",
    "            raise ValueError(f\"Mask shape incorrect: {mask.shape}, should be {(height, width)}\")\n",
    "        \n",
    "        # Union operation: take maximum of current combined mask and new mask\n",
    "        combined_mask = np.maximum(combined_mask, mask.astype(np.uint8))\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "def apply_mask_with_mean(image, mask, imagenet_mean, keep_masked=True):\n",
    "    \"\"\"\n",
    "    Apply mask to image and fill empty areas with ImageNet mean values\n",
    "    \n",
    "    Args:\n",
    "        image: Original RGB image (H, W, 3)\n",
    "        mask: Binary mask (H, W) where 1 = object, 0 = background\n",
    "        imagenet_mean: RGB mean values [R, G, B] in range [0, 1]\n",
    "        keep_masked: If True, keep masked areas (social). If False, remove masked areas (environment)\n",
    "        \n",
    "    Returns:\n",
    "        processed_image: Image with mask applied and filled with mean values\n",
    "    \"\"\"\n",
    "    processed_image = image.copy().astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert imagenet_mean to same range as image\n",
    "    mean_values = np.array(imagenet_mean).reshape(1, 1, 3)\n",
    "    \n",
    "    if keep_masked:\n",
    "        # Social image: keep people/robot, fill background with mean\n",
    "        fill_mask = (mask == 0)  # Areas to fill (background)\n",
    "    else:\n",
    "        # Environment image: keep background, fill people/robot with mean  \n",
    "        fill_mask = (mask == 1)  # Areas to fill (people/robot)\n",
    "    \n",
    "    # Fill specified areas with ImageNet mean values\n",
    "    for c in range(3):  # RGB channels\n",
    "        processed_image[:, :, c][fill_mask] = imagenet_mean[c]\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    processed_image = (processed_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return processed_image\n",
    "\n",
    "def save_image(image, save_path):\n",
    "    \"\"\"\n",
    "    Save image to specified path\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        save_path: Path to save the image\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image and save\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6980dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bz2.BZ2File('autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "#     dataset_home = pickle.load(f)\n",
    "\n",
    "# process_segmentation_data(\n",
    "#     detections_dataset=dataset_home,\n",
    "#     output_dir='../data/masked',\n",
    "#     imagenet_mean=[0.485, 0.456, 0.406]  # ImageNet RGB means\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
