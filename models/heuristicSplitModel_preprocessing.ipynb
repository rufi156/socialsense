{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import bz2\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  # Enforce deterministic algorithms\n",
    "        torch.backends.cudnn.benchmark = False     # Disable benchmark for reproducibility\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)       # Seed Python hashing, which can affect ordering\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb683b",
   "metadata": {},
   "source": [
    "### Explicit Heuristic Split Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361082c6",
   "metadata": {},
   "source": [
    "#### ZoeDepth - HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498b458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n",
    "# from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "# from torchvision import transforms\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# output_dir = Path('../data/depth')\n",
    "# output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", use_fast=True)\n",
    "# model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(device).eval()\n",
    "\n",
    "# # Prepare image paths list\n",
    "# image_paths = df['image_path'].tolist()\n",
    "\n",
    "# batch_size = 10  # or whatever batch size you want\n",
    "# for batch_idx in tqdm(range(0, len(image_paths), batch_size)):\n",
    "#     batch_paths = image_paths[batch_idx:batch_idx + batch_size]\n",
    "    \n",
    "#     # Load images as PIL Images (no manual transform)\n",
    "#     batch_images = [Image.open(img_path).convert(\"RGB\") for img_path in batch_paths]\n",
    "    \n",
    "#     # Preprocess with ZoeDepth image processor\n",
    "#     inputs = image_processor(images=batch_images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "    \n",
    "#     # Post-process depth maps to original sizes\n",
    "#     source_sizes = [(img.height, img.width) for img in batch_images]\n",
    "#     post_processed = image_processor.post_process_depth_estimation(\n",
    "#         outputs,\n",
    "#         source_sizes=source_sizes\n",
    "#     )\n",
    "    \n",
    "#     for i, depth_dict in enumerate(post_processed):\n",
    "#         # Get raw depth map\n",
    "#         depth_array = depth_dict[\"predicted_depth\"].cpu().numpy()\n",
    "#         img_stem = Path(batch_paths[i]).stem\n",
    "#         np.save(output_dir / f\"{img_stem}.npy\", depth_array)\n",
    "#         # Save visualization PNG\n",
    "#         depth_norm = (depth_array - depth_array.min()) / (depth_array.max() - depth_array.min())\n",
    "#         depth_img = Image.fromarray((depth_norm * 255).astype(np.uint8))\n",
    "#         depth_img.save(output_dir / f\"{img_stem}_depth.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72e4b3",
   "metadata": {},
   "source": [
    "#### Segmentation Pipeline GroundingDINO+SAM through Autodistill GroundingSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from autodistill_grounded_sam import GroundedSAM\n",
    "from autodistill.detection import CaptionOntology\n",
    "from tqdm.notebook import tqdm\n",
    "import supervision as sv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc51c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categories for images\n",
    "def get_category(image_name: str) -> str:\n",
    "    categories = [\"Hallway\", \"BigOffice-2\", \"BigOffice-3\", \"MeetingRoom\", \"SmallOffice\", \"Home\"]\n",
    "    for cat in categories:\n",
    "        if cat in image_name:\n",
    "            return cat\n",
    "    return \"Home\"\n",
    "\n",
    "# Helper to filter Detections object dict with only needed attributes\n",
    "def filter_detections(detections):\n",
    "    filtered = {\n",
    "        \"xyxy\": detections.xyxy,\n",
    "        \"mask\": detections.mask,\n",
    "        \"confidence\": detections.confidence,\n",
    "        \"class_id\": detections.class_id,\n",
    "        # discard the rest\n",
    "    }\n",
    "    return filtered\n",
    "\n",
    "def load_detection_from_h5(img_group) -> sv.Detections:\n",
    "    xyxy = img_group[\"xyxy\"][:]            # numpy array (N,4)\n",
    "    masks = img_group[\"mask\"][:]            # numpy array (N, H, W)\n",
    "    confidence = img_group[\"confidence\"][:] # numpy array (N,)\n",
    "    class_id = img_group[\"class_id\"][:]     # numpy array (N,)\n",
    "\n",
    "    detection = sv.Detections(\n",
    "        xyxy=xyxy,\n",
    "        mask=masks,\n",
    "        confidence=confidence,\n",
    "        class_id=class_id,\n",
    "    )\n",
    "    return detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with ontology\n",
    "# define an ontology to map class names to our GroundedSAM prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = GroundedSAM(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"human . child . person\": \"human\",\n",
    "            \"robot\": \"robot\",\n",
    "            \"dog\": \"dog\"\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_images(\n",
    "    image_dir=\"../data/images/\",\n",
    "    image_extension=\".png\",\n",
    "    hdf5_path=\"autodistill_segmentation_dataset.hdf5\",\n",
    "    batch_size=20,\n",
    "    ):\n",
    "\n",
    "    all_images = [f for f in os.listdir(image_dir) if f.endswith(image_extension)]\n",
    "\n",
    "    # Create HDF5 file\n",
    "    if os.path.exists(hdf5_path):\n",
    "        raise FileExistsError(f\"Filec '{hdf5_path}' already exists.\")\n",
    "    else:\n",
    "        h5file = h5py.File(hdf5_path, \"a\")\n",
    "\n",
    "    # Process images\n",
    "    for i in tqdm(range(0, len(all_images), batch_size), desc=\"Batch\"):\n",
    "        batch = all_images[i : i + batch_size]\n",
    "        for image_name in tqdm(batch, desc=\"Image\", leave=False):\n",
    "            img_path = os.path.join(image_dir, image_name)\n",
    "            \n",
    "            detections = base_model.predict(img_path)\n",
    "            data = filter_detections(detections)\n",
    "\n",
    "            # Create group for category\n",
    "            category = get_category(image_name)\n",
    "            if category not in h5file:\n",
    "                h5file.create_group(category)\n",
    "            category_group = h5file[category]\n",
    "\n",
    "            # Create subgroup for image name\n",
    "            if image_name in category_group:\n",
    "                raise RuntimeError(f\"Duplicate entry detected for image '{image_name}' in category '{category}'.\")\n",
    "            img_group = category_group.create_group(image_name)\n",
    "\n",
    "            # Save detection data to datasets in img_group\n",
    "            img_group.create_dataset(\"xyxy\", data=data[\"xyxy\"], compression=\"gzip\")\n",
    "            img_group.create_dataset(\"mask\", data=data[\"mask\"], compression=\"gzip\")\n",
    "            img_group.create_dataset(\"confidence\", data=data[\"confidence\"], compression=\"gzip\")\n",
    "            img_group.create_dataset(\"class_id\", data=data[\"class_id\"], compression=\"gzip\")\n",
    "\n",
    "        # Flush to disk after batch\n",
    "        h5file.flush()\n",
    "    h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ac604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_images(image_dir=\"../data/images/\", image_extension=\".png\", hdf5_path=\"autodistill_segmentation_dataset.hdf5\", batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_detections_dataset(hdf5_path = \"autodistill_segmentation_dataset.hdf5\"):\n",
    "    with h5py.File(hdf5_path, \"r\") as h5file:\n",
    "        categories = list(h5file.keys())\n",
    "        print(f\"Categories ({len(categories)}): {categories}\\n\")\n",
    "\n",
    "        total_images = 0\n",
    "        category_counts = {}\n",
    "        image_paths = []\n",
    "\n",
    "        for category in categories:\n",
    "            images = list(h5file[category].keys())\n",
    "            count = len(images)\n",
    "            category_counts[category] = count\n",
    "            total_images += count\n",
    "            # Save image full path as (category, image_name)\n",
    "            image_paths.extend([(category, img) for img in images])\n",
    "\n",
    "        print(\"Number of images per category:\")\n",
    "        for cat, cnt in category_counts.items():\n",
    "            print(f\"  {cat}: {cnt}\")\n",
    "        print(f\"\\nTotal images: {total_images}\\n\")\n",
    "\n",
    "        # Sample 20 random images for inspection\n",
    "        sample_paths = random.sample(image_paths, min(20, len(image_paths)))\n",
    "        print(\"Sample 20 images (category, image_name) and dataset shapes:\")\n",
    "\n",
    "        for category, image_name in sample_paths:\n",
    "            img_group = h5file[category][image_name]\n",
    "            print(f\"\\n{category}/{image_name}:\")\n",
    "            for dataset_name in img_group.keys():\n",
    "                data = img_group[dataset_name]\n",
    "                print(f\"  - {dataset_name}: shape={data.shape}, dtype={data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_detections_dataset(hdf5_path=\"autodistill_segmentation_dataset.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ab603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def apply_bbox_mask_to_dataset(\n",
    "        image_dir=\"../data/images/\",\n",
    "        hdf5_path=\"autodistill_segmentation_dataset.hdf5\",\n",
    "        output_dir=\"../data/test_segmentations_colored/\",\n",
    "        confidence_threshold=0.3,\n",
    "        resize_to=(512, 288),\n",
    "        deduplicate_bbox=False\n",
    "        ):\n",
    "\n",
    "    # Define class colors with alpha (RGBA)\n",
    "    class_colors = {\n",
    "        0: (255, 0, 0, 100),    # Red (human)\n",
    "        1: (0, 255, 0, 100),    # Green (robot)\n",
    "        2: (0, 0, 255, 100),    # Blue (animal)\n",
    "        3: (122, 122, 0, 100),\n",
    "        # Add more classes/colors as needed\n",
    "    }\n",
    "\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    target_width, target_height = resize_to\n",
    "    \n",
    "\n",
    "    with h5py.File(hdf5_path, \"r\") as h5file:\n",
    "        for category in tqdm(h5file.keys(), desc=\"Categories\"):\n",
    "            category_group = h5file[category]\n",
    "            for image_name in tqdm(category_group.keys(), desc=f\"Images in {category}\", leave=False):\n",
    "                img_group = category_group[image_name]\n",
    "                detections = load_detection_from_h5(img_group)\n",
    "                if deduplicate_bbox:\n",
    "                    detections = detections.with_nms(class_agnostic=True)\n",
    "\n",
    "                xyxy = detections.xyxy\n",
    "                masks = detections.mask\n",
    "                confidences = detections.confidence\n",
    "                class_ids = detections.class_id\n",
    "\n",
    "                img_path = os.path.join(image_dir, image_name)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    print(f\"Warning: Image {image_name} not found, skipping.\")\n",
    "                    continue\n",
    "\n",
    "                orig_h, orig_w = image.shape[:2]\n",
    "                scale_x = target_width / orig_w\n",
    "                scale_y = target_height / orig_h\n",
    "\n",
    "                rgb_image = image[:, :, ::-1]\n",
    "                pil_img = Image.fromarray(rgb_image).convert(\"RGBA\")\n",
    "                pil_img = pil_img.resize((target_width, target_height), resample=Image.LANCZOS)\n",
    "\n",
    "                # Filter detections by confidence threshold\n",
    "                keep = confidences >= confidence_threshold\n",
    "                if not np.any(keep):\n",
    "                    print(f\"No detections above threshold for {image_name}\")\n",
    "                    continue\n",
    "\n",
    "                filtered_boxes = xyxy[keep]\n",
    "                filtered_masks = masks[keep]\n",
    "                filtered_confs = confidences[keep]\n",
    "                filtered_classes = class_ids[keep]\n",
    "\n",
    "                # Scale bounding boxes to resized image\n",
    "                scaled_boxes = []\n",
    "                for box in filtered_boxes:\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    scaled_box = (\n",
    "                        int(x1 * scale_x),\n",
    "                        int(y1 * scale_y),\n",
    "                        int(x2 * scale_x),\n",
    "                        int(y2 * scale_y),\n",
    "                    )\n",
    "                    scaled_boxes.append(scaled_box)\n",
    "\n",
    "                # Resize masks to target size (nearest neighbor)\n",
    "                scaled_masks = []\n",
    "                for mask in filtered_masks:\n",
    "                    pil_mask = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "                    pil_mask = pil_mask.resize((target_width, target_height), resample=Image.NEAREST)\n",
    "                    scaled_mask = np.array(pil_mask) > 128 #turn to binary\n",
    "                    scaled_masks.append(scaled_mask)\n",
    "\n",
    "                # Overlay colored masks with transparency\n",
    "                for mask, class_id in zip(scaled_masks, filtered_classes):\n",
    "                    color = class_colors.get(class_id, (255, 255, 255, 100))  # default white\n",
    "                    mask_img = Image.fromarray((mask.astype(bool) * 255).astype(np.uint8), mode=\"L\")\n",
    "                    colored_mask = Image.new(\"RGBA\", pil_img.size, color)\n",
    "                    pil_img = Image.alpha_composite(pil_img, Image.composite(colored_mask, Image.new(\"RGBA\", pil_img.size), mask_img))\n",
    "\n",
    "                # Draw bounding boxes and confidence text\n",
    "                draw = ImageDraw.Draw(pil_img)\n",
    "                for (x1, y1, x2, y2), conf, class_id in zip(scaled_boxes, filtered_confs, filtered_classes):\n",
    "                    color_rgb = class_colors.get(class_id, (255, 255, 255, 100))[:3]\n",
    "                    draw.rectangle([x1, y1, x2, y2], outline=color_rgb, width=2)\n",
    "                    draw.text((x1, max(y1 - 10 * (class_id + 1), 0)), f\"{class_id}: {conf:.2f}\", fill=color_rgb)\n",
    "\n",
    "                # Save output image as PNG\n",
    "                save_path = os.path.join(output_dir, os.path.splitext(image_name)[0] + \".png\")\n",
    "                pil_img.convert(\"RGB\").save(save_path)\n",
    "\n",
    "    print(\"All images processed and saved with resized overlays.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(hdf5_path=\"autodistill_segmentation_dataset.hdf5\", image_dir=\"../data/images/\", category_name=\"Hallway\", deduplicate_bbox=False):\n",
    "    \"\"\"\n",
    "    Get the dataset for semantic split.\n",
    "    \"\"\"\n",
    "    detections_dataset = []\n",
    "\n",
    "    with h5py.File(hdf5_path, \"r\") as h5file:\n",
    "        category_group = h5file[category_name]\n",
    "        for image_name in category_group.keys():\n",
    "            img_group = category_group[image_name]\n",
    "            detections = load_detection_from_h5(img_group)\n",
    "            if deduplicate_bbox:\n",
    "                detections = detections.with_nms(class_agnostic=True)\n",
    "            img_path = os.path.join(image_dir, image_name)\n",
    "            detections_dataset.append((img_path, detections))\n",
    "\n",
    "    print(f\"Loaded {len(detections_dataset)} images and detections from category '{category_name}'.\")\n",
    "    return detections_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dbcea4",
   "metadata": {},
   "source": [
    "#### depth mean std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def calculate_mean_std_for_npy(folder_path):\n",
    "    total_sum = 0\n",
    "    total_sum_sq = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # List all .npy files\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.npy')]\n",
    "    \n",
    "    # Wrap the loop with tqdm for progress bar\n",
    "    for filename in tqdm(files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        img = np.load(file_path).astype(np.float64)\n",
    "        total_sum += img.sum()\n",
    "        total_sum_sq += (img ** 2).sum()\n",
    "        total_count += img.size\n",
    "\n",
    "    mean = total_sum / total_count if total_count > 0 else None\n",
    "    variance = (total_sum_sq / total_count) - (mean ** 2) if total_count > 0 else None\n",
    "    std = np.sqrt(variance) if variance is not None else None\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7343985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_mean_std_for_npy('../data/depth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996cb40",
   "metadata": {},
   "source": [
    "#### Heuristic Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3625169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_segmentation_data(detections_dataset, output_dir, imagenet_mean=[0.485, 0.456, 0.406], confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process supervision Detections dataset to create social and environment images\n",
    "    \n",
    "    Args:\n",
    "        detections_dataset: List of tuples (image_path, Detection_object, ...)\n",
    "        output_dir: Base directory to save processed images\n",
    "        imagenet_mean: RGB mean values for filling masked areas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directories\n",
    "    social_dir = Path(output_dir) / \"social\"\n",
    "    env_dir = Path(output_dir) / \"environment\"\n",
    "    social_dir.mkdir(parents=True, exist_ok=True)\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = item[0]\n",
    "        detections = item[-1]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = original_image.shape[:2]\n",
    "        \n",
    "        # Get filename for saving\n",
    "        filename = Path(image_path).name\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        filtered_masks = detections.mask[keep_indices]\n",
    "        \n",
    "        # Combine all masks into one\n",
    "        combined_mask = combine_masks(filtered_masks, height, width)\n",
    "        \n",
    "        # Create social image (only people and robot visible)\n",
    "        social_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=True\n",
    "        )\n",
    "        \n",
    "        # Create environment image (room only, people and robot masked out)\n",
    "        env_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=False\n",
    "        )\n",
    "        \n",
    "        # Save images\n",
    "        social_path = social_dir / filename\n",
    "        env_path = env_dir / filename\n",
    "        \n",
    "        save_image(social_image, social_path)\n",
    "        save_image(env_image, env_path)\n",
    "\n",
    "\n",
    "def combine_masks(masks, height, width, confidence_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Combine multiple masks into a single binary mask using union operation\n",
    "    \n",
    "    Args:\n",
    "        masks: Array of individual masks from Detection object\n",
    "        height, width: Dimensions of the original image\n",
    "        \n",
    "    Returns:\n",
    "        combined_mask: Single binary mask (1 = object, 0 = background)\n",
    "    \"\"\"\n",
    "    if masks is None or len(masks) == 0:\n",
    "        return np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Initialize combined mask\n",
    "    combined_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    # Union all individual masks using maximum function (as shown in search results)\n",
    "    for mask in masks:\n",
    "        # Ensure mask is the right size\n",
    "        if mask.shape != (height, width):\n",
    "            raise ValueError(f\"Mask shape incorrect: {mask.shape}, should be {(height, width)}\")\n",
    "        \n",
    "        # Union operation: take maximum of current combined mask and new mask\n",
    "        combined_mask = np.maximum(combined_mask, mask.astype(np.uint8))\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "def apply_mask_with_mean(image, mask, imagenet_mean, keep_masked=True):\n",
    "    \"\"\"\n",
    "    Apply mask to image and fill empty areas with ImageNet mean values\n",
    "    \n",
    "    Args:\n",
    "        image: Original RGB image (H, W, 3)\n",
    "        mask: Binary mask (H, W) where 1 = object, 0 = background\n",
    "        imagenet_mean: RGB mean values [R, G, B] in range [0, 1]\n",
    "        keep_masked: If True, keep masked areas (social). If False, remove masked areas (environment)\n",
    "        \n",
    "    Returns:\n",
    "        processed_image: Image with mask applied and filled with mean values\n",
    "    \"\"\"\n",
    "    processed_image = image.copy().astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert imagenet_mean to same range as image\n",
    "    mean_values = np.array(imagenet_mean).reshape(1, 1, 3)\n",
    "    \n",
    "    if keep_masked:\n",
    "        # Social image: keep people/robot, fill background with mean\n",
    "        fill_mask = (mask == 0)  # Areas to fill (background)\n",
    "    else:\n",
    "        # Environment image: keep background, fill people/robot with mean  \n",
    "        fill_mask = (mask == 1)  # Areas to fill (people/robot)\n",
    "    \n",
    "    # Fill specified areas with ImageNet mean values\n",
    "    for c in range(3):  # RGB channels\n",
    "        processed_image[:, :, c][fill_mask] = imagenet_mean[c]\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    processed_image = (processed_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return processed_image\n",
    "\n",
    "def save_image(image, save_path):\n",
    "    \"\"\"\n",
    "    Save image to specified path\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image array (H, W, 3)\n",
    "        save_path: Path to save the image\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image and save\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6980dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_home = get_dataset(\n",
    "#     hdf5_path=\"autodistill_segmentation_dataset.hdf5\", \n",
    "#     image_dir=\"../data/images/\", \n",
    "#     category_name=\"Home\", \n",
    "#     deduplicate_bbox=False)\n",
    "\n",
    "# process_segmentation_data(\n",
    "#     detections_dataset=dataset_home,\n",
    "#     output_dir='../data/masked',\n",
    "#     imagenet_mean=[0.485, 0.456, 0.406]  # ImageNet RGB means\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2277e",
   "metadata": {},
   "source": [
    "#### obfuscate env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a48ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ellipse_pool(detections_dataset, confidence_threshold=0.3):\n",
    "    ellipses_pool = []\n",
    "\n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/images\") / Path(item[0]).name\n",
    "        detections = item[2]\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "\n",
    "        height, width = cv2.imread(str(image_path)).shape[:2]\n",
    "        \n",
    "        for i in keep_indices:\n",
    "            x1, y1, x2, y2 = detections.xyxy[i]\n",
    "\n",
    "            # create blank mask\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "            # center, axes lengths, angle\n",
    "            center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "            axes = ((x2 - x1) / 2, (y2 - y1) / 2)\n",
    "            angle = 0  # no rotation\n",
    "\n",
    "            # draw ellipse\n",
    "            mask = draw_random_pear_blob_cv(mask, center, axes[0], axes[1],\n",
    "                         irregularity=0.6, spikiness=0.4,\n",
    "                         vertical_stretch=1.2, bottom_bias=0.4)\n",
    "            ellipses_pool.append(mask)\n",
    "\n",
    "    ellipses_pool = np.stack(ellipses_pool, axis=0)\n",
    "    return ellipses_pool\n",
    "\n",
    "def get_silhouettes_pool(detections_dataset, confidence_threshold=0.3):\n",
    "    social_actors_silhouettes=[]\n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/images\") / Path(item[0]).name\n",
    "        detections = item[2]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        social_actors_silhouettes.append(detections.mask[keep_indices])\n",
    "    social_actors_silhouettes = np.concatenate(social_actors_silhouettes, axis=0)\n",
    "    return social_actors_silhouettes\n",
    "        \n",
    "def add_masks_to_image(detections_dataset, output_dir, masks_pool, imagenet_mean=[0.485, 0.456, 0.406]):\n",
    "    \n",
    "    env_dir = Path(output_dir) / \"environment_plus\"\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/masked/environment_elipses\") / Path(item[0]).name\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = original_image.shape[:2]\n",
    "        \n",
    "        # Get filename for saving\n",
    "        filename = Path(image_path).name \n",
    "\n",
    "        idxs = random.sample(range(masks_pool.shape[0]), 15)\n",
    "        filtered_masks = masks_pool[idxs]   # shape (10, 1080, 1920)\n",
    "        # Combine all masks into one\n",
    "        combined_mask = combine_masks(filtered_masks, height, width)\n",
    "        \n",
    "        # Create environment image (room only, people and robot masked out)\n",
    "        env_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=False\n",
    "        )\n",
    "\n",
    "        env_path = env_dir / filename\n",
    "    \n",
    "        save_image(env_image, env_path)\n",
    "\n",
    "\n",
    "def draw_random_pear_blob_cv(mask, center, avg_radius_x, avg_radius_y,\n",
    "                             irregularity=0.3, spikiness=0.2, num_points=14,\n",
    "                             vertical_stretch=1.2, bottom_bias=0.3, color=1):\n",
    "    \"\"\"\n",
    "    Draws a random pear-shaped blob on a NumPy mask.\n",
    "    - vertical_stretch > 1.0 makes shape taller\n",
    "    - bottom_bias > 0 widens the lower half\n",
    "    \"\"\"\n",
    "    cx, cy = center\n",
    "    angle = 0\n",
    "    points = []\n",
    "    for i in range(num_points):\n",
    "        angle_step = 2 * math.pi / num_points\n",
    "        angle += angle_step + random.uniform(-angle_step * irregularity,\n",
    "                                             angle_step * irregularity)\n",
    "\n",
    "        # base radii\n",
    "        radius_x = avg_radius_x + random.uniform(-avg_radius_x * spikiness,\n",
    "                                                 avg_radius_x * spikiness)\n",
    "        radius_y = avg_radius_y + random.uniform(-avg_radius_y * (spikiness * 0.4),\n",
    "                                                 avg_radius_y * (spikiness * 0.4))\n",
    "\n",
    "        # stretch vertically\n",
    "        radius_y *= vertical_stretch\n",
    "\n",
    "        # widen bottom (when pointing down)\n",
    "        if math.sin(angle) > 0:\n",
    "            radius_x *= (1 + bottom_bias * math.sin(angle))\n",
    "\n",
    "        x = cx + math.cos(angle) * radius_x\n",
    "        y = cy + math.sin(angle) * radius_y\n",
    "        points.append((int(x), int(y)))\n",
    "\n",
    "    cv2.fillPoly(mask, [np.array(points, dtype=np.int32)], color)\n",
    "    return mask\n",
    "\n",
    "def process_segmentation_data_eplipses(detections_dataset, output_dir, imagenet_mean=[0.485, 0.456, 0.406], confidence_threshold=0.3):\n",
    "    env_dir = Path(output_dir) / \"environment_elipses\"\n",
    "    env_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for item in tqdm(detections_dataset):\n",
    "        image_path = Path(\"../data/masked/environment\") / Path(item[0]).name\n",
    "        detections = item[2]\n",
    "        \n",
    "        # Load original image\n",
    "        original_image = cv2.imread(str(image_path))\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        height, width = original_image.shape[:2]\n",
    "        \n",
    "        # Get filename for saving\n",
    "        filename = Path(image_path).name\n",
    "\n",
    "        keep_indices = [i for i, conf in enumerate(detections.confidence) if conf >= confidence_threshold]\n",
    "        \n",
    "        filtered_masks = []\n",
    "        for i in keep_indices:\n",
    "            x1, y1, x2, y2 = detections.xyxy[i]\n",
    "\n",
    "            # create blank mask\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "            # center, axes lengths, angle\n",
    "            center = ((x1 + x2) / 2, (y1 + y2) / 2)\n",
    "            axes = ((x2 - x1) / 2, (y2 - y1) / 2)\n",
    "            angle = 0  # no rotation\n",
    "\n",
    "            # draw ellipse\n",
    "            # cv2.ellipse(mask, (int(center[0]), int(center[1])),\n",
    "            #             (int(axes[0]), int(axes[1])), angle, 0, 360, 1, -1)\n",
    "            mask = draw_random_pear_blob_cv(mask, center, axes[0], axes[1],\n",
    "                         irregularity=0.6, spikiness=0.4,\n",
    "                         vertical_stretch=1.2, bottom_bias=0.4)\n",
    "            filtered_masks.append(mask)\n",
    "        \n",
    "        # Combine all masks into one\n",
    "        combined_mask = combine_masks(filtered_masks, height, width)\n",
    "        \n",
    "        # Create environment image (room only, people and robot masked out)\n",
    "        env_image = apply_mask_with_mean(\n",
    "            original_image, combined_mask, imagenet_mean, keep_masked=False\n",
    "        )\n",
    "        env_path = env_dir / filename\n",
    "\n",
    "        save_image(env_image, env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a933abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.BZ2File('../data/autodistill_dataset_home.pbz2', 'rb') as f:\n",
    "    dataset_home = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_segmentation_data_eplipses(\n",
    "    detections_dataset=dataset_home,\n",
    "    output_dir='../data/masked',\n",
    "    imagenet_mean=[0.485, 0.456, 0.406]  # ImageNet RGB means\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37458b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks_pool=get_silhouettes_pool(random.sample(sorted(dataset_home), 100))\n",
    "elipses_pool=get_ellipse_pool(random.sample(sorted(dataset_home), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_masks_to_image(\n",
    "    detections_dataset=dataset_home,\n",
    "    output_dir='../data/masked',\n",
    "    masks_pool=elipses_pool,\n",
    "    imagenet_mean=[0.485, 0.456, 0.406],\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialsense",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
